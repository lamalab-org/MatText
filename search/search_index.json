{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MatText documentation","text":"<p>MatText is a framework for text-based materials modeling. It supports </p> <ul> <li>conversion of crystal structures in to text representations </li> <li>transformations of crystal structures for sensitivity analyses</li> <li>decoding of text representations to crystal structures</li> <li>tokenization of text-representation of crystal structures</li> <li>pre-training, finetuning and testing of language models on text-representations of crystal structures </li> <li>analysis of language models trained on text-representations of crystal structures</li> </ul>"},{"location":"api/","title":"API Documentation","text":""},{"location":"api/#text-representation","title":"Text representation","text":""},{"location":"api/#core-class","title":"Core class","text":"<p>Generate text representations of crystal structure for Language modelling.</p> <p>Attributes:</p> Name Type Description <code>structure</code> <p>pymatgen structure</p> <p>Methods:</p> Name Description <code>from_input </code> <p>a classmethod</p> Source code in <code>xtal2txt/core.py</code> <pre><code>class TextRep:\n    \"\"\"\n    Generate text representations of crystal structure for Language modelling.\n\n    Attributes:\n        structure : pymatgen structure\n\n    Methods:\n        from_input : a classmethod\n        get_cif_string(n=3)\n        get_parameters(n=3)\n        get_coords(name, n=3)\n        get_cartesian(n=3)\n        get_fractional(n=3)\n    \"\"\"\n\n    backend = InvCryRep()\n    condenser = StructureCondenser()\n    describer = StructureDescriber()\n\n    def __init__(\n        self,\n        structure: Structure,\n        transformations: List[Tuple[str, dict]] = None,\n    ) -&gt; None:\n        self.structure = structure\n        self.transformations = transformations or []\n        self.apply_transformations()\n\n    @classmethod\n    def from_input(\n        cls,\n        input_data: Union[str, Path, Structure],\n        transformations: List[Tuple[str, dict]] = None,\n    ) -&gt; \"TextRep\":\n        \"\"\"\n        Instantiate the TextRep class object with the pymatgen structure from a cif file, a cif string, or a pymatgen Structure object.\n\n        Args:\n            input_data (Union[str,pymatgen.core.structure.Structure]): A cif file of a crystal structure, a cif string,\n                or a pymatgen Structure object.\n\n        Returns:\n            TextRep: A TextRep object.\n        \"\"\"\n        if isinstance(input_data, Structure):\n            structure = input_data\n\n        elif isinstance(input_data, (str, Path)):\n            try:\n                if Path(input_data).is_file():\n                    structure = Structure.from_file(str(input_data))\n                else:\n                    raise ValueError\n            except (OSError, ValueError):\n                structure = Structure.from_str(str(input_data), \"cif\")\n\n        else:\n            structure = Structure.from_str(str(input_data), \"cif\")\n\n        return cls(structure, transformations)\n\n    def apply_transformations(self) -&gt; None:\n        \"\"\"\n        Apply transformations to the structure.\n        \"\"\"\n        for transformation, params in self.transformations:\n            transform_func = getattr(TransformationCallback, transformation)\n            self.structure = transform_func(self.structure, **params)\n\n    @staticmethod\n    def _safe_call(func, *args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception:\n            return None\n\n    @staticmethod\n    def round_numbers_in_string(original_string: str, decimal_places: int) -&gt; str:\n        \"\"\"\n        Rounds float numbers in the given string to the specified number of decimal places using regex.\n\n        Args:\n            original_string (str): The input string.\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            str: The string with the float numbers rounded to the specified number of decimal places.\n        \"\"\"\n        pattern = r\"\\b\\d+\\.\\d+\\b\"\n        matches = re.findall(pattern, original_string)\n        rounded_numbers = [round(float(match), decimal_places) for match in matches]\n        new_string = re.sub(\n            pattern, lambda x: str(rounded_numbers.pop(0)), original_string\n        )\n        return new_string\n\n    def get_cif_string(\n        self, format: str = \"symmetrized\", decimal_places: int = 3\n    ) -&gt; str:\n        \"\"\"\n        Generate CIF as string in multi-line format.\n\n        All float numbers can be rounded to the specified number (decimal_places).\n        Currently supports two formats. Symmetrized (cif with symmetry operations and the least symmetric basis) ...\n        and P1 (conventional unit cell , with all the atoms listed and only identity as symmetry operation).\n\n        Args:\n            format (str): The format of the CIF file. Can be \"symmetrized\" or \"p1\".\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            str: The CIF string.\n        \"\"\"\n\n        if format == \"symmetrized\":\n            symmetry_analyzer = SpacegroupAnalyzer(self.structure)\n            symmetrized_structure = symmetry_analyzer.get_symmetrized_structure()\n            cif_string = str(\n                CifWriter(\n                    symmetrized_structure,\n                    symprec=0.1,\n                    significant_figures=decimal_places,\n                ).cif_file\n            )\n            cif = \"\\n\".join(cif_string.split(\"\\n\")[1:])\n            return self.round_numbers_in_string(cif, decimal_places)\n\n        elif format == \"p1\":\n            cif_string = \"\\n\".join(self.structure.to(fmt=\"cif\").split(\"\\n\")[1:])\n            return self.round_numbers_in_string(cif_string, decimal_places)\n\n    def get_lattice_parameters(self, decimal_places: int = 3) -&gt; List[str]:\n        \"\"\"\n        Return lattice parameters of unit cells in a crystal lattice:\n        the lengths of the cell edges (a, b, and c) in angstrom and the angles between them (alpha, beta, and gamma) in degrees.\n\n        All float numbers can be rounded to a specific number (decimal_places).\n\n        Args:\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            List[str]: The lattice parameters.\n        \"\"\"\n        return [\n            str(round(i, decimal_places)) for i in self.structure.lattice.parameters\n        ]\n\n    def get_coords(self, name: str = \"cartesian\", decimal_places: int = 3) -&gt; List[str]:\n        \"\"\"\n        Return list of atoms in unit cell for with their positions in Cartesian or fractional coordinates as per choice.\n\n        Args:\n            name (str): The name of the coordinates. Can be \"cartesian\" or \"fractional\".\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            List[str]: The list of atoms with their positions.\n        \"\"\"\n        elements = []\n        for site in self.structure.sites:\n            elements.append(str(site.specie))\n            coord = [\n                str(x)\n                for x in (\n                    site.coords.round(decimal_places)\n                    if name == \"cartesian\"\n                    else site.frac_coords.round(decimal_places)\n                )\n            ]\n            elements.extend(coord)\n        return elements\n\n    def get_slices(self, primitive: bool = True) -&gt; str:\n        \"\"\"Returns SLICES representation of the crystal structure.\n        https://www.nature.com/articles/s41467-023-42870-7\n\n        Args:\n            primitive (bool): Whether to use the primitive structure or not.\n\n        Returns:\n            str: The SLICE representation of the crystal structure.\n        \"\"\"\n\n        if primitive:\n            primitive_structure = (\n                self.structure.get_primitive_structure()\n            )  # convert to primitive structure\n            return self.backend.structure2SLICES(primitive_structure)\n        return self.backend.structure2SLICES(self.structure)\n\n    def get_composition(self, format=\"hill\") -&gt; str:\n        \"\"\"Return composition in hill format.\n\n        Args:\n            format (str): format in which the composition is required.\n\n        Returns:\n            str: The composition in hill format.\n        \"\"\"\n        if format == \"hill\":\n            composition_string = self.structure.composition.hill_formula\n            composition = composition_string.replace(\" \", \"\")\n        return composition\n\n    def get_local_env_rep(self, local_env_kwargs: Optional[dict] = None) -&gt; str:\n        \"\"\"\n        Get the local environment representation of the crystal structure.\n\n        The local environment representation is a string that contains\n        the space group symbol and the local environment of each atom in the unit cell.\n        The local environment of each atom is represented as SMILES string and the\n        Wyckoff symbol of the local environment.\n\n        Args:\n            local_env_kwargs (dict): Keyword arguments to pass to the LocalEnvAnalyzer.\n\n        Returns:\n            str: The local environment representation of the crystal structure.\n        \"\"\"\n        if not local_env_kwargs:\n            local_env_kwargs = {}\n        analyzer = LocalEnvAnalyzer(**local_env_kwargs)\n        return analyzer.structure_to_local_env_string(self.structure)\n\n    def get_crystal_text_llm(\n        self,\n        permute_atoms: bool = False,\n    ) -&gt; str:\n        \"\"\"\n        Code adopted from https://github.com/facebookresearch/crystal-llm/blob/main/llama_finetune.py\n        https://openreview.net/pdf?id=0r5DE2ZSwJ\n\n        Returns the representation as per the above citation.\n\n        Args:\n            permute_atoms (bool): Whether to permute the atoms in the unit cell.\n\n        Returns:\n            str: The crystal-llm representation of the crystal structure.\n        \"\"\"\n\n        lengths = self.structure.lattice.parameters[:3]\n        angles = self.structure.lattice.parameters[3:]\n        atom_ids = self.structure.species\n        frac_coords = self.structure.frac_coords\n\n        if permute_atoms:\n            atom_coord_pairs = list(zip(atom_ids, frac_coords))\n            random.shuffle(atom_coord_pairs)\n            atom_ids, frac_coords = zip(*atom_coord_pairs)\n\n        crystal_str = (\n            \" \".join([\"{0:.1f}\".format(x) for x in lengths])\n            + \"\\n\"\n            + \" \".join([str(int(x)) for x in angles])\n            + \"\\n\"\n            + \"\\n\".join(\n                [\n                    str(t) + \"\\n\" + \" \".join([\"{0:.2f}\".format(x) for x in c])\n                    for t, c in zip(atom_ids, frac_coords)\n                ]\n            )\n        )\n\n        return crystal_str\n\n    def get_robocrys_rep(self):\n        \"\"\"\n        https://github.com/hackingmaterials/robocrystallographer/tree/main\n        \"\"\"\n\n        condensed_structure = self.condenser.condense_structure(self.structure)\n        return self.describer.describe(condensed_structure)\n\n    def get_wyckoff_positions(self):\n        \"\"\"\n        Getting wyckoff positions of the elements in the unit cell as the combination of...\n        number and letter.\n\n        Returns:\n            str:  A multi-line string that contain elements of the unit cell along with their wyckoff position in each line.\n\n        Hint:\n            At the end of the string, there is an additional newline character.\n        \"\"\"\n\n        spacegroup_analyzer = SpacegroupAnalyzer(self.structure)\n        wyckoff_sites = spacegroup_analyzer.get_symmetry_dataset()\n        element_symbols = [site.specie.element.symbol for site in self.structure.sites]\n\n        data = []\n\n        for i in range(len(wyckoff_sites[\"wyckoffs\"])):\n            sub_data = (\n                element_symbols[i],\n                wyckoff_sites[\"wyckoffs\"][i],\n                wyckoff_sites[\"equivalent_atoms\"][i],\n            )\n            data.append(sub_data)\n\n        a = dict(Counter(data))\n\n        output = \"\"\n        for i, j in a.items():\n            output += str(i[0]) + \" \" + str(j) + \" \" + str(i[1]) + \"\\n\"\n\n        return output\n\n    def get_wycryst(self):\n        \"\"\"\n        Obtaining the wyckoff representation for crystal structures that include:\n            chemical formula\n            space group number\n            elements of the unit cell with their wyckoff positions.\n\n        Returns:\n            str: A multi-line string that contains the chemical formula, space group number,\n                and the elements of the unit cell with their wyckoff positions.\n        \"\"\"\n        output = \"\"\n        chemical_formula = self.structure.composition.formula\n        output += chemical_formula\n        output += \"\\n\" + str(self.structure.get_space_group_info()[1])\n        output += \"\\n\" + self.get_wyckoff_positions()\n\n        return output\n\n    def get_atom_sequences_plusplus(\n        self, lattice_params: bool = False, decimal_places: int = 1\n    ) -&gt; str:\n        \"\"\"\n        Generating a string with the elements of composition inside the crystal lattice with the option to\n        get the lattice parameters as angles (int) and lengths (float) in a string with a space\n        between them\n\n        Args:\n            lattice_params (bool): Whether to include lattice parameters or not.\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            str: The string representation of the crystal structure.\n        \"\"\"\n\n        try:\n            output = [site.specie.element.symbol for site in self.structure.sites]\n        except AttributeError:\n            output = [site.specie.symbol for site in self.structure.sites]\n        if lattice_params:\n            params = self.get_lattice_parameters(decimal_places=decimal_places)\n            params[3:] = [str(int(float(i))) for i in params[3:]]\n            output.extend(params)\n\n        return \" \".join(output)\n\n    def updated_zmatrix_rep(self, zmatrix, decimal_places=1):\n        \"\"\"\n        Replace the variables in the Z-matrix with their values and return the updated Z-matrix.\n        for eg: z-matrix from pymatgen\n        'N\\nN 1 B1\\nN 1 B2 2 A2\\nN 1 B3 2 A3 3 D3\\n\n        B1=3.79\n        B2=6.54\n        ....\n        is replaced to\n        'N\\nN 1 3.79\\nN 1 6.54 2 90\\nN 1 6.54 2 90 3 120\\n'\n\n        Args:\n            Zmatrix (bool): zmatrix multi line string as implemented in pymatgen.\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            str: The updated Z-matrix representation of the crystal structure.\n        \"\"\"\n        lines = zmatrix.split(\"\\n\")\n        main_part = []\n        variables_part = []\n\n        # Determine the main part and the variables part of the Z-matrix\n        for line in lines:\n            if \"=\" in line:\n                variables_part.append(line)\n            else:\n                if line.strip():  # Skip empty lines\n                    main_part.append(line)\n\n        # Extract variables from the variables part\n        variable_dict = {}\n        for var_line in variables_part:\n            var, value = var_line.split(\"=\")\n            if var.startswith(\"B\"):\n                rounded_value = round(float(value.strip()), decimal_places)\n            else:\n                rounded_value = int(round(float(value.strip())))\n            variable_dict[var] = (\n                f\"{rounded_value}\"\n                if var.startswith((\"A\", \"D\"))\n                else f\"{rounded_value:.{decimal_places}f}\"\n            )\n\n        # Replace variables in the main part\n        replaced_lines = []\n        for line in main_part:\n            parts = line.split()\n            # atom = parts[0]\n            replaced_line = line\n            for i in range(1, len(parts)):\n                var = parts[i]\n                if var in variable_dict:\n                    replaced_line = replaced_line.replace(var, variable_dict[var])\n            replaced_lines.append(replaced_line)\n\n        return \"\\n\".join(replaced_lines)\n\n    def get_zmatrix_rep(self, decimal_places=1):\n        \"\"\"\n        Generate the Z-matrix representation of the crystal structure.\n        It provides a description of each atom in terms of its atomic number,\n        bond length, bond angle, and dihedral angle, the so-called internal coordinates.\n\n        Disclaimer: The Z-matrix is meant for molecules, current implementation converts atoms within unit cell to molecule.\n        Hence the current implentation might overlook bonds acrosse unit cells.\n        \"\"\"\n        species = [\n            s.element if hasattr(s, \"element\") else s for s in self.structure.species\n        ]\n        coords = [c for c in self.structure.cart_coords]\n        molecule_ = Molecule(\n            species,\n            coords,\n        )\n        zmatrix = molecule_.get_zmatrix()\n        return self.updated_zmatrix_rep(zmatrix, decimal_places)\n\n    def get_all_text_reps(self, decimal_places: int = 2):\n        \"\"\"\n        Returns all the Text representations of the crystal structure in a dictionary.\n        \"\"\"\n\n        return {\n            \"cif_p1\": self._safe_call(\n                self.get_cif_string, format=\"p1\", decimal_places=decimal_places\n            ),\n            \"cif_symmetrized\": self._safe_call(\n                self.get_cif_string, format=\"symmetrized\", decimal_places=decimal_places\n            ),\n            \"cif_bonding\": None,\n            \"slices\": self._safe_call(self.get_slices),\n            \"composition\": self._safe_call(self.get_composition),\n            \"crystal_text_llm\": self._safe_call(self.get_crystal_text_llm),\n            \"robocrys_rep\": self._safe_call(self.get_robocrys_rep),\n            \"wycoff_rep\": None,\n            \"atom_sequences\": self._safe_call(\n                self.get_atom_sequences_plusplus,\n                lattice_params=False,\n                decimal_places=decimal_places,\n            ),\n            \"atom_sequences_plusplus\": self._safe_call(\n                self.get_atom_sequences_plusplus,\n                lattice_params=True,\n                decimal_places=decimal_places,\n            ),\n            \"zmatrix\": self._safe_call(self.get_zmatrix_rep),\n            \"local_env\": self._safe_call(self.get_local_env_rep, local_env_kwargs=None),\n        }\n\n    def get_requested_text_reps(\n        self, requested_reps: List[str], decimal_places: int = 2\n    ):\n        \"\"\"\n        Returns the requested Text representations of the crystal structure in a dictionary.\n\n        Args:\n            requested_reps (List[str]): The list of representations to return.\n            decimal_places (int): The number of decimal places to round to.\n\n        Returns:\n            dict: A dictionary containing the requested text representations of the crystal structure.\n        \"\"\"\n\n        if requested_reps == \"cif_p1\":\n            return self._safe_call(\n                self.get_cif_string, format=\"p1\", decimal_places=decimal_places\n            )\n\n        elif requested_reps == \"cif_symmetrized\":\n            return self._safe_call(\n                self.get_cif_string,\n                format=\"symmetrized\",\n                decimal_places=decimal_places,\n            )\n\n        elif requested_reps == \"slices\":\n            return self._safe_call(self.get_slices)\n\n        elif requested_reps == \"composition\":\n            return self._safe_call(self.get_composition)\n\n        elif requested_reps == \"crystal_text_llm\":\n            return self._safe_call(self.get_crystal_text_llm)\n\n        elif requested_reps == \"robocrys_rep\":\n            return self._safe_call(self.get_robocrys_rep)\n\n        elif requested_reps == \"atom_sequences\":\n            return self._safe_call(\n                self.get_atom_sequences_plusplus,\n                lattice_params=False,\n                decimal_places=decimal_places,\n            )\n\n        elif requested_reps == \"atom_sequences_plusplus\":\n            return self._safe_call(\n                self.get_atom_sequences_plusplus,\n                lattice_params=True,\n                decimal_places=decimal_places,\n            )\n\n        elif requested_reps == \"zmatrix\":\n            return self._safe_call(self.get_zmatrix_rep)\n\n        elif requested_reps == \"local_env\":\n            return self._safe_call(self.get_local_env_rep, local_env_kwargs=None)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.apply_transformations","title":"<code>apply_transformations()</code>","text":"<p>Apply transformations to the structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def apply_transformations(self) -&gt; None:\n    \"\"\"\n    Apply transformations to the structure.\n    \"\"\"\n    for transformation, params in self.transformations:\n        transform_func = getattr(TransformationCallback, transformation)\n        self.structure = transform_func(self.structure, **params)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.from_input","title":"<code>from_input(input_data, transformations=None)</code>  <code>classmethod</code>","text":"<p>Instantiate the TextRep class object with the pymatgen structure from a cif file, a cif string, or a pymatgen Structure object.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Union[str, Structure]</code> <p>A cif file of a crystal structure, a cif string, or a pymatgen Structure object.</p> required <p>Returns:</p> Name Type Description <code>TextRep</code> <code>TextRep</code> <p>A TextRep object.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>@classmethod\ndef from_input(\n    cls,\n    input_data: Union[str, Path, Structure],\n    transformations: List[Tuple[str, dict]] = None,\n) -&gt; \"TextRep\":\n    \"\"\"\n    Instantiate the TextRep class object with the pymatgen structure from a cif file, a cif string, or a pymatgen Structure object.\n\n    Args:\n        input_data (Union[str,pymatgen.core.structure.Structure]): A cif file of a crystal structure, a cif string,\n            or a pymatgen Structure object.\n\n    Returns:\n        TextRep: A TextRep object.\n    \"\"\"\n    if isinstance(input_data, Structure):\n        structure = input_data\n\n    elif isinstance(input_data, (str, Path)):\n        try:\n            if Path(input_data).is_file():\n                structure = Structure.from_file(str(input_data))\n            else:\n                raise ValueError\n        except (OSError, ValueError):\n            structure = Structure.from_str(str(input_data), \"cif\")\n\n    else:\n        structure = Structure.from_str(str(input_data), \"cif\")\n\n    return cls(structure, transformations)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_all_text_reps","title":"<code>get_all_text_reps(decimal_places=2)</code>","text":"<p>Returns all the Text representations of the crystal structure in a dictionary.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_all_text_reps(self, decimal_places: int = 2):\n    \"\"\"\n    Returns all the Text representations of the crystal structure in a dictionary.\n    \"\"\"\n\n    return {\n        \"cif_p1\": self._safe_call(\n            self.get_cif_string, format=\"p1\", decimal_places=decimal_places\n        ),\n        \"cif_symmetrized\": self._safe_call(\n            self.get_cif_string, format=\"symmetrized\", decimal_places=decimal_places\n        ),\n        \"cif_bonding\": None,\n        \"slices\": self._safe_call(self.get_slices),\n        \"composition\": self._safe_call(self.get_composition),\n        \"crystal_text_llm\": self._safe_call(self.get_crystal_text_llm),\n        \"robocrys_rep\": self._safe_call(self.get_robocrys_rep),\n        \"wycoff_rep\": None,\n        \"atom_sequences\": self._safe_call(\n            self.get_atom_sequences_plusplus,\n            lattice_params=False,\n            decimal_places=decimal_places,\n        ),\n        \"atom_sequences_plusplus\": self._safe_call(\n            self.get_atom_sequences_plusplus,\n            lattice_params=True,\n            decimal_places=decimal_places,\n        ),\n        \"zmatrix\": self._safe_call(self.get_zmatrix_rep),\n        \"local_env\": self._safe_call(self.get_local_env_rep, local_env_kwargs=None),\n    }\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_atom_sequences_plusplus","title":"<code>get_atom_sequences_plusplus(lattice_params=False, decimal_places=1)</code>","text":"<p>Generating a string with the elements of composition inside the crystal lattice with the option to get the lattice parameters as angles (int) and lengths (float) in a string with a space between them</p> <p>Parameters:</p> Name Type Description Default <code>lattice_params</code> <code>bool</code> <p>Whether to include lattice parameters or not.</p> <code>False</code> <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string representation of the crystal structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_atom_sequences_plusplus(\n    self, lattice_params: bool = False, decimal_places: int = 1\n) -&gt; str:\n    \"\"\"\n    Generating a string with the elements of composition inside the crystal lattice with the option to\n    get the lattice parameters as angles (int) and lengths (float) in a string with a space\n    between them\n\n    Args:\n        lattice_params (bool): Whether to include lattice parameters or not.\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        str: The string representation of the crystal structure.\n    \"\"\"\n\n    try:\n        output = [site.specie.element.symbol for site in self.structure.sites]\n    except AttributeError:\n        output = [site.specie.symbol for site in self.structure.sites]\n    if lattice_params:\n        params = self.get_lattice_parameters(decimal_places=decimal_places)\n        params[3:] = [str(int(float(i))) for i in params[3:]]\n        output.extend(params)\n\n    return \" \".join(output)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_cif_string","title":"<code>get_cif_string(format='symmetrized', decimal_places=3)</code>","text":"<p>Generate CIF as string in multi-line format.</p> <p>All float numbers can be rounded to the specified number (decimal_places). Currently supports two formats. Symmetrized (cif with symmetry operations and the least symmetric basis) ... and P1 (conventional unit cell , with all the atoms listed and only identity as symmetry operation).</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>The format of the CIF file. Can be \"symmetrized\" or \"p1\".</p> <code>'symmetrized'</code> <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The CIF string.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_cif_string(\n    self, format: str = \"symmetrized\", decimal_places: int = 3\n) -&gt; str:\n    \"\"\"\n    Generate CIF as string in multi-line format.\n\n    All float numbers can be rounded to the specified number (decimal_places).\n    Currently supports two formats. Symmetrized (cif with symmetry operations and the least symmetric basis) ...\n    and P1 (conventional unit cell , with all the atoms listed and only identity as symmetry operation).\n\n    Args:\n        format (str): The format of the CIF file. Can be \"symmetrized\" or \"p1\".\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        str: The CIF string.\n    \"\"\"\n\n    if format == \"symmetrized\":\n        symmetry_analyzer = SpacegroupAnalyzer(self.structure)\n        symmetrized_structure = symmetry_analyzer.get_symmetrized_structure()\n        cif_string = str(\n            CifWriter(\n                symmetrized_structure,\n                symprec=0.1,\n                significant_figures=decimal_places,\n            ).cif_file\n        )\n        cif = \"\\n\".join(cif_string.split(\"\\n\")[1:])\n        return self.round_numbers_in_string(cif, decimal_places)\n\n    elif format == \"p1\":\n        cif_string = \"\\n\".join(self.structure.to(fmt=\"cif\").split(\"\\n\")[1:])\n        return self.round_numbers_in_string(cif_string, decimal_places)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_composition","title":"<code>get_composition(format='hill')</code>","text":"<p>Return composition in hill format.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>str</code> <p>format in which the composition is required.</p> <code>'hill'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The composition in hill format.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_composition(self, format=\"hill\") -&gt; str:\n    \"\"\"Return composition in hill format.\n\n    Args:\n        format (str): format in which the composition is required.\n\n    Returns:\n        str: The composition in hill format.\n    \"\"\"\n    if format == \"hill\":\n        composition_string = self.structure.composition.hill_formula\n        composition = composition_string.replace(\" \", \"\")\n    return composition\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_coords","title":"<code>get_coords(name='cartesian', decimal_places=3)</code>","text":"<p>Return list of atoms in unit cell for with their positions in Cartesian or fractional coordinates as per choice.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the coordinates. Can be \"cartesian\" or \"fractional\".</p> <code>'cartesian'</code> <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>3</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of atoms with their positions.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_coords(self, name: str = \"cartesian\", decimal_places: int = 3) -&gt; List[str]:\n    \"\"\"\n    Return list of atoms in unit cell for with their positions in Cartesian or fractional coordinates as per choice.\n\n    Args:\n        name (str): The name of the coordinates. Can be \"cartesian\" or \"fractional\".\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        List[str]: The list of atoms with their positions.\n    \"\"\"\n    elements = []\n    for site in self.structure.sites:\n        elements.append(str(site.specie))\n        coord = [\n            str(x)\n            for x in (\n                site.coords.round(decimal_places)\n                if name == \"cartesian\"\n                else site.frac_coords.round(decimal_places)\n            )\n        ]\n        elements.extend(coord)\n    return elements\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_crystal_text_llm","title":"<code>get_crystal_text_llm(permute_atoms=False)</code>","text":"<p>Code adopted from https://github.com/facebookresearch/crystal-llm/blob/main/llama_finetune.py https://openreview.net/pdf?id=0r5DE2ZSwJ</p> <p>Returns the representation as per the above citation.</p> <p>Parameters:</p> Name Type Description Default <code>permute_atoms</code> <code>bool</code> <p>Whether to permute the atoms in the unit cell.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The crystal-llm representation of the crystal structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_crystal_text_llm(\n    self,\n    permute_atoms: bool = False,\n) -&gt; str:\n    \"\"\"\n    Code adopted from https://github.com/facebookresearch/crystal-llm/blob/main/llama_finetune.py\n    https://openreview.net/pdf?id=0r5DE2ZSwJ\n\n    Returns the representation as per the above citation.\n\n    Args:\n        permute_atoms (bool): Whether to permute the atoms in the unit cell.\n\n    Returns:\n        str: The crystal-llm representation of the crystal structure.\n    \"\"\"\n\n    lengths = self.structure.lattice.parameters[:3]\n    angles = self.structure.lattice.parameters[3:]\n    atom_ids = self.structure.species\n    frac_coords = self.structure.frac_coords\n\n    if permute_atoms:\n        atom_coord_pairs = list(zip(atom_ids, frac_coords))\n        random.shuffle(atom_coord_pairs)\n        atom_ids, frac_coords = zip(*atom_coord_pairs)\n\n    crystal_str = (\n        \" \".join([\"{0:.1f}\".format(x) for x in lengths])\n        + \"\\n\"\n        + \" \".join([str(int(x)) for x in angles])\n        + \"\\n\"\n        + \"\\n\".join(\n            [\n                str(t) + \"\\n\" + \" \".join([\"{0:.2f}\".format(x) for x in c])\n                for t, c in zip(atom_ids, frac_coords)\n            ]\n        )\n    )\n\n    return crystal_str\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_lattice_parameters","title":"<code>get_lattice_parameters(decimal_places=3)</code>","text":"<p>Return lattice parameters of unit cells in a crystal lattice: the lengths of the cell edges (a, b, and c) in angstrom and the angles between them (alpha, beta, and gamma) in degrees.</p> <p>All float numbers can be rounded to a specific number (decimal_places).</p> <p>Parameters:</p> Name Type Description Default <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>3</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The lattice parameters.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_lattice_parameters(self, decimal_places: int = 3) -&gt; List[str]:\n    \"\"\"\n    Return lattice parameters of unit cells in a crystal lattice:\n    the lengths of the cell edges (a, b, and c) in angstrom and the angles between them (alpha, beta, and gamma) in degrees.\n\n    All float numbers can be rounded to a specific number (decimal_places).\n\n    Args:\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        List[str]: The lattice parameters.\n    \"\"\"\n    return [\n        str(round(i, decimal_places)) for i in self.structure.lattice.parameters\n    ]\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_local_env_rep","title":"<code>get_local_env_rep(local_env_kwargs=None)</code>","text":"<p>Get the local environment representation of the crystal structure.</p> <p>The local environment representation is a string that contains the space group symbol and the local environment of each atom in the unit cell. The local environment of each atom is represented as SMILES string and the Wyckoff symbol of the local environment.</p> <p>Parameters:</p> Name Type Description Default <code>local_env_kwargs</code> <code>dict</code> <p>Keyword arguments to pass to the LocalEnvAnalyzer.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The local environment representation of the crystal structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_local_env_rep(self, local_env_kwargs: Optional[dict] = None) -&gt; str:\n    \"\"\"\n    Get the local environment representation of the crystal structure.\n\n    The local environment representation is a string that contains\n    the space group symbol and the local environment of each atom in the unit cell.\n    The local environment of each atom is represented as SMILES string and the\n    Wyckoff symbol of the local environment.\n\n    Args:\n        local_env_kwargs (dict): Keyword arguments to pass to the LocalEnvAnalyzer.\n\n    Returns:\n        str: The local environment representation of the crystal structure.\n    \"\"\"\n    if not local_env_kwargs:\n        local_env_kwargs = {}\n    analyzer = LocalEnvAnalyzer(**local_env_kwargs)\n    return analyzer.structure_to_local_env_string(self.structure)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_requested_text_reps","title":"<code>get_requested_text_reps(requested_reps, decimal_places=2)</code>","text":"<p>Returns the requested Text representations of the crystal structure in a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>requested_reps</code> <code>List[str]</code> <p>The list of representations to return.</p> required <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the requested text representations of the crystal structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_requested_text_reps(\n    self, requested_reps: List[str], decimal_places: int = 2\n):\n    \"\"\"\n    Returns the requested Text representations of the crystal structure in a dictionary.\n\n    Args:\n        requested_reps (List[str]): The list of representations to return.\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        dict: A dictionary containing the requested text representations of the crystal structure.\n    \"\"\"\n\n    if requested_reps == \"cif_p1\":\n        return self._safe_call(\n            self.get_cif_string, format=\"p1\", decimal_places=decimal_places\n        )\n\n    elif requested_reps == \"cif_symmetrized\":\n        return self._safe_call(\n            self.get_cif_string,\n            format=\"symmetrized\",\n            decimal_places=decimal_places,\n        )\n\n    elif requested_reps == \"slices\":\n        return self._safe_call(self.get_slices)\n\n    elif requested_reps == \"composition\":\n        return self._safe_call(self.get_composition)\n\n    elif requested_reps == \"crystal_text_llm\":\n        return self._safe_call(self.get_crystal_text_llm)\n\n    elif requested_reps == \"robocrys_rep\":\n        return self._safe_call(self.get_robocrys_rep)\n\n    elif requested_reps == \"atom_sequences\":\n        return self._safe_call(\n            self.get_atom_sequences_plusplus,\n            lattice_params=False,\n            decimal_places=decimal_places,\n        )\n\n    elif requested_reps == \"atom_sequences_plusplus\":\n        return self._safe_call(\n            self.get_atom_sequences_plusplus,\n            lattice_params=True,\n            decimal_places=decimal_places,\n        )\n\n    elif requested_reps == \"zmatrix\":\n        return self._safe_call(self.get_zmatrix_rep)\n\n    elif requested_reps == \"local_env\":\n        return self._safe_call(self.get_local_env_rep, local_env_kwargs=None)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_robocrys_rep","title":"<code>get_robocrys_rep()</code>","text":"Source code in <code>xtal2txt/core.py</code> <pre><code>def get_robocrys_rep(self):\n    \"\"\"\n    https://github.com/hackingmaterials/robocrystallographer/tree/main\n    \"\"\"\n\n    condensed_structure = self.condenser.condense_structure(self.structure)\n    return self.describer.describe(condensed_structure)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_slices","title":"<code>get_slices(primitive=True)</code>","text":"<p>Returns SLICES representation of the crystal structure. https://www.nature.com/articles/s41467-023-42870-7</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <code>bool</code> <p>Whether to use the primitive structure or not.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The SLICE representation of the crystal structure.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_slices(self, primitive: bool = True) -&gt; str:\n    \"\"\"Returns SLICES representation of the crystal structure.\n    https://www.nature.com/articles/s41467-023-42870-7\n\n    Args:\n        primitive (bool): Whether to use the primitive structure or not.\n\n    Returns:\n        str: The SLICE representation of the crystal structure.\n    \"\"\"\n\n    if primitive:\n        primitive_structure = (\n            self.structure.get_primitive_structure()\n        )  # convert to primitive structure\n        return self.backend.structure2SLICES(primitive_structure)\n    return self.backend.structure2SLICES(self.structure)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_wyckoff_positions","title":"<code>get_wyckoff_positions()</code>","text":"<p>Getting wyckoff positions of the elements in the unit cell as the combination of... number and letter.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A multi-line string that contain elements of the unit cell along with their wyckoff position in each line.</p> Hint <p>At the end of the string, there is an additional newline character.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_wyckoff_positions(self):\n    \"\"\"\n    Getting wyckoff positions of the elements in the unit cell as the combination of...\n    number and letter.\n\n    Returns:\n        str:  A multi-line string that contain elements of the unit cell along with their wyckoff position in each line.\n\n    Hint:\n        At the end of the string, there is an additional newline character.\n    \"\"\"\n\n    spacegroup_analyzer = SpacegroupAnalyzer(self.structure)\n    wyckoff_sites = spacegroup_analyzer.get_symmetry_dataset()\n    element_symbols = [site.specie.element.symbol for site in self.structure.sites]\n\n    data = []\n\n    for i in range(len(wyckoff_sites[\"wyckoffs\"])):\n        sub_data = (\n            element_symbols[i],\n            wyckoff_sites[\"wyckoffs\"][i],\n            wyckoff_sites[\"equivalent_atoms\"][i],\n        )\n        data.append(sub_data)\n\n    a = dict(Counter(data))\n\n    output = \"\"\n    for i, j in a.items():\n        output += str(i[0]) + \" \" + str(j) + \" \" + str(i[1]) + \"\\n\"\n\n    return output\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_wycryst","title":"<code>get_wycryst()</code>","text":"Obtaining the wyckoff representation for crystal structures that include <p>chemical formula space group number elements of the unit cell with their wyckoff positions.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A multi-line string that contains the chemical formula, space group number, and the elements of the unit cell with their wyckoff positions.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_wycryst(self):\n    \"\"\"\n    Obtaining the wyckoff representation for crystal structures that include:\n        chemical formula\n        space group number\n        elements of the unit cell with their wyckoff positions.\n\n    Returns:\n        str: A multi-line string that contains the chemical formula, space group number,\n            and the elements of the unit cell with their wyckoff positions.\n    \"\"\"\n    output = \"\"\n    chemical_formula = self.structure.composition.formula\n    output += chemical_formula\n    output += \"\\n\" + str(self.structure.get_space_group_info()[1])\n    output += \"\\n\" + self.get_wyckoff_positions()\n\n    return output\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.get_zmatrix_rep","title":"<code>get_zmatrix_rep(decimal_places=1)</code>","text":"<p>Generate the Z-matrix representation of the crystal structure. It provides a description of each atom in terms of its atomic number, bond length, bond angle, and dihedral angle, the so-called internal coordinates.</p> <p>Disclaimer: The Z-matrix is meant for molecules, current implementation converts atoms within unit cell to molecule. Hence the current implentation might overlook bonds acrosse unit cells.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>def get_zmatrix_rep(self, decimal_places=1):\n    \"\"\"\n    Generate the Z-matrix representation of the crystal structure.\n    It provides a description of each atom in terms of its atomic number,\n    bond length, bond angle, and dihedral angle, the so-called internal coordinates.\n\n    Disclaimer: The Z-matrix is meant for molecules, current implementation converts atoms within unit cell to molecule.\n    Hence the current implentation might overlook bonds acrosse unit cells.\n    \"\"\"\n    species = [\n        s.element if hasattr(s, \"element\") else s for s in self.structure.species\n    ]\n    coords = [c for c in self.structure.cart_coords]\n    molecule_ = Molecule(\n        species,\n        coords,\n    )\n    zmatrix = molecule_.get_zmatrix()\n    return self.updated_zmatrix_rep(zmatrix, decimal_places)\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.round_numbers_in_string","title":"<code>round_numbers_in_string(original_string, decimal_places)</code>  <code>staticmethod</code>","text":"<p>Rounds float numbers in the given string to the specified number of decimal places using regex.</p> <p>Parameters:</p> Name Type Description Default <code>original_string</code> <code>str</code> <p>The input string.</p> required <code>decimal_places</code> <code>int</code> <p>The number of decimal places to round to.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string with the float numbers rounded to the specified number of decimal places.</p> Source code in <code>xtal2txt/core.py</code> <pre><code>@staticmethod\ndef round_numbers_in_string(original_string: str, decimal_places: int) -&gt; str:\n    \"\"\"\n    Rounds float numbers in the given string to the specified number of decimal places using regex.\n\n    Args:\n        original_string (str): The input string.\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        str: The string with the float numbers rounded to the specified number of decimal places.\n    \"\"\"\n    pattern = r\"\\b\\d+\\.\\d+\\b\"\n    matches = re.findall(pattern, original_string)\n    rounded_numbers = [round(float(match), decimal_places) for match in matches]\n    new_string = re.sub(\n        pattern, lambda x: str(rounded_numbers.pop(0)), original_string\n    )\n    return new_string\n</code></pre>"},{"location":"api/#mattext.representations.TextRep.updated_zmatrix_rep","title":"<code>updated_zmatrix_rep(zmatrix, decimal_places=1)</code>","text":"<pre><code>    Replace the variables in the Z-matrix with their values and return the updated Z-matrix.\n    for eg: z-matrix from pymatgen\n    'N\n</code></pre> <p>N 1 B1 N 1 B2 2 A2 N 1 B3 2 A3 3 D3</p> <pre><code>    B1=3.79\n    B2=6.54\n    ....\n    is replaced to\n    'N\n</code></pre> <p>N 1 3.79 N 1 6.54 2 90 N 1 6.54 2 90 3 120 '</p> <pre><code>    Args:\n        Zmatrix (bool): zmatrix multi line string as implemented in pymatgen.\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        str: The updated Z-matrix representation of the crystal structure.\n</code></pre> Source code in <code>xtal2txt/core.py</code> <pre><code>def updated_zmatrix_rep(self, zmatrix, decimal_places=1):\n    \"\"\"\n    Replace the variables in the Z-matrix with their values and return the updated Z-matrix.\n    for eg: z-matrix from pymatgen\n    'N\\nN 1 B1\\nN 1 B2 2 A2\\nN 1 B3 2 A3 3 D3\\n\n    B1=3.79\n    B2=6.54\n    ....\n    is replaced to\n    'N\\nN 1 3.79\\nN 1 6.54 2 90\\nN 1 6.54 2 90 3 120\\n'\n\n    Args:\n        Zmatrix (bool): zmatrix multi line string as implemented in pymatgen.\n        decimal_places (int): The number of decimal places to round to.\n\n    Returns:\n        str: The updated Z-matrix representation of the crystal structure.\n    \"\"\"\n    lines = zmatrix.split(\"\\n\")\n    main_part = []\n    variables_part = []\n\n    # Determine the main part and the variables part of the Z-matrix\n    for line in lines:\n        if \"=\" in line:\n            variables_part.append(line)\n        else:\n            if line.strip():  # Skip empty lines\n                main_part.append(line)\n\n    # Extract variables from the variables part\n    variable_dict = {}\n    for var_line in variables_part:\n        var, value = var_line.split(\"=\")\n        if var.startswith(\"B\"):\n            rounded_value = round(float(value.strip()), decimal_places)\n        else:\n            rounded_value = int(round(float(value.strip())))\n        variable_dict[var] = (\n            f\"{rounded_value}\"\n            if var.startswith((\"A\", \"D\"))\n            else f\"{rounded_value:.{decimal_places}f}\"\n        )\n\n    # Replace variables in the main part\n    replaced_lines = []\n    for line in main_part:\n        parts = line.split()\n        # atom = parts[0]\n        replaced_line = line\n        for i in range(1, len(parts)):\n            var = parts[i]\n            if var in variable_dict:\n                replaced_line = replaced_line.replace(var, variable_dict[var])\n        replaced_lines.append(replaced_line)\n\n    return \"\\n\".join(replaced_lines)\n</code></pre>"},{"location":"api/#decoding","title":"Decoding","text":""},{"location":"api/#mattext.representations.decoder.DecodeTextRep","title":"<code>DecodeTextRep</code>","text":"Source code in <code>xtal2txt/decoder.py</code> <pre><code>class DecodeTextRep:\n    def __init__(self, text):\n        self.text = text\n\n    def decode(self):\n        return self.text\n\n    def wyckoff_decoder(self, input: str, lattice_params: bool = False):\n        \"\"\"\n        Generating a pymatgen object from the output of the get_wyckoff_rep() method by using...\n        pyxtal package. In this method, all data are extracted from the multi-line string of the...\n        mentioned method.\n        In pyxtal package, a 3D crystal is produced by specifying the dimensions, elements,\n        composition of elements, space group, and sites as wyckoff positions of the elements.\n\n        Params:\n            lattice_params: boolean\n                To specify whether use lattice parameters in generating crystal structure.\n\n        Returns:\n            pmg_struc: pymatgen.core.structure.Structure\n        \"\"\"\n\n        # Always dimension is 3.\n        dimensions = 3\n\n        entities = input.split(\"\\n\")[:-1]\n        elements = entities[0]\n        spg = int(entities[1])\n        wyckoff_sites = entities[2:]\n        elements = elements.split(\" \")\n\n        atoms = []\n        composition = []\n        for el in elements:\n            atom = el.rstrip(\"0123456789\")\n            number = el[len(atom) :]\n            atoms.append(atom)\n            composition.append(int(number))\n\n        sites = []\n        for atom in atoms:\n            sub_site = []\n            for site in wyckoff_sites:\n                if atom in site:\n                    sub_site.append(site.split()[1])\n\n            sites.append(sub_site)\n\n        xtal_struc = pyxtal()\n\n        if lattice_params:\n            a, b, c, alpha, beta, gamma = self.get_lattice_parameters()\n            cell = pyLattice.from_para(\n                float(a), float(b), float(c), float(alpha), float(beta), float(gamma)\n            )\n            xtal_struc.from_random(dimensions, spg, atoms, composition, sites=sites, lattice=cell)\n        else:\n            xtal_struc.from_random(dimensions, spg, atoms, composition, sites=sites)\n\n        pmg_struc = xtal_struc.to_pymatgen()\n\n        return pmg_struc\n\n    def llm_decoder(self, input: str):\n        \"\"\"\n        Returning pymatgen structure out of multi-line representation.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n        Returns:\n            pymatgen.core.structure.Structure\n        \"\"\"\n        entities = input.split(\"\\n\")\n        lengths = entities[0].split(\" \")\n        angles = entities[1].split(\" \")\n        lattice = Lattice.from_parameters(\n            a=float(lengths[0]),\n            b=float(lengths[1]),\n            c=float(lengths[2]),\n            alpha=float(angles[0]),\n            beta=float(angles[1]),\n            gamma=float(angles[2]),\n        )\n\n        elements = entities[2::2]\n        coordinates = entities[3::2]\n        m_coord = []\n        for i in coordinates:\n            s = [float(j) for j in i.split(\" \")]\n            m_coord.append(s)\n\n        return Structure(lattice, elements, m_coord)\n\n    def cif_string_decoder_p1(self, input: str):\n        \"\"\"\n        Returning a pymatgen structure out of a string format of a cif file.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n        Returns:\n            pymatgen.core.structure.Structure\n        \"\"\"\n        entities = input.split(\"\\n\")[:-1]\n\n        params = []\n        for i in range(2, 8):\n            params.append(entities[i].split(\"   \")[1])\n\n        lattice = Lattice.from_parameters(\n            a=float(params[0]),\n            b=float(params[1]),\n            c=float(params[2]),\n            alpha=float(params[3]),\n            beta=float(params[4]),\n            gamma=float(params[5]),\n        )\n\n        elements = []\n        m_coord = []\n        atoms = entities[entities.index(\" _atom_site_occupancy\") + 1 :]\n        for atom in atoms:\n            ls = atom.split(\"  \")\n            elements.append(ls[1])\n            m_coord.append([float(ls[4]), float(ls[5]), float(ls[6])])\n\n        return Structure(lattice, elements, m_coord)\n\n    def cif_string_decoder_sym(self, input: str):\n        \"\"\"\n        Returning a pymatgen structure out of a string format of a symmetrized cif file.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n        Returns:\n            pymatgen.core.structure.Structure\n        \"\"\"\n        entities = input.split(\"\\n\")[:-1]\n\n        params = []\n        for i in range(1, 8):\n            params.append(entities[i].split(\"   \")[1])\n\n        spg = params[0]\n        params = params[1:]\n        lattice = Lattice.from_parameters(\n            a=float(params[0]),\n            b=float(params[1]),\n            c=float(params[2]),\n            alpha=float(params[3]),\n            beta=float(params[4]),\n            gamma=float(params[5]),\n        )\n\n        elements = []\n        m_coord = []\n        atoms = entities[entities.index(\" _atom_site_occupancy\") + 1 :]\n        for atom in atoms:\n            ls = atom.split(\"  \")\n            elements.append(ls[1])\n            m_coord.append([float(ls[4]), float(ls[5]), float(ls[6])])\n\n        # print(atoms)\n\n        return Structure.from_spacegroup(spg, lattice, elements, m_coord)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.DecodeTextRep.cif_string_decoder_p1","title":"<code>cif_string_decoder_p1(input)</code>","text":"<p>Returning a pymatgen structure out of a string format of a cif file.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>str String to obtain the items needed for the structure.</p> required <p>Returns:</p> Type Description <p>pymatgen.core.structure.Structure</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def cif_string_decoder_p1(self, input: str):\n    \"\"\"\n    Returning a pymatgen structure out of a string format of a cif file.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n    Returns:\n        pymatgen.core.structure.Structure\n    \"\"\"\n    entities = input.split(\"\\n\")[:-1]\n\n    params = []\n    for i in range(2, 8):\n        params.append(entities[i].split(\"   \")[1])\n\n    lattice = Lattice.from_parameters(\n        a=float(params[0]),\n        b=float(params[1]),\n        c=float(params[2]),\n        alpha=float(params[3]),\n        beta=float(params[4]),\n        gamma=float(params[5]),\n    )\n\n    elements = []\n    m_coord = []\n    atoms = entities[entities.index(\" _atom_site_occupancy\") + 1 :]\n    for atom in atoms:\n        ls = atom.split(\"  \")\n        elements.append(ls[1])\n        m_coord.append([float(ls[4]), float(ls[5]), float(ls[6])])\n\n    return Structure(lattice, elements, m_coord)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.DecodeTextRep.cif_string_decoder_sym","title":"<code>cif_string_decoder_sym(input)</code>","text":"<p>Returning a pymatgen structure out of a string format of a symmetrized cif file.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>str String to obtain the items needed for the structure.</p> required <p>Returns:</p> Type Description <p>pymatgen.core.structure.Structure</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def cif_string_decoder_sym(self, input: str):\n    \"\"\"\n    Returning a pymatgen structure out of a string format of a symmetrized cif file.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n    Returns:\n        pymatgen.core.structure.Structure\n    \"\"\"\n    entities = input.split(\"\\n\")[:-1]\n\n    params = []\n    for i in range(1, 8):\n        params.append(entities[i].split(\"   \")[1])\n\n    spg = params[0]\n    params = params[1:]\n    lattice = Lattice.from_parameters(\n        a=float(params[0]),\n        b=float(params[1]),\n        c=float(params[2]),\n        alpha=float(params[3]),\n        beta=float(params[4]),\n        gamma=float(params[5]),\n    )\n\n    elements = []\n    m_coord = []\n    atoms = entities[entities.index(\" _atom_site_occupancy\") + 1 :]\n    for atom in atoms:\n        ls = atom.split(\"  \")\n        elements.append(ls[1])\n        m_coord.append([float(ls[4]), float(ls[5]), float(ls[6])])\n\n    # print(atoms)\n\n    return Structure.from_spacegroup(spg, lattice, elements, m_coord)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.DecodeTextRep.llm_decoder","title":"<code>llm_decoder(input)</code>","text":"<p>Returning pymatgen structure out of multi-line representation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>str String to obtain the items needed for the structure.</p> required <p>Returns:</p> Type Description <p>pymatgen.core.structure.Structure</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def llm_decoder(self, input: str):\n    \"\"\"\n    Returning pymatgen structure out of multi-line representation.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n    Returns:\n        pymatgen.core.structure.Structure\n    \"\"\"\n    entities = input.split(\"\\n\")\n    lengths = entities[0].split(\" \")\n    angles = entities[1].split(\" \")\n    lattice = Lattice.from_parameters(\n        a=float(lengths[0]),\n        b=float(lengths[1]),\n        c=float(lengths[2]),\n        alpha=float(angles[0]),\n        beta=float(angles[1]),\n        gamma=float(angles[2]),\n    )\n\n    elements = entities[2::2]\n    coordinates = entities[3::2]\n    m_coord = []\n    for i in coordinates:\n        s = [float(j) for j in i.split(\" \")]\n        m_coord.append(s)\n\n    return Structure(lattice, elements, m_coord)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.DecodeTextRep.wyckoff_decoder","title":"<code>wyckoff_decoder(input, lattice_params=False)</code>","text":"<p>Generating a pymatgen object from the output of the get_wyckoff_rep() method by using... pyxtal package. In this method, all data are extracted from the multi-line string of the... mentioned method. In pyxtal package, a 3D crystal is produced by specifying the dimensions, elements, composition of elements, space group, and sites as wyckoff positions of the elements.</p> <p>Parameters:</p> Name Type Description Default <code>lattice_params</code> <code>bool</code> <p>boolean To specify whether use lattice parameters in generating crystal structure.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pmg_struc</code> <p>pymatgen.core.structure.Structure</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def wyckoff_decoder(self, input: str, lattice_params: bool = False):\n    \"\"\"\n    Generating a pymatgen object from the output of the get_wyckoff_rep() method by using...\n    pyxtal package. In this method, all data are extracted from the multi-line string of the...\n    mentioned method.\n    In pyxtal package, a 3D crystal is produced by specifying the dimensions, elements,\n    composition of elements, space group, and sites as wyckoff positions of the elements.\n\n    Params:\n        lattice_params: boolean\n            To specify whether use lattice parameters in generating crystal structure.\n\n    Returns:\n        pmg_struc: pymatgen.core.structure.Structure\n    \"\"\"\n\n    # Always dimension is 3.\n    dimensions = 3\n\n    entities = input.split(\"\\n\")[:-1]\n    elements = entities[0]\n    spg = int(entities[1])\n    wyckoff_sites = entities[2:]\n    elements = elements.split(\" \")\n\n    atoms = []\n    composition = []\n    for el in elements:\n        atom = el.rstrip(\"0123456789\")\n        number = el[len(atom) :]\n        atoms.append(atom)\n        composition.append(int(number))\n\n    sites = []\n    for atom in atoms:\n        sub_site = []\n        for site in wyckoff_sites:\n            if atom in site:\n                sub_site.append(site.split()[1])\n\n        sites.append(sub_site)\n\n    xtal_struc = pyxtal()\n\n    if lattice_params:\n        a, b, c, alpha, beta, gamma = self.get_lattice_parameters()\n        cell = pyLattice.from_para(\n            float(a), float(b), float(c), float(alpha), float(beta), float(gamma)\n        )\n        xtal_struc.from_random(dimensions, spg, atoms, composition, sites=sites, lattice=cell)\n    else:\n        xtal_struc.from_random(dimensions, spg, atoms, composition, sites=sites)\n\n    pmg_struc = xtal_struc.to_pymatgen()\n\n    return pmg_struc\n</code></pre>"},{"location":"api/#mattext.representations.decoder.MatchRep","title":"<code>MatchRep</code>","text":"Source code in <code>xtal2txt/decoder.py</code> <pre><code>class MatchRep:\n    def __init__(self, textrep, structure):\n        self.text = textrep\n        self.structure = structure\n\n    def wyckoff_matcher(\n        self,\n        ltol=0.2,\n        stol=0.5,\n        angle_tol=5,\n        primitive_cell=True,\n        scale=True,\n        allow_subset=True,\n        attempt_supercell=True,\n        lattice_params: bool = False,\n    ):\n        \"\"\"\n        To check if pymatgen object from the original cif file match with the generated...\n        pymatgen structure from wyckoff_decoder method out of wyckoff representation...\n        using fit() method of StructureMatcher module in pymatgen package.\n\n        Params:\n            StructureMatcher module can be access in below link with its parameters:\n                https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n            lattice_params: bool\n                To specify using lattice parameters in the wyckoff_decoder method.\n\n        Returns:\n            StructureMatcher().fit_anonymous(): bool\n        \"\"\"\n\n        original_struct = self.structure\n\n        # output_struct = self.wyckoff_decoder(input, lattice_params)\n        output_struct = DecodeTextRep(self.text).wyckoff_decoder(self.text, lattice_params=True)\n\n        return StructureMatcher(\n            ltol,\n            stol,\n            angle_tol,\n            primitive_cell,\n            scale,\n            allow_subset,\n            attempt_supercell,\n        ).fit_anonymous(output_struct, original_struct)\n\n    def llm_matcher(\n        self,\n        ltol=0.2,\n        stol=0.5,\n        angle_tol=5,\n        primitive_cell=True,\n        scale=True,\n        allow_subset=True,\n        attempt_supercell=True,\n    ):\n        \"\"\"\n        To check if pymatgen object from the original cif file match with the generated\n        pymatgen structure from llm_decoder method out of llm representation\n        using fit() method of StructureMatcher module in pymatgen package.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n            StructureMatcher module can be access in below link with its parameters:\n                https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n        Returns:\n            StructureMatcher().fit(): bool\n        \"\"\"\n\n        original_struct = self.structure\n        output_struct = DecodeTextRep(self.text).llm_decoder(self.text)\n\n        return StructureMatcher(\n            ltol,\n            stol,\n            angle_tol,\n            primitive_cell,\n            scale,\n            allow_subset,\n            attempt_supercell,\n        ).fit(output_struct, original_struct)\n\n    def cif_string_matcher_sym(\n        self,\n        #        input: str,\n        ltol=0.2,\n        stol=0.5,\n        angle_tol=5,\n        primitive_cell=True,\n        scale=True,\n        allow_subset=True,\n        attempt_supercell=True,\n    ):\n        \"\"\"\n        To check if pymatgen object from the original cif file match with the generated\n        pymatgen structure from cif_string_decoder_sym method out of string cif representation.\n        using fit() method of StructureMatcher module in pymatgen package.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n            StructureMatcher module can be access in below link with its parameters:\n                https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n        Returns:\n            StructureMatcher().fit(): bool\n        \"\"\"\n\n        original_struct = self.structure\n        output_struct = DecodeTextRep(self.text).cif_string_decoder_sym(self.text)\n\n        return StructureMatcher(\n            ltol,\n            stol,\n            angle_tol,\n            primitive_cell,\n            scale,\n            allow_subset,\n            attempt_supercell,\n        ).fit(output_struct, original_struct)\n\n    def cif_string_matcher_p1(\n        self,\n        ltol=0.2,\n        stol=0.5,\n        angle_tol=5,\n        primitive_cell=True,\n        scale=True,\n        allow_subset=True,\n        attempt_supercell=True,\n    ):\n        \"\"\"\n        To check if pymatgen object from the original cif file match with the generated\n        pymatgen structure from cif_string_decoder_p1 method out of string cif representation\n        using fit() method of StructureMatcher module in pymatgen package.\n\n        Params:\n            input: str\n                String to obtain the items needed for the structure.\n\n            StructureMatcher module can be access in below link with its parameters:\n                https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n        Returns:\n            StructureMatcher().fit(): bool\n        \"\"\"\n\n        original_struct = self.structure\n        output_struct = DecodeTextRep(self.text).cif_string_decoder_p1(self.text)\n\n        return StructureMatcher(\n            ltol,\n            stol,\n            angle_tol,\n            primitive_cell,\n            scale,\n            allow_subset,\n            attempt_supercell,\n        ).fit(output_struct, original_struct)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.MatchRep.cif_string_matcher_p1","title":"<code>cif_string_matcher_p1(ltol=0.2, stol=0.5, angle_tol=5, primitive_cell=True, scale=True, allow_subset=True, attempt_supercell=True)</code>","text":"<p>To check if pymatgen object from the original cif file match with the generated pymatgen structure from cif_string_decoder_p1 method out of string cif representation using fit() method of StructureMatcher module in pymatgen package.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>str String to obtain the items needed for the structure.</p> required <code>StructureMatcher</code> <code>module can be access in below link with its parameters</code> <p>https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping</p> required <p>Returns:</p> Name Type Description <code>StructureMatcher</code> <code>).fit(</code> <p>bool</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def cif_string_matcher_p1(\n    self,\n    ltol=0.2,\n    stol=0.5,\n    angle_tol=5,\n    primitive_cell=True,\n    scale=True,\n    allow_subset=True,\n    attempt_supercell=True,\n):\n    \"\"\"\n    To check if pymatgen object from the original cif file match with the generated\n    pymatgen structure from cif_string_decoder_p1 method out of string cif representation\n    using fit() method of StructureMatcher module in pymatgen package.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n        StructureMatcher module can be access in below link with its parameters:\n            https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n    Returns:\n        StructureMatcher().fit(): bool\n    \"\"\"\n\n    original_struct = self.structure\n    output_struct = DecodeTextRep(self.text).cif_string_decoder_p1(self.text)\n\n    return StructureMatcher(\n        ltol,\n        stol,\n        angle_tol,\n        primitive_cell,\n        scale,\n        allow_subset,\n        attempt_supercell,\n    ).fit(output_struct, original_struct)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.MatchRep.cif_string_matcher_sym","title":"<code>cif_string_matcher_sym(ltol=0.2, stol=0.5, angle_tol=5, primitive_cell=True, scale=True, allow_subset=True, attempt_supercell=True)</code>","text":"<p>To check if pymatgen object from the original cif file match with the generated pymatgen structure from cif_string_decoder_sym method out of string cif representation. using fit() method of StructureMatcher module in pymatgen package.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>str String to obtain the items needed for the structure.</p> required <code>StructureMatcher</code> <code>module can be access in below link with its parameters</code> <p>https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping</p> required <p>Returns:</p> Name Type Description <code>StructureMatcher</code> <code>).fit(</code> <p>bool</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def cif_string_matcher_sym(\n    self,\n    #        input: str,\n    ltol=0.2,\n    stol=0.5,\n    angle_tol=5,\n    primitive_cell=True,\n    scale=True,\n    allow_subset=True,\n    attempt_supercell=True,\n):\n    \"\"\"\n    To check if pymatgen object from the original cif file match with the generated\n    pymatgen structure from cif_string_decoder_sym method out of string cif representation.\n    using fit() method of StructureMatcher module in pymatgen package.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n        StructureMatcher module can be access in below link with its parameters:\n            https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n    Returns:\n        StructureMatcher().fit(): bool\n    \"\"\"\n\n    original_struct = self.structure\n    output_struct = DecodeTextRep(self.text).cif_string_decoder_sym(self.text)\n\n    return StructureMatcher(\n        ltol,\n        stol,\n        angle_tol,\n        primitive_cell,\n        scale,\n        allow_subset,\n        attempt_supercell,\n    ).fit(output_struct, original_struct)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.MatchRep.llm_matcher","title":"<code>llm_matcher(ltol=0.2, stol=0.5, angle_tol=5, primitive_cell=True, scale=True, allow_subset=True, attempt_supercell=True)</code>","text":"<p>To check if pymatgen object from the original cif file match with the generated pymatgen structure from llm_decoder method out of llm representation using fit() method of StructureMatcher module in pymatgen package.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <p>str String to obtain the items needed for the structure.</p> required <code>StructureMatcher</code> <code>module can be access in below link with its parameters</code> <p>https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping</p> required <p>Returns:</p> Name Type Description <code>StructureMatcher</code> <code>).fit(</code> <p>bool</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def llm_matcher(\n    self,\n    ltol=0.2,\n    stol=0.5,\n    angle_tol=5,\n    primitive_cell=True,\n    scale=True,\n    allow_subset=True,\n    attempt_supercell=True,\n):\n    \"\"\"\n    To check if pymatgen object from the original cif file match with the generated\n    pymatgen structure from llm_decoder method out of llm representation\n    using fit() method of StructureMatcher module in pymatgen package.\n\n    Params:\n        input: str\n            String to obtain the items needed for the structure.\n\n        StructureMatcher module can be access in below link with its parameters:\n            https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n\n    Returns:\n        StructureMatcher().fit(): bool\n    \"\"\"\n\n    original_struct = self.structure\n    output_struct = DecodeTextRep(self.text).llm_decoder(self.text)\n\n    return StructureMatcher(\n        ltol,\n        stol,\n        angle_tol,\n        primitive_cell,\n        scale,\n        allow_subset,\n        attempt_supercell,\n    ).fit(output_struct, original_struct)\n</code></pre>"},{"location":"api/#mattext.representations.decoder.MatchRep.wyckoff_matcher","title":"<code>wyckoff_matcher(ltol=0.2, stol=0.5, angle_tol=5, primitive_cell=True, scale=True, allow_subset=True, attempt_supercell=True, lattice_params=False)</code>","text":"<p>To check if pymatgen object from the original cif file match with the generated... pymatgen structure from wyckoff_decoder method out of wyckoff representation... using fit() method of StructureMatcher module in pymatgen package.</p> <p>Parameters:</p> Name Type Description Default <code>StructureMatcher</code> <code>module can be access in below link with its parameters</code> <p>https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping</p> required <code>lattice_params</code> <code>bool</code> <p>bool To specify using lattice parameters in the wyckoff_decoder method.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>StructureMatcher</code> <code>).fit_anonymous(</code> <p>bool</p> Source code in <code>xtal2txt/decoder.py</code> <pre><code>def wyckoff_matcher(\n    self,\n    ltol=0.2,\n    stol=0.5,\n    angle_tol=5,\n    primitive_cell=True,\n    scale=True,\n    allow_subset=True,\n    attempt_supercell=True,\n    lattice_params: bool = False,\n):\n    \"\"\"\n    To check if pymatgen object from the original cif file match with the generated...\n    pymatgen structure from wyckoff_decoder method out of wyckoff representation...\n    using fit() method of StructureMatcher module in pymatgen package.\n\n    Params:\n        StructureMatcher module can be access in below link with its parameters:\n            https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.structure_matcher.StructureMatcher.get_mapping\n        lattice_params: bool\n            To specify using lattice parameters in the wyckoff_decoder method.\n\n    Returns:\n        StructureMatcher().fit_anonymous(): bool\n    \"\"\"\n\n    original_struct = self.structure\n\n    # output_struct = self.wyckoff_decoder(input, lattice_params)\n    output_struct = DecodeTextRep(self.text).wyckoff_decoder(self.text, lattice_params=True)\n\n    return StructureMatcher(\n        ltol,\n        stol,\n        angle_tol,\n        primitive_cell,\n        scale,\n        allow_subset,\n        attempt_supercell,\n    ).fit_anonymous(output_struct, original_struct)\n</code></pre>"},{"location":"api/#transformations","title":"Transformations","text":""},{"location":"api/#tokenizer","title":"Tokenizer","text":""},{"location":"api/#mattext.tokenizer.CifTokenizer","title":"<code>CifTokenizer</code>","text":"<p>               Bases: <code>Xtal2txtTokenizer</code></p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class CifTokenizer(Xtal2txtTokenizer):\n    def __init__(\n        self,\n        special_num_token: bool = False,\n        vocab_file=None,\n        model_max_length=None,\n        padding_length=None,\n        **kwargs,\n    ):\n        if special_num_token:\n            vocab_file = CIF_RT_VOCAB\n        else:\n            vocab_file = CIF_VOCAB\n        super(CifTokenizer, self).__init__(\n            special_num_token=special_num_token,\n            vocab_file=vocab_file,\n            model_max_length=model_max_length,\n            padding_length=padding_length,\n            **kwargs,\n        )\n\n    def token_analysis(self, list_of_tokens):\n        \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n        token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n        \"\"\"\n        analysis_masks = ANALYSIS_MASK_TOKENS\n        token_type = CIF_ANALYSIS_DICT\n        return [\n            analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n            for token in list_of_tokens\n        ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.CifTokenizer.token_analysis","title":"<code>token_analysis(list_of_tokens)</code>","text":"<p>Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def token_analysis(self, list_of_tokens):\n    \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n    token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n    \"\"\"\n    analysis_masks = ANALYSIS_MASK_TOKENS\n    token_type = CIF_ANALYSIS_DICT\n    return [\n        analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n        for token in list_of_tokens\n    ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.CompositionTokenizer","title":"<code>CompositionTokenizer</code>","text":"<p>               Bases: <code>Xtal2txtTokenizer</code></p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class CompositionTokenizer(Xtal2txtTokenizer):\n    def __init__(\n        self,\n        special_num_token: bool = False,\n        vocab_file=None,\n        model_max_length=None,\n        padding_length=None,\n        **kwargs,\n    ):\n        if special_num_token:\n            vocab_file = COMPOSITION_RT_VOCAB if vocab_file is None else vocab_file\n        else:\n            vocab_file = COMPOSITION_VOCAB if vocab_file is None else vocab_file\n        super(CompositionTokenizer, self).__init__(\n            special_num_token=special_num_token,\n            vocab_file=vocab_file,\n            model_max_length=model_max_length,\n            padding_length=padding_length,\n            **kwargs,\n        )\n\n    def token_analysis(self, list_of_tokens):\n        \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n        token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n        \"\"\"\n        analysis_masks = ANALYSIS_MASK_TOKENS\n        token_type = COMPOSITION_ANALYSIS_DICT\n        return [\n            analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n            for token in list_of_tokens\n        ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.CompositionTokenizer.token_analysis","title":"<code>token_analysis(list_of_tokens)</code>","text":"<p>Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def token_analysis(self, list_of_tokens):\n    \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n    token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n    \"\"\"\n    analysis_masks = ANALYSIS_MASK_TOKENS\n    token_type = COMPOSITION_ANALYSIS_DICT\n    return [\n        analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n        for token in list_of_tokens\n    ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.CrysllmTokenizer","title":"<code>CrysllmTokenizer</code>","text":"<p>               Bases: <code>Xtal2txtTokenizer</code></p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class CrysllmTokenizer(Xtal2txtTokenizer):\n    def __init__(\n        self,\n        special_num_token: bool = False,\n        vocab_file=CRYSTAL_LLM_VOCAB,\n        model_max_length=None,\n        padding_length=None,\n        **kwargs,\n    ):\n        if special_num_token:\n            vocab_file = CRYSTAL_LLM_RT_VOCAB\n        else:\n            vocab_file = CRYSTAL_LLM_VOCAB\n        super(CrysllmTokenizer, self).__init__(\n            special_num_token=special_num_token,\n            vocab_file=vocab_file,\n            model_max_length=model_max_length,\n            padding_length=padding_length,\n            **kwargs,\n        )\n\n    def token_analysis(self, list_of_tokens):\n        \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n        token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n        \"\"\"\n        analysis_masks = ANALYSIS_MASK_TOKENS\n        token_type = CRYSTAL_LLM_ANALYSIS_DICT\n        return [\n            analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n            for token in list_of_tokens\n        ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.CrysllmTokenizer.token_analysis","title":"<code>token_analysis(list_of_tokens)</code>","text":"<p>Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def token_analysis(self, list_of_tokens):\n    \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n    token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n    \"\"\"\n    analysis_masks = ANALYSIS_MASK_TOKENS\n    token_type = CRYSTAL_LLM_ANALYSIS_DICT\n    return [\n        analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n        for token in list_of_tokens\n    ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer","title":"<code>NumTokenizer</code>","text":"<p>Tokenize numbers as implemented in Regression Transformer. https://www.nature.com/articles/s42256-023-00639-z https://github.com/IBM/regression-transformer/tree/main</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class NumTokenizer:\n    \"\"\"Tokenize numbers as implemented in Regression Transformer.\n    https://www.nature.com/articles/s42256-023-00639-z\n    https://github.com/IBM/regression-transformer/tree/main\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Tokenizer for numbers.\"\"\"\n        self.regex = re.compile(r\"(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n\n    def num_matcher(self, text: str) -&gt; str:\n        \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n        # pattern = re.findall(r'(\\d+\\.\\d+|\\d+)', text)  # This regex captures both whole numbers and decimal numbers\n\n        pattern = (\n            r\"\\d+(?:\\.\\d+)?\"  # Match any number, whether it is part of a string or not\n        )\n        matches = list(re.finditer(pattern, text))\n        for match in reversed(\n            matches\n        ):  # since we are replacing substring with a bigger subtring the string we are working on\n            start, end = match.start(), match.end()\n            tokens = self.tokenize(match.group())\n            replacement = \"\".join(tokens)\n            text = text[:start] + replacement + text[end:]\n        return text\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"Tokenization of numbers as in RT.\n         '0.9' -&gt; '_0_0_', '_._', '_9_-1_'\n\n        Args:\n            text: number as string to be tokenized.\n\n        Returns:\n            extracted tokens.\n        \"\"\"\n        tokens = []\n        matched = self.regex.match(text)\n        if matched:\n            sign, units, dot, decimals = matched.groups()\n            tokens = []\n            if sign:\n                tokens += [f\"_{sign}_\"]\n            tokens += [\n                f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n            ][::-1]\n            if dot:\n                tokens += [f\"_{dot}_\"]\n            if decimals:\n                tokens += [\n                    f\"_{number}_-{position}_\"\n                    for position, number in enumerate(decimals, 1)\n                ]\n        return tokens\n\n    @staticmethod\n    def convert_tokens_to_float(tokens: List[str]) -&gt; float:\n        \"\"\"Converts tokens representing a float value into a float.\n        NOTE: Expects that non-floating tokens are strippped off\n\n        Args:\n            tokens: List of tokens, each representing a float.\n                E.g.: ['_0_0_', '_._', '_9_-1_', '_3_-2_', '_1_-3_']\n\n        Returns:\n            float: Float representation for the list of tokens.\n        \"\"\"\n        try:\n            float_string = \"\".join([token.split(\"_\")[1] for token in tokens])\n            float_value = float(float_string)\n        except ValueError:\n            float_value = -1\n        return float_value\n\n    def convert_tokens_to_string(self, tokens: List[str]) -&gt; str:\n        \"\"\"Converts tokens to string.\n\n        Args:\n            tokens: List of tokens.\n\n        Returns:\n            str: String representation of the tokens.\n        \"\"\"\n        return \"\".join([token.split(\"_\")[1] for token in tokens])\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer.__init__","title":"<code>__init__()</code>","text":"<p>Tokenizer for numbers.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Tokenizer for numbers.\"\"\"\n    self.regex = re.compile(r\"(\\+|-)?(\\d+)(\\.)?(\\d+)?\\s*\")\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer.convert_tokens_to_float","title":"<code>convert_tokens_to_float(tokens)</code>  <code>staticmethod</code>","text":"<p>Converts tokens representing a float value into a float. NOTE: Expects that non-floating tokens are strippped off</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens, each representing a float. E.g.: ['0_0', '.', '9-1_', '3-2_', '1-3_']</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Float representation for the list of tokens.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>@staticmethod\ndef convert_tokens_to_float(tokens: List[str]) -&gt; float:\n    \"\"\"Converts tokens representing a float value into a float.\n    NOTE: Expects that non-floating tokens are strippped off\n\n    Args:\n        tokens: List of tokens, each representing a float.\n            E.g.: ['_0_0_', '_._', '_9_-1_', '_3_-2_', '_1_-3_']\n\n    Returns:\n        float: Float representation for the list of tokens.\n    \"\"\"\n    try:\n        float_string = \"\".join([token.split(\"_\")[1] for token in tokens])\n        float_value = float(float_string)\n    except ValueError:\n        float_value = -1\n    return float_value\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer.convert_tokens_to_string","title":"<code>convert_tokens_to_string(tokens)</code>","text":"<p>Converts tokens to string.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the tokens.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def convert_tokens_to_string(self, tokens: List[str]) -&gt; str:\n    \"\"\"Converts tokens to string.\n\n    Args:\n        tokens: List of tokens.\n\n    Returns:\n        str: String representation of the tokens.\n    \"\"\"\n    return \"\".join([token.split(\"_\")[1] for token in tokens])\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer.num_matcher","title":"<code>num_matcher(text)</code>","text":"<p>Extract numbers from a sentence and replace them with tokens.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def num_matcher(self, text: str) -&gt; str:\n    \"\"\"Extract numbers from a sentence and replace them with tokens.\"\"\"\n    # pattern = re.findall(r'(\\d+\\.\\d+|\\d+)', text)  # This regex captures both whole numbers and decimal numbers\n\n    pattern = (\n        r\"\\d+(?:\\.\\d+)?\"  # Match any number, whether it is part of a string or not\n    )\n    matches = list(re.finditer(pattern, text))\n    for match in reversed(\n        matches\n    ):  # since we are replacing substring with a bigger subtring the string we are working on\n        start, end = match.start(), match.end()\n        tokens = self.tokenize(match.group())\n        replacement = \"\".join(tokens)\n        text = text[:start] + replacement + text[end:]\n    return text\n</code></pre>"},{"location":"api/#mattext.tokenizer.NumTokenizer.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenization of numbers as in RT.  '0.9' -&gt; '0_0', '.', '9-1_'</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>number as string to be tokenized.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>extracted tokens.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; List[str]:\n    \"\"\"Tokenization of numbers as in RT.\n     '0.9' -&gt; '_0_0_', '_._', '_9_-1_'\n\n    Args:\n        text: number as string to be tokenized.\n\n    Returns:\n        extracted tokens.\n    \"\"\"\n    tokens = []\n    matched = self.regex.match(text)\n    if matched:\n        sign, units, dot, decimals = matched.groups()\n        tokens = []\n        if sign:\n            tokens += [f\"_{sign}_\"]\n        tokens += [\n            f\"_{number}_{position}_\" for position, number in enumerate(units[::-1])\n        ][::-1]\n        if dot:\n            tokens += [f\"_{dot}_\"]\n        if decimals:\n            tokens += [\n                f\"_{number}_-{position}_\"\n                for position, number in enumerate(decimals, 1)\n            ]\n    return tokens\n</code></pre>"},{"location":"api/#mattext.tokenizer.RobocrysTokenizer","title":"<code>RobocrysTokenizer</code>","text":"<p>Tokenizer for Robocrystallographer. Would be BPE tokenizer. trained on the Robocrystallographer dataset. TODO: Implement this tokenizer.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class RobocrysTokenizer:\n    \"\"\"Tokenizer for Robocrystallographer. Would be BPE tokenizer.\n    trained on the Robocrystallographer dataset.\n    TODO: Implement this tokenizer.\n    \"\"\"\n\n    def __init__(self, vocab_file=ROBOCRYS_VOCAB, **kwargs):\n        tokenizer = Tokenizer.from_file(vocab_file)\n        wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n        self._tokenizer = wrapped_tokenizer\n\n    def tokenize(self, text):\n        return self._tokenizer.tokenize(text)\n\n    def encode(self, text):\n        return self._tokenizer.encode(text)\n\n    def decode(self, token_ids, skip_special_tokens=True):\n        # Check if token_ids is a string and convert it to a list of integers\n        if isinstance(token_ids, str):\n            token_ids = [int(token_ids)]\n        return self._tokenizer.decode(\n            token_ids, skip_special_tokens=skip_special_tokens\n        )\n</code></pre>"},{"location":"api/#mattext.tokenizer.SliceTokenizer","title":"<code>SliceTokenizer</code>","text":"<p>               Bases: <code>Xtal2txtTokenizer</code></p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>class SliceTokenizer(Xtal2txtTokenizer):\n    def __init__(\n        self,\n        special_num_token: bool = False,\n        vocab_file=None,\n        model_max_length=None,\n        padding_length=None,\n        **kwargs,\n    ):\n        if special_num_token:\n            vocab_file = SLICE_RT_VOCAB if vocab_file is None else vocab_file\n        else:\n            vocab_file = SLICE_VOCAB if vocab_file is None else vocab_file\n        super(SliceTokenizer, self).__init__(\n            special_num_token=special_num_token,\n            vocab_file=vocab_file,\n            model_max_length=model_max_length,\n            padding_length=padding_length,\n            **kwargs,\n        )\n\n    def convert_tokens_to_string(self, tokens):\n        \"\"\"Converts tokens to string.\"\"\"\n        if self.special_num_tokens:\n            return \" \".join(\n                [\n                    (\n                        token\n                        if not (token.startswith(\"_\") and token.endswith(\"_\"))\n                        else token.split(\"_\")[1]\n                    )\n                    for token in tokens\n                ]\n            )\n        return \" \".join(tokens).rstrip()\n\n    def token_analysis(self, list_of_tokens):\n        \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n        token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n        \"\"\"\n        analysis_masks = ANALYSIS_MASK_TOKENS\n        token_type = SLICE_ANALYSIS_DICT\n        return [\n            analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n            for token in list_of_tokens\n        ]\n</code></pre>"},{"location":"api/#mattext.tokenizer.SliceTokenizer.convert_tokens_to_string","title":"<code>convert_tokens_to_string(tokens)</code>","text":"<p>Converts tokens to string.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def convert_tokens_to_string(self, tokens):\n    \"\"\"Converts tokens to string.\"\"\"\n    if self.special_num_tokens:\n        return \" \".join(\n            [\n                (\n                    token\n                    if not (token.startswith(\"_\") and token.endswith(\"_\"))\n                    else token.split(\"_\")[1]\n                )\n                for token in tokens\n            ]\n        )\n    return \" \".join(tokens).rstrip()\n</code></pre>"},{"location":"api/#mattext.tokenizer.SliceTokenizer.token_analysis","title":"<code>token_analysis(list_of_tokens)</code>","text":"<p>Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.</p> Source code in <code>xtal2txt/tokenizer.py</code> <pre><code>def token_analysis(self, list_of_tokens):\n    \"\"\"Takes tokens after tokenize and returns a list with replacing the tokens with their MASK token. The\n    token type is determined from the dict declared globally, and the token is replaced with the corresponding MASK token.\n    \"\"\"\n    analysis_masks = ANALYSIS_MASK_TOKENS\n    token_type = SLICE_ANALYSIS_DICT\n    return [\n        analysis_masks[next((k for k, v in token_type.items() if token in v), None)]\n        for token in list_of_tokens\n    ]\n</code></pre>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#mattext.models.llama_sft.FinetuneLLamaSFT","title":"<code>FinetuneLLamaSFT</code>","text":"<p>Class to perform finetuning of a language model.     Initialize the FinetuneModel.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration for the fine-tuning.</p> required <code>local_rank</code> <code>int</code> <p>Local rank for distributed training. Defaults to None.</p> <code>None</code> Source code in <code>src/mattext/models/llama_sft.py</code> <pre><code>class FinetuneLLamaSFT:\n    \"\"\"Class to perform finetuning of a language model.\n        Initialize the FinetuneModel.\n\n    Args:\n        cfg (DictConfig): Configuration for the fine-tuning.\n        local_rank (int, optional): Local rank for distributed training. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self, cfg: DictConfig, local_rank=None, fold=\"fold_0\", test_sample_size=None\n    ) -&gt; None:\n        self.fold = fold\n        self.local_rank = local_rank\n        self.representation = cfg.model.representation\n        self.data_repository = cfg.model.data_repository\n        self.dataset_ = cfg.model.dataset\n        self.add_special_tokens = cfg.model.add_special_tokens\n        self.property_map = cfg.model.PROPERTY_MAP\n        self.material_map = cfg.model.MATERIAL_MAP\n        self.cfg = cfg.model.finetune\n        self.train_data = self.cfg.dataset_name\n        self.test_data = self.cfg.benchmark_dataset\n        self.context_length: int = self.cfg.context_length\n        self.dataprep_seed: int = self.cfg.dataprep_seed\n        self.callbacks = self.cfg.callbacks\n        self.ckpt = self.cfg.path.pretrained_checkpoint\n        self.bnb_config = self.cfg.bnb_config\n        self.dataset = self.prepare_data(self.train_data)\n        self.testdata = self.prepare_test_data(self.test_data)\n        self.model, self.tokenizer, self.peft_config = self._setup_model_tokenizer()\n        self.property_ = self.property_map[self.dataset_]\n        self.material_ = self.material_map[self.dataset_]\n        self.test_sample_size = test_sample_size\n\n    def prepare_test_data(self, subset):\n        dataset = load_dataset(self.data_repository, subset)[self.fold]\n        if self.test_sample_size:\n            dataset = dataset.select(range(self.test_sample_size))\n        return dataset\n\n    def prepare_data(self, subset):\n        dataset = load_dataset(self.data_repository, subset)\n        dataset = dataset.shuffle(seed=self.dataprep_seed)[self.fold]\n        return dataset.train_test_split(test_size=0.1, seed=self.dataprep_seed)\n\n    def _setup_model_tokenizer(self) -&gt; None:\n        # device_string = PartialState().process_index\n        # compute_dtype = getattr(torch, \"float16\")\n\n        if self.bnb_config.use_4bit and self.bnb_config.use_8bit:\n            raise ValueError(\n                \"You can't load the model in 8 bits and 4 bits at the same time\"\n            )\n\n        elif self.bnb_config.use_4bit or self.bnb_config.use_8bit:\n            compute_dtype = getattr(torch, self.bnb_config.bnb_4bit_compute_dtype)\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=self.bnb_config.use_4bit,\n                load_in_8bit=self.bnb_config.use_8bit,\n                bnb_4bit_quant_type=self.bnb_config.bnb_4bit_quant_type,\n                bnb_4bit_compute_dtype=compute_dtype,\n                bnb_4bit_use_double_quant=self.bnb_config.use_nested_quant,\n            )\n        else:\n            bnb_config = None\n\n        # Check GPU compatibility with bfloat16\n        if compute_dtype == torch.float16:\n            major, _ = torch.cuda.get_device_capability()\n            if major &gt;= 8:\n                logger.info(\n                    \"Your GPU supports bfloat16: accelerate training with bf16=True!\"\n                )\n\n        # LoRA config\n        peft_config = LoraConfig(**self.cfg.lora_config)\n\n        tokenizer = AutoTokenizer.from_pretrained(\n            self.ckpt,\n            use_fast=False,\n        )\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n\n        model = AutoModelForCausalLM.from_pretrained(\n            self.ckpt,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n        )\n\n        return model, tokenizer, peft_config\n\n    def formatting_prompts_func(self, example):\n        output_texts = []\n        for i in range(len(example[self.representation])):\n            text = f\"### What is the {self.property_} of {example[self.representation][i]}\\n ### Answer: {example['labels'][i]:.3f}@@@\"\n            output_texts.append(text)\n        return output_texts\n\n    def formatting_tests_func(self, example):\n        output_texts = []\n        for i in range(len(example[self.representation])):\n            text = f\"### What is the {self.property_} of {example[self.representation][i]}\\n \"\n            output_texts.append(text)\n        return output_texts\n\n    def _callbacks(self) -&gt; List[TrainerCallback]:\n        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n        callbacks = []\n\n        if self.callbacks.early_stopping:\n            callbacks.append(\n                EarlyStoppingCallback(\n                    early_stopping_patience=self.callbacks.early_stopping_patience,\n                    early_stopping_threshold=self.callbacks.early_stopping_threshold,\n                )\n            )\n        callbacks.append(EvaluateFirstStepCallback)\n        return callbacks\n\n    def finetune(self) -&gt; None:\n        \"\"\"\n        Perform fine-tuning of the language model.\n        \"\"\"\n\n        config_train_args = self.cfg.training_arguments\n        training_args = TrainingArguments(\n            **config_train_args,\n        )\n        callbacks = self._callbacks()\n\n        response_template = \" ### Answer:\"\n        collator = DataCollatorForCompletionOnlyLM(\n            response_template, tokenizer=self.tokenizer\n        )\n\n        packing = False\n        max_seq_length = None\n        if self.representation == \"cif_p1\":\n            max_seq_length = 2048\n\n        trainer = SFTTrainer(\n            model=self.model,\n            peft_config=self.peft_config,\n            train_dataset=self.dataset[\"train\"],\n            eval_dataset=self.dataset[\"test\"],\n            formatting_func=self.formatting_prompts_func,\n            data_collator=collator,\n            max_seq_length=max_seq_length,\n            tokenizer=self.tokenizer,\n            args=training_args,\n            packing=packing,\n            callbacks=callbacks,\n        )\n\n        wandb.log({\"Training Arguments\": str(config_train_args)})\n        wandb.log({\"model_summary\": str(self.model)})\n\n        self.output_dir_ = (\n            f\"{self.cfg.path.finetuned_modelname}/llamav3-8b-lora-fine-tune\"\n        )\n        trainer.train()\n\n        pipe = pipeline(\n            \"text-generation\",\n            model=trainer.model,\n            tokenizer=self.tokenizer,\n            return_full_text=False,\n            do_sample=False,\n            max_new_tokens=4,\n        )\n        with torch.cuda.amp.autocast():\n            pred = pipe(self.formatting_tests_func(self.testdata))\n        logger.debug(\"Prediction: %s\", pred)\n\n        with open(\n            f\"{self.cfg.path.finetuned_modelname}_{self.fold}_predictions.json\", \"w\"\n        ) as json_file:\n            json.dump(pred, json_file)\n\n        trainer.save_state()\n        trainer.save_model(self.output_dir_)\n\n        # # Merge LoRA and base model\n        # merged_model = trainer.model.merge_and_unload()\n        # # Save the merged model\n        # merged_model.save_pretrained(\n        #     f\"{self.cfg.path.finetuned_modelname}_{self.fold}/llamav3-8b-lora-save-pretrained\",\n        #     save_config=True,\n        #     safe_serialization=True,\n        # )\n        self.tokenizer.save_pretrained(\n            f\"{self.cfg.path.finetuned_modelname}_{self.fold}/llamav3-8b-lora-save-pretrained\"\n        )\n\n        with torch.cuda.amp.autocast():\n            merge_pred = pipe(self.formatting_tests_func(self.testdata))\n        logger.debug(\"Prediction: %s\", merge_pred)\n\n        with open(\n            f\"{self.cfg.path.finetuned_modelname}__{self.fold}_predictions_merged.json\",\n            \"w\",\n        ) as json_file:\n            json.dump(merge_pred, json_file)\n\n        # Empty VRAM\n        del trainer\n        del collator\n        del pipe\n        del self.model\n        del self.tokenizer\n        import gc\n\n        gc.collect()\n        gc.collect()\n        wandb.finish()\n        return self.cfg.path.finetuned_modelname\n</code></pre>"},{"location":"api/#mattext.models.llama_sft.FinetuneLLamaSFT.finetune","title":"<code>finetune()</code>","text":"<p>Perform fine-tuning of the language model.</p> Source code in <code>src/mattext/models/llama_sft.py</code> <pre><code>def finetune(self) -&gt; None:\n    \"\"\"\n    Perform fine-tuning of the language model.\n    \"\"\"\n\n    config_train_args = self.cfg.training_arguments\n    training_args = TrainingArguments(\n        **config_train_args,\n    )\n    callbacks = self._callbacks()\n\n    response_template = \" ### Answer:\"\n    collator = DataCollatorForCompletionOnlyLM(\n        response_template, tokenizer=self.tokenizer\n    )\n\n    packing = False\n    max_seq_length = None\n    if self.representation == \"cif_p1\":\n        max_seq_length = 2048\n\n    trainer = SFTTrainer(\n        model=self.model,\n        peft_config=self.peft_config,\n        train_dataset=self.dataset[\"train\"],\n        eval_dataset=self.dataset[\"test\"],\n        formatting_func=self.formatting_prompts_func,\n        data_collator=collator,\n        max_seq_length=max_seq_length,\n        tokenizer=self.tokenizer,\n        args=training_args,\n        packing=packing,\n        callbacks=callbacks,\n    )\n\n    wandb.log({\"Training Arguments\": str(config_train_args)})\n    wandb.log({\"model_summary\": str(self.model)})\n\n    self.output_dir_ = (\n        f\"{self.cfg.path.finetuned_modelname}/llamav3-8b-lora-fine-tune\"\n    )\n    trainer.train()\n\n    pipe = pipeline(\n        \"text-generation\",\n        model=trainer.model,\n        tokenizer=self.tokenizer,\n        return_full_text=False,\n        do_sample=False,\n        max_new_tokens=4,\n    )\n    with torch.cuda.amp.autocast():\n        pred = pipe(self.formatting_tests_func(self.testdata))\n    logger.debug(\"Prediction: %s\", pred)\n\n    with open(\n        f\"{self.cfg.path.finetuned_modelname}_{self.fold}_predictions.json\", \"w\"\n    ) as json_file:\n        json.dump(pred, json_file)\n\n    trainer.save_state()\n    trainer.save_model(self.output_dir_)\n\n    # # Merge LoRA and base model\n    # merged_model = trainer.model.merge_and_unload()\n    # # Save the merged model\n    # merged_model.save_pretrained(\n    #     f\"{self.cfg.path.finetuned_modelname}_{self.fold}/llamav3-8b-lora-save-pretrained\",\n    #     save_config=True,\n    #     safe_serialization=True,\n    # )\n    self.tokenizer.save_pretrained(\n        f\"{self.cfg.path.finetuned_modelname}_{self.fold}/llamav3-8b-lora-save-pretrained\"\n    )\n\n    with torch.cuda.amp.autocast():\n        merge_pred = pipe(self.formatting_tests_func(self.testdata))\n    logger.debug(\"Prediction: %s\", merge_pred)\n\n    with open(\n        f\"{self.cfg.path.finetuned_modelname}__{self.fold}_predictions_merged.json\",\n        \"w\",\n    ) as json_file:\n        json.dump(merge_pred, json_file)\n\n    # Empty VRAM\n    del trainer\n    del collator\n    del pipe\n    del self.model\n    del self.tokenizer\n    import gc\n\n    gc.collect()\n    gc.collect()\n    wandb.finish()\n    return self.cfg.path.finetuned_modelname\n</code></pre>"},{"location":"api/#mattext.models.llama.FinetuneLLama","title":"<code>FinetuneLLama</code>","text":"<p>Class to perform finetuning of LLama using a regression head.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration for the fine-tuning.</p> required <code>local_rank</code> <code>int</code> <p>Local rank for distributed training. Defaults to None.</p> <code>None</code> Source code in <code>src/mattext/models/llama.py</code> <pre><code>class FinetuneLLama:\n    \"\"\"Class to perform finetuning of LLama using\n    a regression head.\n\n    Args:\n        cfg (DictConfig): Configuration for the fine-tuning.\n        local_rank (int, optional): Local rank for distributed training. Defaults to None.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig, local_rank=None) -&gt; None:\n        self.local_rank = local_rank\n        self.representation = cfg.model.representation\n        self.cfg = cfg.model.finetune\n        self.context_length: int = self.cfg.context_length\n        self.callbacks = self.cfg.callbacks\n        self.ckpt = self.cfg.path.pretrained_checkpoint\n        self.bnb_config = self.cfg.bnb_config\n        self.model, self.tokenizer = self._setup_model_tokenizer()\n        self.tokenized_dataset = self._prepare_datasets(\n            self.cfg.path.finetune_traindata\n        )\n\n    def _setup_model_tokenizer(self) -&gt; None:\n        llama_tokenizer = LlamaTokenizer.from_pretrained(\n            self.ckpt,\n            model_max_length=MAX_LENGTH,\n            padding_side=\"right\",\n            use_fast=False,\n        )\n\n        if self.bnb_config.use_4bit and self.bnb_config.use_8bit:\n            raise ValueError(\n                \"You can't load the model in 8 bits and 4 bits at the same time\"\n            )\n\n        elif self.bnb_config.use_4bit or self.bnb_config.use_8bit:\n            compute_dtype = getattr(torch, self.bnb_config.bnb_4bit_compute_dtype)\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=self.bnb_config.use_4bit,\n                load_in_8bit=self.bnb_config.use_8bit,\n                bnb_4bit_quant_type=self.bnb_config.bnb_4bit_quant_type,\n                bnb_4bit_compute_dtype=compute_dtype,\n                bnb_4bit_use_double_quant=self.bnb_config.use_nested_quant,\n            )\n        else:\n            bnb_config = None\n\n        # Check GPU compatibility with bfloat16\n        if compute_dtype == torch.float16:\n            major, _ = torch.cuda.get_device_capability()\n            if major &gt;= 8:\n                print(\"=\" * 80)\n                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n                print(\"=\" * 80)\n\n        device_map = {\"\": 0}\n        model = LlamaForSequenceClassification.from_pretrained(\n            self.ckpt,\n            num_labels=1,\n            quantization_config=bnb_config,\n            device_map=device_map,\n        )\n\n        lora_config = LoraConfig(**self.cfg.lora_config)\n        model = get_peft_model(model, lora_config)\n        model.print_trainable_parameters()\n\n        special_tokens_dict = dict()\n        if llama_tokenizer.pad_token is None:\n            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n        if llama_tokenizer.eos_token is None:\n            special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n        if llama_tokenizer.bos_token is None:\n            special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n        if llama_tokenizer.unk_token is None:\n            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=special_tokens_dict,\n            llama_tokenizer=llama_tokenizer,\n            model=model,\n        )\n\n        print(len(llama_tokenizer))\n        return model, llama_tokenizer\n\n    def _tokenize(self, examples):\n        tokenized_examples = self.tokenizer(\n            examples[self.representation],\n            truncation=True,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        return tokenized_examples\n\n    def _prepare_datasets(self, path: str) -&gt; DatasetDict:\n        \"\"\"\n        Prepare training and validation datasets.\n\n        Args:\n           path (Union[str, Path]): Path to json file containing the data\n\n        Returns:\n            DatasetDict: Dictionary containing training and validation datasets.\n        \"\"\"\n\n        ds = load_dataset(\"json\", data_files=path, split=\"train\")\n        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n        return dataset.map(self._tokenize, batched=True)\n\n    def _callbacks(self) -&gt; List[TrainerCallback]:\n        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n        callbacks = []\n\n        if self.callbacks.early_stopping:\n            callbacks.append(\n                EarlyStoppingCallback(\n                    early_stopping_patience=self.callbacks.early_stopping_patience,\n                    early_stopping_threshold=self.callbacks.early_stopping_threshold,\n                )\n            )\n\n        if self.callbacks.custom_logger:\n            callbacks.append(CustomWandbCallback_FineTune())\n\n        callbacks.append(EvaluateFirstStepCallback)\n\n        return callbacks\n\n    def _compute_metrics(self, p: Any, eval=True) -&gt; Dict[str, float]:\n        preds = torch.tensor(\n            p.predictions.squeeze()\n        )  # Convert predictions to PyTorch tensor\n        label_ids = torch.tensor(p.label_ids)  # Convert label_ids to PyTorch tensor\n\n        if eval:\n            # Calculate RMSE as evaluation metric\n            eval_rmse = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n            return {\"eval_rmse\": round(eval_rmse, 3)}\n        else:\n            # Calculate RMSE as training metric\n            loss = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n            return {\"train_rmse\": round(loss, 3), \"loss\": round(loss, 3)}\n\n    def finetune(self) -&gt; None:\n        \"\"\"\n        Perform fine-tuning of the language model.\n        \"\"\"\n\n        config_train_args = self.cfg.training_arguments\n        callbacks = self._callbacks()\n\n        # os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n        training_args = TrainingArguments(\n            **config_train_args,\n            metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n            greater_is_better=False,  # Lower eval_rmse is better\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            data_collator=None,\n            compute_metrics=self._compute_metrics,\n            tokenizer=self.tokenizer,\n            train_dataset=self.tokenized_dataset[\"train\"],\n            eval_dataset=self.tokenized_dataset[\"test\"],\n            callbacks=callbacks,\n        )\n\n        wandb.log({\"Training Arguments\": str(config_train_args)})\n        wandb.log({\"model_summary\": str(self.model)})\n\n        trainer.train()\n        trainer.save_model(\n            f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\"\n        )\n\n        eval_result = trainer.evaluate(eval_dataset=self.tokenized_dataset[\"test\"])\n        wandb.log(eval_result)\n\n        self.model.save_pretrained(self.cfg.path.finetuned_modelname)\n        wandb.finish()\n        return self.cfg.path.finetuned_modelname\n\n    def evaluate(self):\n        \"\"\"\n        Evaluate the fine-tuned model on the test dataset.\n        \"\"\"\n        ckpt = self.finetune()\n</code></pre>"},{"location":"api/#mattext.models.llama.FinetuneLLama.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the fine-tuned model on the test dataset.</p> Source code in <code>src/mattext/models/llama.py</code> <pre><code>def evaluate(self):\n    \"\"\"\n    Evaluate the fine-tuned model on the test dataset.\n    \"\"\"\n    ckpt = self.finetune()\n</code></pre>"},{"location":"api/#mattext.models.llama.FinetuneLLama.finetune","title":"<code>finetune()</code>","text":"<p>Perform fine-tuning of the language model.</p> Source code in <code>src/mattext/models/llama.py</code> <pre><code>def finetune(self) -&gt; None:\n    \"\"\"\n    Perform fine-tuning of the language model.\n    \"\"\"\n\n    config_train_args = self.cfg.training_arguments\n    callbacks = self._callbacks()\n\n    # os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n    training_args = TrainingArguments(\n        **config_train_args,\n        metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n        greater_is_better=False,  # Lower eval_rmse is better\n    )\n\n    trainer = Trainer(\n        model=self.model,\n        args=training_args,\n        data_collator=None,\n        compute_metrics=self._compute_metrics,\n        tokenizer=self.tokenizer,\n        train_dataset=self.tokenized_dataset[\"train\"],\n        eval_dataset=self.tokenized_dataset[\"test\"],\n        callbacks=callbacks,\n    )\n\n    wandb.log({\"Training Arguments\": str(config_train_args)})\n    wandb.log({\"model_summary\": str(self.model)})\n\n    trainer.train()\n    trainer.save_model(\n        f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\"\n    )\n\n    eval_result = trainer.evaluate(eval_dataset=self.tokenized_dataset[\"test\"])\n    wandb.log(eval_result)\n\n    self.model.save_pretrained(self.cfg.path.finetuned_modelname)\n    wandb.finish()\n    return self.cfg.path.finetuned_modelname\n</code></pre>"},{"location":"api/#mattext.models.llama.smart_tokenizer_and_embedding_resize","title":"<code>smart_tokenizer_and_embedding_resize(special_tokens_dict, llama_tokenizer, model)</code>","text":"<p>Resize tokenizer and embedding.</p> <p>Note: This is the unoptimized version that may make your embedding size not be divisible by 64.</p> Source code in <code>src/mattext/models/llama.py</code> <pre><code>def smart_tokenizer_and_embedding_resize(\n    special_tokens_dict,\n    llama_tokenizer,\n    model,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n    llama_tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(llama_tokenizer), pad_to_multiple_of=8)\n\n    if num_new_tokens &gt; 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        #   output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n        #   output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n\n    model.config.pad_token_id = llama_tokenizer.pad_token_id\n</code></pre>"},{"location":"api/#mattext.models.potential.PotentialModel","title":"<code>PotentialModel</code>","text":"<p>               Bases: <code>TokenizerMixin</code></p> <p>Class to perform finetuning of a language model on the hypothetical potential task.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration for the fine-tuning.</p> required <code>local_rank</code> <code>int</code> <p>Local rank for distributed training. Defaults to None.</p> <code>None</code> Source code in <code>src/mattext/models/potential.py</code> <pre><code>class PotentialModel(TokenizerMixin):\n    \"\"\"Class to perform finetuning of a language model on\n    the hypothetical potential task.\n\n    Args:\n        cfg (DictConfig): Configuration for the fine-tuning.\n        local_rank (int, optional): Local rank for distributed training. Defaults to None.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig, local_rank=None) -&gt; None:\n        super().__init__(\n            cfg=cfg.model.representation,\n            special_tokens=cfg.model.special_tokens,\n            special_num_token=cfg.model.special_num_token,\n        )\n        self.local_rank = local_rank\n        self.representation = cfg.model.representation\n        self.alpha = cfg.model.alpha\n        self.test_data = cfg.model.inference.path.test_data\n        self.cfg = cfg.model.finetune\n        self.context_length: int = self.cfg.context_length\n        self.callbacks = self.cfg.callbacks\n        self.tokenized_dataset = self._prepare_datasets(\n            self.cfg.path.finetune_traindata, split=\"train\"\n        )\n        self.tokenized_testset = self._prepare_datasets(self.test_data, split=\"test\")\n\n    def _prepare_datasets(self, path: str, split) -&gt; DatasetDict:\n        \"\"\"\n        Prepare training and validation datasets.\n\n        Args:\n            train_df (pd.DataFrame): DataFrame containing training data.\n\n        Returns:\n            DatasetDict: Dictionary containing training and validation datasets.\n        \"\"\"\n\n        ds = load_dataset(\"json\", data_files=path, split=\"train\")\n        # with contextlib.suppress(KeyError):\n        #     ds = ds.remove_columns(\"labels\")\n        if split == \"train\":\n            ds = ds.remove_columns(\"labels\")\n        else:\n            print(\"test set\")\n\n        labal_name = f\"total_energy_alpha_{self.alpha}\"\n        ds = ds.rename_column(labal_name, \"labels\")\n        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n        # dataset= dataset.filter(lambda example: example[self.representation] is not None)\n        return dataset.map(\n            partial(\n                self._tokenize_pad_and_truncate, context_length=self.context_length\n            ),\n            batched=True,\n        )\n\n    def _callbacks(self) -&gt; List[TrainerCallback]:\n        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n        callbacks = []\n\n        if self.callbacks.early_stopping:\n            callbacks.append(\n                EarlyStoppingCallback(\n                    early_stopping_patience=self.callbacks.early_stopping_patience,\n                    early_stopping_threshold=self.callbacks.early_stopping_threshold,\n                )\n            )\n\n        if self.callbacks.custom_logger:\n            callbacks.append(CustomWandbCallback_FineTune())\n\n        callbacks.append(EvaluateFirstStepCallback)\n\n        return callbacks\n\n    def _compute_metrics(self, p: Any, eval=True) -&gt; Dict[str, float]:\n        preds = torch.tensor(\n            p.predictions.squeeze()\n        )  # Convert predictions to PyTorch tensor\n        label_ids = torch.tensor(p.label_ids)  # Convert label_ids to PyTorch tensor\n\n        if eval:\n            # Calculate RMSE as evaluation metric\n            eval_rmse = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n            return {\"eval_rmse\": round(eval_rmse, 3)}\n        else:\n            # Calculate RMSE as training metric\n            loss = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n            return {\"train_rmse\": round(loss, 3), \"loss\": round(loss, 3)}\n\n    def finetune(self) -&gt; None:\n        \"\"\"\n        Perform fine-tuning of the language model.\n        \"\"\"\n\n        pretrained_ckpt = self.cfg.path.pretrained_checkpoint\n\n        config_train_args = self.cfg.training_arguments\n        callbacks = self._callbacks()\n\n        training_args = TrainingArguments(\n            **config_train_args,\n            metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n            greater_is_better=False,  # Lower eval_rmse is better\n        )\n\n        model = AutoModelForSequenceClassification.from_pretrained(\n            pretrained_ckpt, num_labels=1, ignore_mismatched_sizes=False\n        )\n\n        if self.cfg.freeze_base_model:\n            for param in model.base_model.parameters():\n                param.requires_grad = False\n\n        if self.local_rank is not None:\n            model = model.to(self.local_rank)\n            model = nn.parallel.DistributedDataParallel(\n                model, device_ids=[self.local_rank]\n            )\n        else:\n            model = model.to(\"cuda\")\n\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            data_collator=None,\n            compute_metrics=self._compute_metrics,\n            tokenizer=self._wrapped_tokenizer,\n            train_dataset=self.tokenized_dataset[\"train\"],\n            eval_dataset=self.tokenized_dataset[\"test\"],\n            callbacks=callbacks,\n        )\n\n        wandb.log({\"Training Arguments\": str(config_train_args)})\n        wandb.log({\"model_summary\": str(model)})\n\n        trainer.train()\n        model.save_pretrained(self.cfg.path.finetuned_modelname)\n\n        eval_result = trainer.evaluate(eval_dataset=self.tokenized_testset)\n        wandb.log(eval_result)\n        wandb.finish()\n        return self.cfg.path.finetuned_modelname\n\n    def evaluate(self):\n        \"\"\"\n        Evaluate the fine-tuned model on the test dataset.\n        \"\"\"\n        ckpt = self.finetune()\n</code></pre>"},{"location":"api/#mattext.models.potential.PotentialModel.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate the fine-tuned model on the test dataset.</p> Source code in <code>src/mattext/models/potential.py</code> <pre><code>def evaluate(self):\n    \"\"\"\n    Evaluate the fine-tuned model on the test dataset.\n    \"\"\"\n    ckpt = self.finetune()\n</code></pre>"},{"location":"api/#mattext.models.potential.PotentialModel.finetune","title":"<code>finetune()</code>","text":"<p>Perform fine-tuning of the language model.</p> Source code in <code>src/mattext/models/potential.py</code> <pre><code>def finetune(self) -&gt; None:\n    \"\"\"\n    Perform fine-tuning of the language model.\n    \"\"\"\n\n    pretrained_ckpt = self.cfg.path.pretrained_checkpoint\n\n    config_train_args = self.cfg.training_arguments\n    callbacks = self._callbacks()\n\n    training_args = TrainingArguments(\n        **config_train_args,\n        metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n        greater_is_better=False,  # Lower eval_rmse is better\n    )\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        pretrained_ckpt, num_labels=1, ignore_mismatched_sizes=False\n    )\n\n    if self.cfg.freeze_base_model:\n        for param in model.base_model.parameters():\n            param.requires_grad = False\n\n    if self.local_rank is not None:\n        model = model.to(self.local_rank)\n        model = nn.parallel.DistributedDataParallel(\n            model, device_ids=[self.local_rank]\n        )\n    else:\n        model = model.to(\"cuda\")\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=None,\n        compute_metrics=self._compute_metrics,\n        tokenizer=self._wrapped_tokenizer,\n        train_dataset=self.tokenized_dataset[\"train\"],\n        eval_dataset=self.tokenized_dataset[\"test\"],\n        callbacks=callbacks,\n    )\n\n    wandb.log({\"Training Arguments\": str(config_train_args)})\n    wandb.log({\"model_summary\": str(model)})\n\n    trainer.train()\n    model.save_pretrained(self.cfg.path.finetuned_modelname)\n\n    eval_result = trainer.evaluate(eval_dataset=self.tokenized_testset)\n    wandb.log(eval_result)\n    wandb.finish()\n    return self.cfg.path.finetuned_modelname\n</code></pre>"},{"location":"api/#mattext.models.pretrain.PretrainModel","title":"<code>PretrainModel</code>","text":"<p>               Bases: <code>TokenizerMixin</code></p> <p>Class to perform pretraining of a language model.</p> Source code in <code>src/mattext/models/pretrain.py</code> <pre><code>class PretrainModel(TokenizerMixin):\n    \"\"\"Class to perform pretraining of a language model.\"\"\"\n\n    def __init__(self, cfg: DictConfig, local_rank=None):\n        super().__init__(\n            cfg=cfg.model.representation,\n            special_tokens=cfg.model.special_tokens,\n            special_num_token=cfg.model.special_num_token,\n        )\n        self.local_rank = local_rank\n        self.representation = cfg.model.representation\n        self.data_repository = cfg.model.data_repository\n        self.cfg = cfg.model.pretrain\n        self.context_length: int = self.cfg.context_length\n        self.callbacks = self.cfg.callbacks\n        self.model_name_or_path: str = self.cfg.model_name_or_path\n        self.local_file_path = cfg.model.dataset_local_path if cfg.model.dataset_local_path else None\n        self.tokenized_train_datasets, self.tokenized_eval_datasets =  self._prepare_datasets(\n            subset=self.cfg.dataset_name,local_file_path=self.local_file_path\n        )\n\n    def _prepare_datasets(self, subset: str, local_file_path: Optional[str] = None) -&gt; DatasetDict:\n        \"\"\"\n        Prepare training and validation datasets.\n\n        Args:\n            train_df (pd.DataFrame): DataFrame containing training data.\n\n        Returns:\n            DatasetDict: Dictionary containing training and validation datasets.\n        \"\"\"\n        if local_file_path:\n            # Load data from a local JSON file\n            train_dataset = load_dataset(\"json\", data_files=f\"{local_file_path}/train.json\", split=\"train\")\n            eval_dataset = load_dataset(\"json\", data_files=f\"{local_file_path}/test.json\", split=\"train\")\n        else:\n            # Load data from the repository\n            dataset = load_dataset(self.data_repository, subset)\n            train_dataset = dataset[\"train\"]\n            eval_dataset = dataset[\"test\"]\n\n        filtered_train_dataset = train_dataset.filter(\n            lambda example: example[self.representation] is not None\n        )\n        filtered_eval_dataset = eval_dataset.filter(\n            lambda example: example[self.representation] is not None\n        )\n\n        return filtered_train_dataset.map(\n            partial(\n                self._tokenize_pad_and_truncate, context_length=self.context_length\n            ),\n            batched=True,\n        ), filtered_eval_dataset.map(\n            partial(\n                self._tokenize_pad_and_truncate, context_length=self.context_length\n            ),\n            batched=True,\n        )\n\n    def _callbacks(self) -&gt; List[TrainerCallback]:\n        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n        callbacks = []\n\n        if self.callbacks.early_stopping:\n            callbacks.append(\n                EarlyStoppingCallback(\n                    early_stopping_patience=self.callbacks.early_stopping_patience,\n                    early_stopping_threshold=self.callbacks.early_stopping_threshold,\n                )\n            )\n\n        if self.callbacks.custom_logger:\n            callbacks.append(CustomWandbCallback_Pretrain())\n\n        return callbacks\n\n    def pretrain_mlm(self) -&gt; None:\n        \"\"\"Performs MLM pretraining of the language model.\"\"\"\n        config_mlm = self.cfg.mlm\n        config_train_args = self.cfg.training_arguments\n        config_model_args = self.cfg.model_config\n        #config_model_args[\"max_position_embeddings\"] = self.context_length\n\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self._wrapped_tokenizer,\n            mlm=config_mlm.is_mlm,\n            mlm_probability=config_mlm.mlm_probability,\n        )\n\n        callbacks = self._callbacks()\n\n        config = AutoConfig.from_pretrained(\n            self.model_name_or_path, **config_model_args\n        )\n\n        model = AutoModelForMaskedLM.from_config(config)\n\n        if self.local_rank is not None:\n            model = model.to(self.local_rank)\n            model = nn.parallel.DistributedDataParallel(\n                model, device_ids=[self.local_rank]\n            )\n        else:\n            model = model.to(\"cuda\")\n\n        training_args = TrainingArguments(**config_train_args)\n\n        trainer = Trainer(\n            model=model,\n            data_collator=data_collator,\n            train_dataset=self.tokenized_train_datasets,\n            eval_dataset=self.tokenized_eval_datasets,\n            args=training_args,\n            callbacks=callbacks,\n        )\n\n        wandb.log({\"config_details\": str(config)})\n        wandb.log({\"Training Arguments\": str(config_train_args)})\n        wandb.log({\"model_summary\": str(model)})\n\n        trainer.train()\n\n        # Save the fine-tuned model\n        model.save_pretrained(self.cfg.path.finetuned_modelname)\n</code></pre>"},{"location":"api/#mattext.models.pretrain.PretrainModel.pretrain_mlm","title":"<code>pretrain_mlm()</code>","text":"<p>Performs MLM pretraining of the language model.</p> Source code in <code>src/mattext/models/pretrain.py</code> <pre><code>def pretrain_mlm(self) -&gt; None:\n    \"\"\"Performs MLM pretraining of the language model.\"\"\"\n    config_mlm = self.cfg.mlm\n    config_train_args = self.cfg.training_arguments\n    config_model_args = self.cfg.model_config\n    #config_model_args[\"max_position_embeddings\"] = self.context_length\n\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=self._wrapped_tokenizer,\n        mlm=config_mlm.is_mlm,\n        mlm_probability=config_mlm.mlm_probability,\n    )\n\n    callbacks = self._callbacks()\n\n    config = AutoConfig.from_pretrained(\n        self.model_name_or_path, **config_model_args\n    )\n\n    model = AutoModelForMaskedLM.from_config(config)\n\n    if self.local_rank is not None:\n        model = model.to(self.local_rank)\n        model = nn.parallel.DistributedDataParallel(\n            model, device_ids=[self.local_rank]\n        )\n    else:\n        model = model.to(\"cuda\")\n\n    training_args = TrainingArguments(**config_train_args)\n\n    trainer = Trainer(\n        model=model,\n        data_collator=data_collator,\n        train_dataset=self.tokenized_train_datasets,\n        eval_dataset=self.tokenized_eval_datasets,\n        args=training_args,\n        callbacks=callbacks,\n    )\n\n    wandb.log({\"config_details\": str(config)})\n    wandb.log({\"Training Arguments\": str(config_train_args)})\n    wandb.log({\"model_summary\": str(model)})\n\n    trainer.train()\n\n    # Save the fine-tuned model\n    model.save_pretrained(self.cfg.path.finetuned_modelname)\n</code></pre>"},{"location":"benchmarking/","title":"Modeling and Benchmarking","text":"<p>MatText provides pipelines for seamless pretraining(<code>pretrain</code>) and benchmarking (<code>benchmark</code>) with finetuning (<code>finetune</code>) on multiple MatText representations. We use the Hydra framework to dynamically create hierarchical configurations based on the pipeline and representations that we want to use.</p>"},{"location":"benchmarking/#pretraining-on-single-mattext-representation","title":"Pretraining on Single MatText Representation","text":"Warning <p>The pretraining datasets we provide in MatText are only deduplicated based on the CIF string. That means that structures with slightly translated positions (e.g. conformers) might occur multiple times in the training set.</p> <p>Depending on the use case, this can lead to problems including data leakage. Hence, you might need to use, for example, one of the other representations for deduplications.</p> <pre><code>python main.py -cn=pretrain model=pretrain_example +model.representation=composition +model.dataset_type=pretrain30k +model.context_length=32\n</code></pre> <p>Here, <code>model=pretrain_example</code> would select <code>pretrain_example</code> as the base config for pretrain run <code>-cn=pretrain</code>.</p> <p>All config file can be found here, inside respective directories. <pre><code>cd /conf/\n</code></pre> Base configs can be found at <code>/conf/model</code></p> <p><code>+model.representation</code> can be any MatText representation. The pipeline would use data represented using that particular representation for training. <code>+model.dataset_type</code> can be one of the MatText pretraining datasets. <code>+model.context_length</code> would define the context length.</p> <code>context_length</code> <p>Use meaningful context length according to the representation to avoid truncation of structures.</p> <p>The <code>+</code> symbol before a configuration key indicates that you are adding a new key-value pair to the configuration. This is useful when you want to specify parameters that are not part of the default configuration.</p> <p>In order to override the existing default configuration from CLI, use <code>++</code>, e.g., <code>++model.pretrain.training_arguments.per_device_train_batch_size=32</code>.</p> <p>For advanced usage (changing architecture, training arguments, or modeling parameters), it would be easier to make the changes in the base config file which is <code>/conf/model/pretrain_example</code>, than having to override parameters with lengthy CLI commands!</p>"},{"location":"benchmarking/#running-benchmark-on-a-single-mattext-representation","title":"Running Benchmark on a Single MatText Representation","text":"<pre><code>python main.py -cn=benchmark model=benchmark_example +model.dataset_type=filtered +model.representation=composition +model.dataset=perovskites +model.checkpoint=path/to/checkpoint\n</code></pre> <p>Here, for the benchmarking pipeline(<code>-cn=benchmark</code>) the base config is <code>benchmark_example.yaml</code>. You can define the parameters for the experiment hence at <code>\\conf\\model\\benchmark_example.yaml</code>.</p> <code>dataset_type</code> <p>Here <code>+model.dataset_type=filtered</code> would select the type of benchmark. It can be <code>filtered</code> (avoid having truncated structure in train and test set, Only relatively small structures are present here, but this would also mean having less number of sampels to train on ) or <code>matbench</code> (complete dataset, there are few big structures , which would be trunated if the context length for modelling is less than <code>2048</code>).</p> Info <p><code>+model.dataset_type=filtered</code> would produce the report compatible with matbench leaderboard.</p> Reports path <p>Benchmark report is saved to the path defined in the base config. By default to <code>\"${hydra:runtime.cwd}/../../results/${now:%Y-%m-%d}/${now:%H-%M-%S}/$</code></p>"},{"location":"benchmarking/#pretraining-or-benchmarking-multiple-mattext-representations","title":"Pretraining or Benchmarking Multiple MatText Representations","text":"<p>The easiest way to model multiple representation in one run would be by using <code>config-groups</code> and multirun</p> <pre><code>python main.py --multirun -cn=benchmark model=benchmark_example +model.dataset_type=matbench +group-test=slices,composition\n</code></pre> <p>Here, we create a config group (directory with config files for different representations) at <code>/conf/&lt;config group name&gt;</code></p> <p>In the above example, we have two config files (<code>slices.yaml, composition.yaml</code>) inside the config group <code>group-test.</code> with <code>--multirun</code> enabled we can launch the pipeline parallely or sequentially (by default) for the representations, Here two but representations, but once can add more.</p> <p>The <code>child config</code> (config inside the <code>config group</code> ) would override or add the key value pair on top of the  <code>base config</code> (here <code>benchmark_example</code>).</p> Configs <p>configs inside group-test extends the <code>benchmark_example</code> and override the   <code>representation name</code>, <code>batch size</code>, <code>etc</code> in the <code>base config</code>.</p> <p>example <code>child config</code> <pre><code># @package _global_\nmodel:\n  logging:\n    wandb_project: pt_30k_test\n\n  representation: cif_p1\n  pretrain:\n    name: pt_30k_test\n    context_length:  1024\n    training_arguments:\n      per_device_train_batch_size: 32\n    path:\n      data_root_path: &lt;/path/to/dataset&gt;\n</code></pre></p> Reference <p>Read more about extending configs here.</p>"},{"location":"benchmarking/#configuring-experiments-and-model","title":"Configuring Experiments and Model","text":"<p>The main configuration for the run is in <code>config.yaml</code> and other configs are grouped in respective folders. An example directory structure of configs is below. <pre><code>\u251c\u2500\u2500 conf\n\u2502   \u251c\u2500\u2500 pretrain.yaml\n\u2502   \u251c\u2500\u2500 pretrain30k\n\u2502   \u2502   \u251c\u2500\u2500 cifp1.yaml\n\u2502   \u2502   \u251c\u2500\u2500 cifsymmetrized.yaml\n\u2502   \u2502   \u251c\u2500\u2500 composition.yaml\n\u2502   \u2502   \u251c\u2500\u2500 crystal_llm.yaml\n\u2502   \u2502   \u2514\u2500\u2500 slice.yaml\n\u2502   \u251c\u2500\u2500 finetune30k\n\u2502   \u2502   \u251c\u2500\u2500 cifp1.yaml\n\u2502   \u2502   \u251c\u2500\u2500 cifsymmetrized.yaml\n\u2502   \u2502   \u251c\u2500\u2500 composition.yaml\n\u2502   \u2502   \u251c\u2500\u2500 crystal_llm.yaml\n\u2502   \u2502   \u2514\u2500\u2500 slice.yaml\n\u2502   \u2514\u2500\u2500 model\n\u2502       \u251c\u2500\u2500 finetune_template.yaml\n\u2502       \u2514\u2500\u2500 pretrain_template.yaml\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 models\n</code></pre></p> <p>We use the HF Trainer, and hence, by default, it supports DP. For DDP support you can run <pre><code>python -m torch.distributed.run --nproc_per_node=4  /path/to/main.py --multirun model=pretrain_template +pretrain30k=cifp1,cifsym,composition,crystal_llm,slice\n</code></pre></p> <p>Here <code>model=pretrain_template</code> selects <code>pretrain_template</code> as the base config and override/extend it with <code>+pretrain30k=cifp1</code>. This would essentially start pretraining with cifp1 representation for the dataset-30K</p> <p>Note <code>+pretrain30k=cifp1,cifsym,composition,crystal_llm,slice</code> will launch 5 jobs parallelly, each of them with <code>pretrain_template</code> as the base config and corresponding experiment template extending them.</p> Launcher - Tips <p>For launching runs parallely checkout hydra submitit slurm launcher. you can override it from cli / or change it in the main config file. For kubernetes based infrastructures hydra submitit local launcher is ideal for parallel jobs. Or you can use the default hydra multirun launcher, which will run jobs sequentially.</p>"},{"location":"benchmarking/#adding-new-experiments","title":"Adding New Experiments","text":"<p>New experiments can be easily added with the following step.</p> <ol> <li>Create an experiment config group inside <code>conf/</code> . Make a new directory and add an experiment template inside it.</li> <li>Add / Edit the configs you want for the new experiments. e.g, override the pretrain checkpoints to new pretrained checkpoint</li> <li>Launch runs similarly but now with the new experiment group</li> </ol> <pre><code>python main.py --multirun model=pretrain_template ++hydra.launcher.gres=gpu:1 +&lt;new_exp_group&gt;=&lt;new_exp_template_1&gt;,&lt;new_exp_template_2&gt;, ..\n</code></pre>"},{"location":"benchmarking/#running-a-benchmark","title":"Running a Benchmark","text":"<pre><code>python main.py -cn=benchmark model=benchmark_example +model.dataset_type=filtered +model.representation=composition +model.dataset=perovskites +model.checkpoint=path/to/checkpoint\n</code></pre>"},{"location":"benchmarking/#finetuning-llm","title":"Finetuning LLM","text":"<pre><code>python main.py -cn=llm_sft model=llama_example +model.representation=composition +model.dataset_type=filtered +model.dataset=perovskites\n</code></pre> <p>The <code>+</code> symbol before a configuration key indicates that you are adding a new key-value pair to the configuration. This is useful when you want to specify parameters that are not part of the default configuration.</p> <p>To override the existing default configuration, use <code>++</code>, for e.g., <code>++model.pretrain.training_arguments.per_device_train_batch_size=32</code>. Refer to the docs for more examples and advanced ways to use the configs with config groups.</p> Cross-validation <p>Define the number of folds for k-fold cross-validation in the config or through the CLI. For Matbench benchmarks, however, the number of folds should be 5. The default value for all experiments is set to 5.</p>"},{"location":"benchmarking/#using-data","title":"Using Data","text":"<p>The MatText datasets can be easily obtained from HuggingFace, for example</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"n0w0f/MatText\", \"pretrain300k\")\n</code></pre>"},{"location":"benchmarking/#using-pretrained-mattext-models","title":"Using Pretrained MatText Models","text":"<p>The pretrained MatText models can be easily loaded from HuggingFace, for example</p> <pre><code>from transformers import AutoModel\n\nmodel = AutoModel.from_pretrained (\"n0w0f/MatText\u2212cifp1\u22122m\")\n</code></pre> Checkpoint <p>Here the pretrained model checkpoint is downloaded form Hugging Face hub, this step require internet for the first time.</p>"},{"location":"benchmarking/#training-other-language-models-using-mattext-pipeline","title":"Training Other Language Models Using Mattext Pipeline","text":"<p>You can pretrain models that are compatible with AutoModelForMaskedLM from Hugging Face using our framework. The path to the model or the model name (model name in Hugging Face) can be defined using the +model.pretrain.model_name_or_path argument. Here\u2019s an example:</p> <pre><code>python main.py -cn=pretrain model=pretrain_other_models +model.representation=slices +model.dataset_type=pretrain30k +model.context_length=32 +model.pretrain.model_name_or_path=\"FacebookAI/roberta-base\"\n</code></pre> <p>For better manageability, you can define the model name and configuration in the base config file. This way, you don't need to specify the model and model configs in the CLI.</p> <pre><code>pretrain:\n  name: test-pretrain\n  exp_name: \"${model.representation}_${model.pretrain.name}\"\n  model_name_or_path: \"FacebookAI/roberta-base\"\n  dataset_name: \"${model.dataset_type}\"\n  context_length: \"${model.context_length}\"\n\n  model_config:\n    hidden_size: 768\n    num_hidden_layers: 4\n    num_attention_heads: 8\n    is_decoder: False\n    add_cross_attention: False\n    max_position_embeddings: 768\n</code></pre>"},{"location":"benchmarking/#pretraining-on-your-custom-dataset","title":"Pretraining on Your Custom Dataset","text":"<p>You can use your own     datasets stored locally on your machine or hosted on the Hugging Face Hub.</p> <p>For using dataset in a local path,  Define the dataset_local_path variable in your base configuration file. By default, this value is set to False, indicating no local dataset is used. Refer to the example configuration file <code>/conf/model/pretrain_own_data_example</code>, execute the following command to run the pretraining script with your local dataset:</p> <pre><code>python main.py -cn=pretrain model=pretrain_own_data_example +model.representation=composition +model.context_length=32 ++model.dataset_local_path=path/to/local\n</code></pre> <p>For Hugging Face datasets, specify the repository (<code>data_repository</code>) and dataset type (<code>dataset_type</code>) in the configuration.</p> <code>key names</code> <p>Ensure your dataset has a key for the specific representation used for training</p>"},{"location":"getting_started/","title":"Installation","text":"<p>The most recent code and data can be installed directly from GitHub with:</p> <pre><code>$ pip install git+https://github.com/lamalab-org/mattext.git\n</code></pre> <p>To install in development mode, use the following:</p> <pre><code>$ git clone git+https://github.com/lamalab-org/mattext.git\n$ cd mattext\n$ pip install -e .\n</code></pre> <p>If you want to use the Local Env representation, you will also need to install OpenBabel, e.g. using </p> <pre><code>conda install openbabel -c conda-forge\n</code></pre>"},{"location":"representations/","title":"MatText Representations","text":""},{"location":"representations/#creating-text-representations-for-crystal-structures","title":"Creating Text Representations for Crystal Structures","text":"<p>Converting structures into text representations can be done using our <code>TextRep</code> class</p> <pre><code>from mattext.representations import TextRep\nfrom pymatgen.core import Structure\n\n\n# Load structure from a CIF file\nfrom_file = \"InCuS2_p1.cif\"\nstructure = Structure.from_file(from_file, \"cif\")\n\n# Initialize TextRep Class\ntext_rep = TextRep.from_input(structure)\n</code></pre>"},{"location":"representations/#get_requested_text_reps-method","title":"<code>get_requested_text_reps</code> Method","text":"<p>The <code>get_requested_text_reps</code> method retrieves the requested text representations of the crystal structure and returns them in a dictionary.</p> <p>For example, to obtain the <code>cif_p1</code>, <code>slice</code>, <code>atoms_params</code>, <code>crystal_llm_rep</code>, and <code>zmatrix</code> representations, use the following code:</p> <pre><code>text_rep = TextRep(structure)\n\n# Define a list of requested representations\nrequested_reps = [\n    \"cif_p1\",\n    \"slice\",\n    \"atoms_params\",\n    \"crystal_llm_rep\",\n    \"zmatrix\"\n]\n\n# Get the requested text representations\nrequested_text_reps = text_rep.get_requested_text_reps(requested_reps)\nprint(requested_text_reps)\n</code></pre> output <pre><code>{'cif_p1': \"data_InCuS2\\n_symmetry_space_group_name_H-M   'P 1'\\n_cell_length_a   5.52\\n_cell_length_b   5.52\\n_cell_length_c   6.8\\n_cell_angle_alpha   113.96\\n_cell_angle_beta   113.96\\n_cell_angle_gamma   90.0\\n_symmetry_Int_Tables_number   1\\n_chemical_formula_structural   InCuS2\\n_chemical_formula_sum   'In2 Cu2 S4'\\n_cell_volume   169.53\\n_cell_formula_units_Z   2\\nloop_\\n _symmetry_equiv_pos_site_id\\n _symmetry_equiv_pos_as_xyz\\n  1  'x, y, z'\\nloop_\\n _atom_type_symbol\\n _atom_type_oxidation_number\\n  In3+  3.0\\n  Cu+  1.0\\n  S2-  -2.0\\nloop_\\n _atom_site_type_symbol\\n _atom_site_label\\n _atom_site_symmetry_multiplicity\\n _atom_site_fract_x\\n _atom_site_fract_y\\n _atom_site_fract_z\\n _atom_site_occupancy\\n  Cu+  Cu4  1  0.25  0.75  0.5  1.0\\n  Cu+  Cu5  1  0.0  0.0  0.0  1.0\\n  In3+  In0  1  0.5  0.5  0.0  1.0\\n  In3+  In1  1  0.75  0.25  0.5  1.0\\n  S2-  S8  1  0.9  0.88  0.25  1.0\\n  S2-  S9  1  0.62  0.1  0.75  1.0\\n  S2-  S10  1  0.35  0.38  0.25  1.0\\n  S2-  S11  1  0.12  0.65  0.75  1.0\\n\", 'slice': 'Cu Cu In In S S S S 0 7 o o o 0 4 - o o 0 6 o o o 0 5 o + o 1 4 - - o 1 5 - o - 1 7 o - - 1 6 o o o 2 6 o o o 2 7 o o - 2 5 o o - 2 4 o o o 3 5 o o o 3 6 o o o 3 4 o - o 3 7 + o o ', 'atoms_params': 'Cu Cu In In S S S S 5.52 5.52 6.8 113 113 90', 'crystal_llm_rep': '5.5 5.5 6.8\\n113 113 90\\nCu+\\n0.25 0.75 0.50\\nCu+\\n0.00 0.00 0.00\\nIn3+\\n0.50 0.50 0.00\\nIn3+\\n0.75 0.25 0.50\\nS2-\\n0.90 0.87 0.25\\nS2-\\n0.62 0.10 0.75\\nS2-\\n0.35 0.37 0.25\\nS2-\\n0.12 0.65 0.75', 'zmatrix': 'Cu\\nCu 1 3.9\\nIn 2 3.9 1 60\\nIn 1 3.9 2 60 3 -71\\nS 3 2.5 4 90 1 93\\nS 4 2.5 2 90 1 93\\nS 1 2.3 2 32 3 -35\\nS 1 2.3 7 111 6 -32'}\n</code></pre>"},{"location":"representations/#supported-text-representations","title":"Supported Text Representations","text":"<p>The <code>TextRep</code> class currently supports the following text representations:</p> <ul> <li>SlICES (<code>slices</code>): SLICE representation of the crystal structure.</li> <li>Composition (<code>composition</code>): Chemical composition in hill format.</li> <li>CIF Symmetrized (<code>cif_symmetrized</code>): Multi-line CIF representation with symmetrized structure and rounded float numbers.</li> <li>CIF \\(P_1\\) (<code>cif_p1</code>): Multi-line CIF representation with the conventional unit cell and rounded float numbers.</li> <li>Crystal-text-LLM Representation (<code>crystal_text_llm</code>): Representation following the format specified in the cited work.</li> <li>Robocrystallographer Representation (<code>robocrys_rep</code>): Representation generated by Robocrystallographer.</li> <li>Atom Sequences (<code>atom_sequences</code>): List of atoms inside the unit cell.</li> <li>Atoms Squences++ (<code>atom_sequences_plusplus</code>): List of atoms with lattice parameters.</li> <li>Z-Matrix (<code>zmatrix</code>): Z-Matrix representation of the crystal structure.</li> <li>Local-Env (<code>local_env</code>):  List of Wyckoff label and SMILES separated by line breaks for each local environment.</li> </ul> <p>For more details on each representation and how to obtain them, refer to the respective method documentation in the <code>TextRep</code> class.</p>"},{"location":"representations/#transformations","title":"Transformations","text":"<p>The <code>TextRep</code> class supports various transformations that can be applied to the input structure.</p>"},{"location":"representations/#permute-structure","title":"Permute Structure","text":"<p>The <code>permute_structure</code> transformation randomly permutes the order of atoms in a structure. </p> <pre><code>from mattext.representations import TextRep\nfrom pymatgen.core.structure import Structure\n\nstructure_1 = Structure.from_file(\"N2_p1.cif\", \"cif\")\n\ntransformations = [(\"permute_structure\", {\"seed\": 42})]\n\ntext_rep = TextRep.from_input(structure_1, transformations)\ntext_representations_requested = [\"atoms\", \"crystal_llm_rep\"]\nprint(\"Permuted Pymatgen Structure:\")\nprint(text_rep.structure)\nprint(\"Permuted Text Representations:\")\nprint(text_rep.get_requested_text_reps(text_representations_requested))\n</code></pre> output <pre><code>Permuted Pymatgen Structure:\nFull Formula (N4)\nReduced Formula: N2\nabc   :   5.605409   5.605409   5.605409\nangles:  90.000000  90.000000  90.000000\npbc   :       True       True       True\nSites (4)\n#  SP          a        b        c\n---  ----  -------  -------  -------\n0  N0+   0.02321  0.02321  0.02321\n1  N0+   0.97679  0.52321  0.47679\n2  N0+   0.52321  0.47679  0.97679\n3  N0+   0.47679  0.97679  0.52321\nPermuted Text Representations:\n{'atoms': 'N N N N', 'crystal_llm_rep': '5.6 5.6 5.6\\n90 90 90\\nN0+\\n0.02 0.02 0.02\\nN0+\\n0.98 0.52 0.48\\nN0+\\n0.52 0.48 0.98\\nN0+\\n0.48 0.98 0.52'}\n</code></pre>"},{"location":"representations/#translate-structure","title":"Translate Structure","text":"<p>The <code>translate_structure</code> transformation randomly translates all atoms in a structure by a specified vector. This can simulate small displacements in the structure.</p> <pre><code>transformations = [(\"translate_structure\", {\"seed\": 42, \"vector\": [0.1, 0.1, 0.1]})]\n\ntext_rep = TextRep.from_input(structure_1, transformations)\ntext_representations_requested = [\"crystal_llm_rep\"]\nprint(\"Translated Crystal-text-LLM Representations:\")\nprint(text_rep.get_requested_text_reps(text_representations_requested))\n</code></pre> output <pre><code>Translated Crystal-text-LLM Representations:\n{'crystal_llm_rep': '5.6 5.6 5.6\\n90 90 90\\nN0+\\n0.58 0.08 0.62\\nN0+\\n0.08 0.62 0.58\\nN0+\\n0.12 0.12 0.12\\nN0+\\n0.62 0.58 0.08'}\n</code></pre>"},{"location":"representations/#augmenting-data","title":"Augmenting Data","text":"<p>In principle, we can generate valid text representations with random transformations with physically meaningful parameters. Dummy example is shown below</p> <pre><code>from mattext.representations import TextRep\n\n# Define transformations\ntranslation_vectors = np.random.uniform(low=0.1, high=0.5, size=(3, 3))\nfor vector in translation_vectors:\n    transformations = [\n        (\"permute_structure\", {\"seed\": 42}),\n        (\"perturb_structure\", {\"seed\": 42, \"max_distance\": 0.1}),\n        (\"translate_structure\", {\"seed\": 42, \"vector\": vector.tolist()})\n    ]\n    text_rep = TextRep.from_input(structure_2, transformations)\n    text_representations_requested = [\"crystal_llm_rep\"]\n    print(\"Translated Text Representations:\")\n    print(text_rep.get_requested_text_reps(text_representations_requested))\n</code></pre> output <pre><code>Translated Text Representations:{'crystal_llm_rep': '3.9 3.9 3.9\\n90 90 90\\nO2-\\n0.76 0.98 0.41\\nTi4+\\n0.77 0.98 0.89\\nO2-\\n0.76 0.49 0.89\\nO2-\\n0.26 0.97 0.88\\nSr2+\\n0.25 0.47 0.38'}\n\nTranslated Text Representations:{'crystal_llm_rep': '3.9 3.9 3.9\\n90 90 90\\nO2-\\n0.85 0.66 0.18\\nTi4+\\n0.86 0.66 0.66\\nO2-\\n0.85 0.17 0.66\\nO2-\\n0.35 0.65 0.65\\nSr2+\\n0.34 0.15 0.15'}\n\nTranslated Text Representations:{'crystal_llm_rep': '3.9 3.9 3.9\\n90 90 90\\nO2-\\n0.63 0.94 0.35\\nTi4+\\n0.64 0.94 0.84\\nO2-\\n0.64 0.45 0.84\\nO2-\\n0.13 0.94 0.83\\nSr2+\\n0.12 0.43 0.33'}\n</code></pre> Example <p>More examples are available as notebook in the repository.</p> <p>The following transformations are available for transforming structures:</p>"},{"location":"representations/#randomly-permute-structure","title":"Randomly permute structure","text":"<p><code>permute_structure</code> randomly permutes the order of atoms in a structure.</p>"},{"location":"representations/#randomly-translate-single-atom","title":"Randomly Translate Single Atom","text":"<p><code>translate_single_atom</code> randomly translates one or more atoms in a structure.</p>"},{"location":"representations/#randomly-perturb-structure","title":"Randomly Perturb Structure","text":"<p><code>perturb_structure</code> randomly perturbs atoms in a structure.</p>"},{"location":"representations/#randomly-translate-structure","title":"Randomly Translate Structure","text":"<p><code>translate_structure</code> randomly translates the atoms in a structure.</p> Tip <p>This transformation supports additional keyword arguments for fine-tuning the translation.</p> <p>MatText leverages methods from pymatgen and support all the keyword arguments in <code>Structure.translate_sites</code> method.</p> <p>All transformations utilize a common seed value for reproducibility and accept additional parameters for customization.</p> <p>For more details on each transformation and its parameters, refer to the respective function documentation.</p>"},{"location":"tokenizers/","title":"MatText Tokenizers","text":"<p>MatText comes with custom tokenizers suitable for modelling text representations. Currently available <code>tokenizer</code> are <code>SliceTokenizer</code>, <code>CompositionTokenizer</code>, <code>CifTokenizer</code>, <code>RobocrysTokenizer</code>, <code>SmilesTokenizer</code>. All the MatText representations can be translated to tokens using one of these tokenizers.</p>"},{"location":"tokenizers/#using-mattext-tokenizers","title":"Using MatText Tokenizers","text":"<p>By default, the tokenizer is initialized with <code>[CLS]</code> and <code>[SEP]</code> tokens. For an example, see the <code>SliceTokenizer</code> usage: </p> <pre><code>from mattext.tokenizer import SliceTokenizer\n\ntokenizer = SliceTokenizer(\n                model_max_length=512, \n                truncation=True, \n                padding=\"max_length\", \n                max_length=512\n            )\nprint(tokenizer.cls_token) \nprint(tokenizer.tokenize(\"Ga Ga P P 0 3 - - o 0 2 - o - 0 1 o - -\"))\n</code></pre> output <pre><code>[CLS]\n['[CLS]', 'Ga', 'Ga', 'P', 'P', '0', '3', '- - o', '0', '2', '- o -', '0', '1', 'o - -', '[SEP]']\n</code></pre> tip <p>You can access the <code>[CLS]</code> token using the <code>cls_token</code> attribute of the tokenizer. </p> <p>During decoding, you can utilize the <code>skip_special_tokens</code> parameter to skip these special tokens.</p> <pre><code>token_ids = tokenizer.encode(\"Ga Ga P P 0 3 - - o 0 2 - o - 0 1 o - -\")\nprint(token_ids)\ndecoded = tokenizer.decode(token_ids, skip_special_tokens=True)\nprint(decoded)\n</code></pre> output <pre><code>[149, 57, 57, 41, 41, 139, 142, 24, 139, 141, 20, 139, 140, 8, 150]\nGa Ga P P 0 3 - - o 0 2 - o - 0 1 o - -\n</code></pre>"},{"location":"tokenizers/#initializing-tokenizers-with-custom-special-tokens","title":"Initializing Tokenizers With Custom Special Tokens","text":"<p>In scenarios where the <code>[CLS]</code> token is not required, you can initialize the tokenizer with an empty special_tokens dictionary.</p> <p>Initialization without <code>[CLS]</code> and <code>[SEP]</code> tokens:</p> <pre><code>tokenizer = SliceTokenizer(\n                model_max_length=512, \n                special_tokens={}, \n                truncation=True,\n                padding=\"max_length\", \n                max_length=512\n            )\n</code></pre> <p>All <code>MatText Tokenizer</code> instances inherit from PreTrainedTokenizer and accept arguments compatible with the Hugging Face tokenizer.</p>"},{"location":"tokenizers/#tokenizers-with-special-number-tokenization","title":"Tokenizers With Special Number Tokenization","text":"<p>The <code>special_num_token</code> argument (by default <code>False</code>) can be set to <code>True</code>  to tokenize numbers in a special way as designed and implemented by RegressionTransformer.</p> <pre><code>tokenizer = SliceTokenizer(\n                special_num_token=True,\n                model_max_length=512, \n                special_tokens={}, \n                truncation=True,\n                padding=\"max_length\", \n                max_length=512\n            )\ntokenizer.tokenize(\"H2SO4\")\n</code></pre> output <pre><code>['H', '_2_0_', 'S', 'O', '_4_0_']\n</code></pre>"},{"location":"tokenizers/#updating-vocabulary-in-the-tokenizers","title":"Updating Vocabulary in the Tokenizers","text":"<p>The MatText tokenizers allow one to update the vocabulary or load a custom vocabulary file.</p> <p>Default vocabulary can be determined by calling the <code>vocab</code> attribute.</p> <pre><code>tokenizer = SliceTokenizer()\nprint(tokenizer.vocab)\n</code></pre> output <pre><code>{'o o o': 0, 'o o +': 1, 'o o -': 2, 'o + o': 3, 'o + +': 4, 'o + -': 5, 'o - o': 6, 'o - +': 7, 'o - -': 8, '+ o o': 9, '+ o +': 10, '+ o -': 11, '+ + o': 12, '+ + +': 13, '+ + -': 14, '+ - o': 15, '+ - +': 16, '+ - -': 17, '- o o': 18, '- o +': 19, '- o -': 20, '- + o': 21, '- + +': 22, '- + -': 23, '- - o': 24, '- - +': 25, '- - -': 26, 'H': 27, 'He': 28, 'Li': 29, 'Be': 30, 'B': 31, 'C': 32, 'N': 33, 'O': 34, 'F': 35, 'Ne': 36, 'Na': 37, 'Mg': 38, 'Al': 39, 'Si': 40, 'P': 41, 'S': 42, 'Cl': 43, 'K': 44, 'Ar': 45, 'Ca': 46, 'Sc': 47, 'Ti': 48, 'V': 49, 'Cr': 50, 'Mn': 51, 'Fe': 52, 'Ni': 53, 'Co': 54, 'Cu': 55, 'Zn': 56, 'Ga': 57, 'Ge': 58, 'As': 59, 'Se': 60, 'Br': 61, 'Kr': 62, 'Rb': 63, 'Sr': 64, 'Y': 65, 'Zr': 66, 'Nb': 67, 'Mo': 68, 'Tc': 69, 'Ru': 70, 'Rh': 71, 'Pd': 72, 'Ag': 73, 'Cd': 74, 'In': 75, 'Sn': 76, 'Sb': 77, 'Te': 78, 'I': 79, 'Xe': 80, 'Cs': 81, 'Ba': 82, 'La': 83, 'Ce': 84, 'Pr': 85, 'Nd': 86, 'Pm': 87, 'Sm': 88, 'Eu': 89, 'Gd': 90, 'Tb': 91, 'Dy': 92, 'Ho': 93, 'Er': 94, 'Tm': 95, 'Yb': 96, 'Lu': 97, 'Hf': 98, 'Ta': 99, 'W': 100, 'Re': 101, 'Os': 102, 'Ir': 103, 'Pt': 104, 'Au': 105, 'Hg': 106, 'Tl': 107, 'Pb': 108, 'Bi': 109, 'Th': 110, 'Pa': 111, 'U': 112, 'Np': 113, 'Pu': 114, 'Am': 115, 'Cm': 116, 'Bk': 117, 'Cf': 118, 'Es': 119, 'Fm': 120, 'Md': 121, 'No': 122, 'Lr': 123, 'Rf': 124, 'Db': 125, 'Sg': 126, 'Bh': 127, 'Hs': 128, 'Mt': 129, 'Ds': 130, 'Rg': 131, 'Cn': 132, 'Nh': 133, 'Fl': 134, 'Mc': 135, 'Lv': 136, 'Ts': 137, 'Og': 138, '0': 139, '1': 140, '2': 141, '3': 142, '4': 143, '5': 144, '6': 145, '7': 146, '8': 147, '9': 148, '[CLS]': 149, '[SEP]': 150}\n</code></pre> <pre><code># Path to your custom vocabulary file (JSON or TXT format)\nvocab_file_path = \"path/to/your/vocab_file.json\"\n\ntokenizer = SliceTokenizer(\n                special_num_token=True,\n                model_max_length=512, \n                special_tokens={}, \n                truncation=True,\n                padding=\"max_length\", \n                max_length=512,\n                vocab_file=vocab_file_path\n            )\n</code></pre> <p>here is an example format for the vocabulary json file</p> <pre><code>    import json\n    import tempfile # (1)\n    new_vocab = {\n    \"H\": 1,\n    \"He\": 2,\n    \"New_Atom\": 3,\n    \"1\":4,\n    \"2\":5,}\n\n    with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.json') as temp_file:\n        # Write the JSON string to the temporary file\n        json.dump(my_dict, temp_file)\n        temp_file_path = temp_file.name\n\n\n    tokenizer = SliceTokenizer(\n                    model_max_length=512,\n                    truncation=True,\n                    padding=\"max_length\",\n                    max_length=512,\n                    vocab_file=temp_file_path\n                )\n\n    print(tokenizer.tokenize(\"H He Na New_Atom 1  2 9\"))\n    print(tokenizer.vocab)\n</code></pre> <ol> <li> here notice that we are writing the vocabulary dictionary to a temporary file, since Tokenizer class accepts only file paths.</li> </ol> output <pre><code>['[CLS]', 'H', 'He', 'New_Atom', '1', '2', '[SEP]']\n\n#Atoms that are not within the vocabulary are ignored. and newly added atoms are correctly tokenized.\n\n{'H': 1, 'He': 2, 'New_Atom': 3, '1': 4, '2': 5, '[CLS]': 6, '[SEP]': 7}\n</code></pre>"}]}