{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_TOKENIZER_PATH = \"/home/so87pot/n0w0f/regression-transformer/slice-assets/slice_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SliceTokenizer:\n",
    "    def __init__(self, vocab_file=SLICE_TOKENIZER_PATH):\n",
    "        _tokenizer = Tokenizer.from_file(vocab_file)\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "                    tokenizer_object=_tokenizer,\n",
    "                    unk_token=\"[UNK]\",\n",
    "                    pad_token=\"[PAD]\",\n",
    "                    cls_token=\"[CLS]\",\n",
    "                    sep_token=\"[SEP]\",\n",
    "                    mask_token=\"[MASK]\",\n",
    "                )\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ga Ga Ge Ge Te Te 0 5 - - o 0 5 - o - 0 5 o - - 0 2 o o o 1 3 o o o 1 4 o + + 1 4 + o + 1 4 + + o 2 3 - o o 2 3 o - o 2 3 o o - \n",
    "Li Li Co Si O O O O 0 4 o - o 0 7 + + o 0 5 o o o 0 6 o - o 1 6 o - o 1 5 o + o 1 7 o + o 1 4 - o o 2 5 o - + 2 4 o + o 2 6 o o o 2 7 o o + 3 7 o - + 3 6 o o o 3 4 - o o 3 5 - - + \n",
    "F F F F F F Y Cs Cs Cs 0 6 + o o 0 7 o - - 0 9 o o o 1 6 o + o 1 7 o o o 1 9 - o - 2 6 + o + 2 7 + o + 2 9 o o o 3 6 o + o 3 7 o o - 3 9 o + o 4 6 o o o 4 8 o o + 5 6 + + + 5 8 o o o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file 'extended_periodic_table_vocab.txt' created successfully.\n"
     ]
    }
   ],
   "source": [
    "elements = [\n",
    "    'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',\n",
    "    'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar',\n",
    "    'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn',\n",
    "    'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo',\n",
    "    'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n",
    "    'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho',\n",
    "    'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
    "    'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es',\n",
    "    'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn',\n",
    "    'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Define symbols\n",
    "symbols = ['o', '+', '-']\n",
    "\n",
    "# Generate all combinations of length 3\n",
    "combinations = [' '.join(combination) for combination in product(symbols, repeat=3)]\n",
    "\n",
    "# Numbers\n",
    "numbers = [str(i) for i in range(10)]\n",
    "\n",
    "# Combine all elements, symbols, and numbers\n",
    "all_tokens = combinations + elements + numbers\n",
    "\n",
    "# Path to the vocabulary file\n",
    "vocab_file_path = 'extended_periodic_table_vocab.txt'\n",
    "\n",
    "# Write all tokens to the vocabulary file\n",
    "with open(vocab_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_tokens))\n",
    "\n",
    "print(f\"Vocabulary file '{vocab_file_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -', '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -', '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -', 'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "class AtomVocabTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n",
    "        super(AtomVocabTokenizer, self).__init__(model_max_length=model_max_length, **kwargs)\n",
    "        \n",
    "        # Load vocabulary from the provided file\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        elements = [\n",
    "            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',\n",
    "            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar',\n",
    "            'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn',\n",
    "            'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo',\n",
    "            'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n",
    "            'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho',\n",
    "            'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
    "            'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es',\n",
    "            'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn',\n",
    "            'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n",
    "        ]\n",
    "\n",
    "        from itertools import product\n",
    "\n",
    "        # Define symbols\n",
    "        symbols = ['o', '+', '-']\n",
    "\n",
    "        # Generate all combinations of length 3\n",
    "        combinations = [' '.join(combination) for combination in product(symbols, repeat=3)]\n",
    "\n",
    "        # Numbers\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "\n",
    "        # Combine all elements, symbols, and numbers\n",
    "        self.all_tokens = combinations + elements + numbers\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().splitlines()\n",
    "        return {token: idx for idx, token in enumerate(vocab)}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # List of tokens\n",
    "        tokens = self.all_tokens\n",
    "\n",
    "        # Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "        escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "        # Join the escaped vocab terms into a regex pattern\n",
    "        pattern_str = '|'.join(escaped_tokens)\n",
    "        pattern = re.compile(pattern_str)\n",
    "\n",
    "        # Find all matches in the text\n",
    "        matches = pattern.findall(text)\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def _add_tokens(self, new_tokens, **kwargs):\n",
    "        # Override _add_tokens to add new tokens to the vocabulary\n",
    "        for token in new_tokens:\n",
    "            if token not in self.added_tokens_encoder:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.ids_to_tokens[len(self.ids_to_tokens)] = token\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return list(self.vocab.keys())[index]\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(self.vocab))\n",
    "\n",
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"N F N F N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'F', 'F', 'F', 'K', 'K', 'o o o', '+ o o', 'o o -', 'o - -', 'o o o', 'o - o', 'o + o', 'o o o', 'o o -', '- o -', 'o o o', '- o o', '+ + +', '+ + o', '+ o o', 'o + o', 'o o o', 'o o o', 'o o o', 'o o o', 'o o o', 'o - o', '- o o', '- - o']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "\n",
    "# List of tokens\n",
    "tokens = ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -',\n",
    "          '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -',\n",
    "          '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -',\n",
    "          'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe']\n",
    "\n",
    "# Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "# Join the escaped vocab terms into a regex pattern\n",
    "pattern_str = '|'.join(escaped_tokens)\n",
    "pattern = re.compile(pattern_str)\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = pattern.findall(text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', '- o o', '- o o', '- o +', 'o o o', 'o o o', '- o o', '- o +', '- o +', 'o o o', 'o o o']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of tokens\n",
    "tokens = ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -',\n",
    "          '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -',\n",
    "          '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -',\n",
    "          'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe']\n",
    "\n",
    "# Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "# Join the escaped vocab terms into a regex pattern, allowing for spaces\n",
    "pattern_str = r'(?:' + '|'.join(escaped_tokens) + r')'\n",
    "pattern = re.compile(pattern_str)\n",
    "\n",
    "# Test the pattern on a sample text\n",
    "text = \"P P P P P P P P 0 5 - o o 0 6 - o o 0 6 - o + 0 3 o o o 1 2 o o o 3 5 - o o 3 5 - o + 3 6 - o + 4 7 o o o 5 6 o o o\"\n",
    "matches = pattern.findall(text)\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N F N F N'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import os\n",
    "\n",
    "class AtomVocabTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n",
    "        super(AtomVocabTokenizer, self).__init__(model_max_length=model_max_length, **kwargs)\n",
    "        \n",
    "        # Load vocabulary from the provided file\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().splitlines()\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            tokens.append(char) if char in self.vocab else tokens.append(self.unk_token)\n",
    "        return tokens\n",
    "    \n",
    "    def _add_tokens(self):\n",
    "        # Override _add_tokens to prevent NotImplementedError\n",
    "        pass\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ''.join(tokens)\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(self.vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded String:\", decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded String:\", decoded_string)\n",
    "\n",
    "# Save the custom vocabulary\n",
    "output_vocab_path = 'path/to/your/output/vocab.txt'\n",
    "tokenizer.save_vocabulary(output_vocab_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slice_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
