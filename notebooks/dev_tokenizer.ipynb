{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_TOKENIZER_PATH = \"/home/so87pot/n0w0f/regression-transformer/slice-assets/slice_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "from structllm.tokenizer.slice_tokenizer import AtomVocabTokenizer\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path, \n",
    "                model_max_length=512,truncation=False, padding=False)\n",
    "\n",
    "special_tokens = {\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"mask_token\": \"[MASK]\",\n",
    "}\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9783bf07ca0d470584480bd3478b3bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes, pads, and truncates input texts.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslices\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tokenized_train_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_tokenize_pad_and_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 853\u001b[0m     {\n\u001b[1;32m    854\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    855\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    856\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    857\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    858\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    859\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    860\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    861\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    862\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    863\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    864\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    865\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    866\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    867\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    868\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    869\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    870\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    871\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    853\u001b[0m     {\n\u001b[0;32m--> 854\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3470\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3472\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3474\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3483\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m, in \u001b[0;36m_tokenize_pad_and_truncate\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tokenize_pad_and_truncate\u001b[39m(texts):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes, pads, and truncates input texts.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2798\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[1;32m   2883\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2886\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2905\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2906\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2923\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3066\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   3051\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   3063\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3065\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 3066\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   3076\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   3077\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3092\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3093\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/slice_llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2699\u001b[0m             max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;66;03m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m-> 2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m):\n\u001b[1;32m   2703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2705\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2706\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m})`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2707\u001b[0m     )\n\u001b[1;32m   2709\u001b[0m \u001b[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# def _tokenize_pad_and_truncate(texts):\n",
    "#     \"\"\"Tokenizes, pads, and truncates input texts.\"\"\"\n",
    "#     return tokenizer(texts[\"slices\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# tokenized_train_datasets = train_dataset.map(_tokenize_pad_and_truncate, batched=True)\n",
    "\n",
    "def _tokenize_pad_and_truncate(texts):\n",
    "    \"\"\"Tokenizes, pads, and truncates input texts.\"\"\"\n",
    "    print(texts[\"slices\"])\n",
    "    return tokenizer(texts[\"slices\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tokenize_save_and_plot(tokenizer, csv_path, txt_path, plot_path):\n",
    "    tokenizer = tokenizer\n",
    "    token_counts = []\n",
    "    with open(csv_path, \"rt\") as fp, open(txt_path, \"w\") as out_fp:\n",
    "        for line in tqdm(fp):\n",
    "            line = line.strip()\n",
    "            tokens = tokenizer.tokenize(text=line)\n",
    "            out_fp.write(line + '\\n')  # write original line to file\n",
    "            out_fp.write(','.join(tokens) + '\\n')  # write tokens to file\n",
    "            out_fp.write(f\"length:{len(tokens)}\\n\")  # write number of tokens to file\n",
    "            out_fp.write(\"--------------\\n\")  # write separator to file\n",
    "            token_counts.append(len(tokens))\n",
    "\n",
    "    plt.hist(token_counts, bins='auto')\n",
    "    plt.title('Token Count Distribution')\n",
    "    plt.xlabel('Number of Tokens')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(plot_path)  # save the plot to a file\n",
    "    plt.show()\n",
    "\n",
    "    # Save distribution details to the file\n",
    "    with open(txt_path, \"a\") as out_fp:\n",
    "        out_fp.write(\"\\nToken Count Distribution:\\n\")\n",
    "        out_fp.write(f\"Min: {min(token_counts)}\\n\")\n",
    "        out_fp.write(f\"Max: {max(token_counts)}\\n\")\n",
    "        out_fp.write(f\"Average: {sum(token_counts) / len(token_counts)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84891it [00:24, 3439.77it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = \"/home/so87pot/n0w0f/structllm/data/mb_1/csv/train_matbench_mp_gap_0.csv\"\n",
    "\n",
    "file_name = \"ft_mp_gap_atom_tokenizer\"\n",
    "txt_path = f\"/home/so87pot/n0w0f/structllm/results/atom_tokenizer_counts/{file_name}_tokens.txt\"\n",
    "plot_path = f\"/home/so87pot/n0w0f/structllm/results/atom_tokenizer_counts/{file_name}_tokens.png\"\n",
    "tokenize_save_and_plot(tokenizer, csv_path, txt_path, plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SliceTokenizer:\n",
    "    def __init__(self, vocab_file=SLICE_TOKENIZER_PATH):\n",
    "        _tokenizer = Tokenizer.from_file(vocab_file)\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "                    tokenizer_object=_tokenizer,\n",
    "                    unk_token=\"[UNK]\",\n",
    "                    pad_token=\"[PAD]\",\n",
    "                    cls_token=\"[CLS]\",\n",
    "                    sep_token=\"[SEP]\",\n",
    "                    mask_token=\"[MASK]\",\n",
    "                )\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ga Ga Ge Ge Te Te 0 5 - - o 0 5 - o - 0 5 o - - 0 2 o o o 1 3 o o o 1 4 o + + 1 4 + o + 1 4 + + o 2 3 - o o 2 3 o - o 2 3 o o - \n",
    "Li Li Co Si O O O O 0 4 o - o 0 7 + + o 0 5 o o o 0 6 o - o 1 6 o - o 1 5 o + o 1 7 o + o 1 4 - o o 2 5 o - + 2 4 o + o 2 6 o o o 2 7 o o + 3 7 o - + 3 6 o o o 3 4 - o o 3 5 - - + \n",
    "F F F F F F Y Cs Cs Cs 0 6 + o o 0 7 o - - 0 9 o o o 1 6 o + o 1 7 o o o 1 9 - o - 2 6 + o + 2 7 + o + 2 9 o o o 3 6 o + o 3 7 o o - 3 9 o + o 4 6 o o o 4 8 o o + 5 6 + + + 5 8 o o o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file 'extended_periodic_table_vocab.txt' created successfully.\n"
     ]
    }
   ],
   "source": [
    "elements = [\n",
    "    'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',\n",
    "    'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar',\n",
    "    'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn',\n",
    "    'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo',\n",
    "    'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n",
    "    'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho',\n",
    "    'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
    "    'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es',\n",
    "    'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn',\n",
    "    'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n",
    "]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Define symbols\n",
    "symbols = ['o', '+', '-']\n",
    "\n",
    "# Generate all combinations of length 3\n",
    "combinations = [' '.join(combination) for combination in product(symbols, repeat=3)]\n",
    "\n",
    "# Numbers\n",
    "numbers = [str(i) for i in range(10)]\n",
    "\n",
    "# Combine all elements, symbols, and numbers\n",
    "all_tokens = combinations + elements + numbers\n",
    "\n",
    "# Path to the vocabulary file\n",
    "vocab_file_path = 'extended_periodic_table_vocab.txt'\n",
    "\n",
    "# Write all tokens to the vocabulary file\n",
    "with open(vocab_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_tokens))\n",
    "\n",
    "print(f\"Vocabulary file '{vocab_file_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -', '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -', '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -', 'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "class AtomVocabTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n",
    "        super(AtomVocabTokenizer, self).__init__(model_max_length=model_max_length, **kwargs)\n",
    "        \n",
    "        # Load vocabulary from the provided file\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        elements = [\n",
    "            'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',\n",
    "            'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar',\n",
    "            'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Ni', 'Co', 'Cu', 'Zn',\n",
    "            'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo',\n",
    "            'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe',\n",
    "            'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho',\n",
    "            'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg',\n",
    "            'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es',\n",
    "            'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn',\n",
    "            'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og'\n",
    "        ]\n",
    "\n",
    "        from itertools import product\n",
    "\n",
    "        # Define symbols\n",
    "        symbols = ['o', '+', '-']\n",
    "\n",
    "        # Generate all combinations of length 3\n",
    "        combinations = [' '.join(combination) for combination in product(symbols, repeat=3)]\n",
    "\n",
    "        # Numbers\n",
    "        numbers = [str(i) for i in range(10)]\n",
    "\n",
    "        # Combine all elements, symbols, and numbers\n",
    "        self.all_tokens = combinations + elements + numbers\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().splitlines()\n",
    "        return {token: idx for idx, token in enumerate(vocab)}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # List of tokens\n",
    "        tokens = self.all_tokens\n",
    "\n",
    "        # Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "        escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "        # Join the escaped vocab terms into a regex pattern\n",
    "        pattern_str = '|'.join(escaped_tokens)\n",
    "        pattern = re.compile(pattern_str)\n",
    "\n",
    "        # Find all matches in the text\n",
    "        matches = pattern.findall(text)\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def _add_tokens(self, new_tokens, **kwargs):\n",
    "        # Override _add_tokens to add new tokens to the vocabulary\n",
    "        for token in new_tokens:\n",
    "            if token not in self.added_tokens_encoder:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.ids_to_tokens[len(self.ids_to_tokens)] = token\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return list(self.vocab.keys())[index]\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(self.vocab))\n",
    "\n",
    "# Example usage\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n"
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"N F N F N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'F', 'F', 'F', 'K', 'K', 'o o o', '+ o o', 'o o -', 'o - -', 'o o o', 'o - o', 'o + o', 'o o o', 'o o -', '- o -', 'o o o', '- o o', '+ + +', '+ + o', '+ o o', 'o + o', 'o o o', 'o o o', 'o o o', 'o o o', 'o o o', 'o - o', '- o o', '- - o']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "\n",
    "# List of tokens\n",
    "tokens = ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -',\n",
    "          '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -',\n",
    "          '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -',\n",
    "          'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe']\n",
    "\n",
    "# Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "# Join the escaped vocab terms into a regex pattern\n",
    "pattern_str = '|'.join(escaped_tokens)\n",
    "pattern = re.compile(pattern_str)\n",
    "\n",
    "# Find all matches in the text\n",
    "matches = pattern.findall(text)\n",
    "\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', '- o o', '- o o', '- o +', 'o o o', 'o o o', '- o o', '- o +', '- o +', 'o o o', 'o o o']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of tokens\n",
    "tokens = ['o o o', 'o o +', 'o o -', 'o + o', 'o + +', 'o + -', 'o - o', 'o - +', 'o - -',\n",
    "          '+ o o', '+ o +', '+ o -', '+ + o', '+ + +', '+ + -', '+ - o', '+ - +', '+ - -',\n",
    "          '- o o', '- o +', '- o -', '- + o', '- + +', '- + -', '- - o', '- - +', '- - -',\n",
    "          'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ar', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe']\n",
    "\n",
    "# Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "# Join the escaped vocab terms into a regex pattern, allowing for spaces\n",
    "pattern_str = r'(?:' + '|'.join(escaped_tokens) + r')'\n",
    "pattern = re.compile(pattern_str)\n",
    "\n",
    "# Test the pattern on a sample text\n",
    "text = \"P P P P P P P P 0 5 - o o 0 6 - o o 0 6 - o + 0 3 o o o 1 2 o o o 3 5 - o o 3 5 - o + 3 6 - o + 4 7 o o o 5 6 o o o\"\n",
    "matches = pattern.findall(text)\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N F N F N'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "tokenizer.decode([0, 1, 0, 1, 0])"
=======
    "tokenizer.convert_ids_to_tokens(token_ids)"
>>>>>>> fe1a53855c2d8aeb2525037ba6e545cff62edd98
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import os\n",
    "\n",
    "class AtomVocabTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n",
    "        super(AtomVocabTokenizer, self).__init__(model_max_length=model_max_length, **kwargs)\n",
    "        \n",
    "        # Load vocabulary from the provided file\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().splitlines()\n",
<<<<<<< HEAD
    "        return vocab\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        for char in text:\n",
    "            tokens.append(char) if char in self.vocab else tokens.append(self.unk_token)\n",
    "        return tokens\n",
    "    \n",
    "    def _add_tokens(self):\n",
    "        # Override _add_tokens to prevent NotImplementedError\n",
    "        pass\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ''.join(tokens)\n",
=======
    "        return {token: idx for idx, token in enumerate(vocab)}\n",
    "        \n",
    "        \n",
<<<<<<< HEAD
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded String:\", decoded_string)"
=======
    "\n",
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"len of Tokens:\", len(tokens))\n"
>>>>>>> fe1a53855c2d8aeb2525037ba6e545cff62edd98
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "decoded_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(\"Decoded String:\", decoded_string)\n",
    "\n",
    "# Save the custom vocabulary\n",
    "output_vocab_path = 'path/to/your/output/vocab.txt'\n",
    "tokenizer.save_vocabulary(output_vocab_path)\n"
=======
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"len of Tokens:\", len(tokens))\n"
>>>>>>> fe1a53855c2d8aeb2525037ba6e545cff62edd98
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slice_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
