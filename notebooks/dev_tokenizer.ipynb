{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_TOKENIZER_PATH = \"/home/so87pot/n0w0f/regression-transformer/slice-assets/slice_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SliceTokenizer:\n",
    "    def __init__(self, vocab_file=SLICE_TOKENIZER_PATH):\n",
    "        _tokenizer = Tokenizer.from_file(vocab_file)\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "                    tokenizer_object=_tokenizer,\n",
    "                    unk_token=\"[UNK]\",\n",
    "                    pad_token=\"[PAD]\",\n",
    "                    cls_token=\"[CLS]\",\n",
    "                    sep_token=\"[SEP]\",\n",
    "                    mask_token=\"[MASK]\",\n",
    "                )\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structllm.tokenizer.slice_tokenizer import AtomVocabTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['F', 'F', 'F', 'F', 'K', 'K', 'Zn', '0', '6', 'o o o', '0', '6', '+ o o', '0', '4', '1', '4', '- o -', '1', '5', 'o o o', '1', '5', '- o o', '2', '6', '+ + +', '2', '4', '+ + o', '2', '4', '+ o o', '2', '4', 'o + o', '2', '4', 'o o o', '2', '5', 'o o o', '3', '6', 'o o o', '3', '4', 'o o o', '3', '5', 'o o o', '3', '5', 'o - o', '3', '5', '- o o', '3', '5', '- - o']\n",
      "len of Tokens: 60\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"len of Tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 16:28:10.816882: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 16:28:10.956627: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-12 16:28:13.110270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 16:28:19.471794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ga Ga Ga Ga Ga Ga Ga Ga 0 4 o o o 1 5 o o o 2 6 o o o 3 7 o o o'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Ga Ga Ga Ga Ga Ga Ga Ga 0 4 o o o 1 5 o o o 2 6 o o o 3 7 o o o \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['F', 'F', 'F', 'F', 'K', 'K', 'Zn', '0', '6', 'o o o', '0', '6', '+ o o', '0', '4', 'o o -', '0', '4', 'o - -', '0', '5', 'o o o', '0', '5', 'o - o', '1', '6', 'o + o', '1', '6', 'o o o', '1', '4', 'o o -', '1', '4', '- o -', '1', '5', 'o o o', '1', '5', '- o o', '2', '6', '+ + +', '2', '4', '+ + o', '2', '4', '+ o o', '2', '4', 'o + o', '2', '4', 'o o o', '2', '5', 'o o o', '3', '6', 'o o o', '3', '4', 'o o o', '3', '5', 'o o o', '3', '5', 'o - o', '3', '5', '- o o', '3', '5', '- - o']\n",
      "len of Tokens: 79\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "class AtomVocabTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_file, model_max_length=None, **kwargs):\n",
    "        super(AtomVocabTokenizer, self).__init__(model_max_length=model_max_length, **kwargs)\n",
    "        \n",
    "        # Load vocabulary from the provided file\n",
    "        self.vocab = self.load_vocab(vocab_file)\n",
    "        \n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().splitlines()\n",
    "        return {token: idx for idx, token in enumerate(vocab)}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # List of tokens\n",
    "        tokens = list(self.vocab.keys())\n",
    "\n",
    "        # Escape special characters in the vocab to ensure they are treated as literals in the regex\n",
    "        escaped_tokens = [re.escape(token) for token in tokens]\n",
    "\n",
    "        # Join the escaped vocab terms into a regex pattern\n",
    "        pattern_str = '|'.join(escaped_tokens)\n",
    "        pattern = re.compile(pattern_str)\n",
    "\n",
    "        # Find all matches in the text\n",
    "        matches = pattern.findall(text)\n",
    "        return matches\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def _add_tokens(self, new_tokens, **kwargs):\n",
    "        # Override _add_tokens to add new tokens to the vocabulary\n",
    "        for token in new_tokens:\n",
    "            if token not in self.added_tokens_encoder:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.ids_to_tokens[len(self.ids_to_tokens)] = token\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return list(self.vocab.keys())[index]\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        with open(vocab_path, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(self.vocab))\n",
    "\n",
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"len of Tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "vocab_file_path = '/home/so87pot/n0w0f/structllm/notebooks/extended_periodic_table_vocab.txt'\n",
    "tokenizer = AtomVocabTokenizer(vocab_file_path)\n",
    "\n",
    "input_string = \"F F F F K K Zn 0 6 o o o 0 6 + o o 0 4 o o - 0 4 o - - 0 5 o o o 0 5 o - o 1 6 o + o 1 6 o o o 1 4 o o - 1 4 - o - 1 5 o o o 1 5 - o o 2 6 + + + 2 4 + + o 2 4 + o o 2 4 o + o 2 4 o o o 2 5 o o o 3 6 o o o 3 4 o o o 3 5 o o o 3 5 o - o 3 5 - o o 3 5 - - o \"\n",
    "tokens = tokenizer.tokenize(input_string)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"len of Tokens:\", len(tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slice_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
