{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "!module load nvidia/cuda/12.1.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-04-27 18:00:08.852678: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2024-04-27 18:00:10.249069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "2024-04-27 18:00:10.249311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "2024-04-27 18:00:10.379683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2024-04-27 18:00:11.081331: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2024-04-27 18:00:23.672026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
                        "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
                    ]
                }
            ],
            "source": [
                "from trl import SFTTrainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "\n",
                "from torch.utils.data import Dataset\n",
                "from peft import (\n",
                "    LoraConfig, \n",
                "    get_peft_model, \n",
                ")\n",
                "\n",
                "from functools import partial\n",
                "from typing import Any, Dict, List\n",
                "\n",
                "import torch\n",
                "import wandb\n",
                "from datasets import DatasetDict, load_dataset\n",
                "from omegaconf import DictConfig\n",
                "\n",
                "from torch import nn\n",
                "\n",
                "from transformers import (\n",
                "    LlamaTokenizer, \n",
                "    LlamaForSequenceClassification,\n",
                "    AutoTokenizer,\n",
                "    Trainer, \n",
                "    TrainingArguments,\n",
                "    AutoModelForSequenceClassification,\n",
                "    EarlyStoppingCallback,\n",
                "    TrainerCallback,\n",
                ")\n",
                "\n",
                "from mattext.models.utils import CustomWandbCallback_FineTune, EvaluateFirstStepCallback\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "IGNORE_INDEX = -100\n",
                "MAX_LENGTH = 2048\n",
                "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
                "DEFAULT_EOS_TOKEN = \"</s>\"\n",
                "DEFAULT_BOS_TOKEN = \"<s>\"\n",
                "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
                "\n",
                "\n",
                "pretrained_ckpt = \"meta-llama/Llama-2-7b-hf\"\n",
                "\n",
                "llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
                "    pretrained_ckpt,\n",
                "    model_max_length=MAX_LENGTH,\n",
                "    padding_side=\"right\",\n",
                "    use_fast=False,\n",
                "    )\n",
                "special_tokens_dict = dict()\n",
                "if llama_tokenizer.pad_token is None:\n",
                "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
                "if llama_tokenizer.eos_token is None:\n",
                "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
                "if llama_tokenizer.bos_token is None:\n",
                "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
                "if llama_tokenizer.unk_token is None:\n",
                "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
                "\n",
                "num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
                "llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
                "   \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "32001"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(llama_tokenizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def _tokenize(examples):\n",
                "    # Tokenize the 'crystal_llm' column using the LAMA tokenizer\n",
                "    tokenized_examples = llama_tokenizer(examples[\"crystal_llm_rep\"],truncation=True, padding=True,)\n",
                "    return tokenized_examples\n",
                "\n",
                "def _prepare_datasets(path: str) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        Prepare training and validation datasets.\n",
                "\n",
                "        Args:\n",
                "            train_df (pd.DataFrame): DataFrame containing training data.\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict: Dictionary containing training and validation datasets.\n",
                "        \"\"\"\n",
                "\n",
                "        ds = load_dataset(\"json\", data_files=path,split=\"train\")\n",
                "        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n",
                "        return dataset.map(_tokenize, batched=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "03bb3a38f5984fb6a3126742448f01d3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/4858 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "99b655517a1e4f259a9ed6293e8ad61b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/1215 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "dataset = _prepare_datasets(\"/work/so87pot/material_db/matbench_sml/train_matbench_log_kvrh_0.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_dolly(sample):\n",
                "    instruction = f\"<s>[INST] {sample['instruction']}\"\n",
                "    context = f\"Here's some context: {sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
                "    response = f\" [/INST] {sample['response']}\"\n",
                "    # join all the parts together\n",
                "    prompt = \"\".join([i for i in [instruction, context, response] if i is not None])\n",
                "    return prompt\n",
                "\n",
                "# template dataset to add prompt to each sample\n",
                "def template_dataset(sample):\n",
                "    sample[\"text\"] = f\"{format_dolly(sample)}{llama_tokenizer.eos_token}\"\n",
                "    return sample"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shuffle the dataset\n",
                "dataset_shuffled = dataset.shuffle(seed=42)\n",
                "\n",
                "# Select the first 50 rows from the shuffled dataset, comment if you want 15k\n",
                "dataset = dataset_shuffled.select(range(50))\n",
                "\n",
                "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
                "dataset"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}