{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 10:34:04.416479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-22 10:34:04.416691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-22 10:34:04.491738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-22 10:34:04.646810: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 10:34:06.557939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    LlamaTokenizer, \n",
    "    LlamaForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "from structllm.models.utils import CustomWandbCallback_FineTune, EvaluateFirstStepCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "MAX_LENGTH = 2048\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict, \n",
    "    llama_tokenizer, \n",
    "    model,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(llama_tokenizer),pad_to_multiple_of=8)\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "\n",
    "    model.config.pad_token_id = llama_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_ckpt = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    pretrained_ckpt,\n",
    "    model_max_length=MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368f8eff4a914b43a90c83df6caf7085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,198,400 || all params: 6,611,546,112 || trainable%: 0.0635010318143267\n"
     ]
    }
   ],
   "source": [
    "device_map = {\"\": 0}\n",
    "model = LlamaForSequenceClassification.from_pretrained(pretrained_ckpt,\n",
    "                                                            num_labels=1,\n",
    "                                                            quantization_config=bnb_config,\n",
    "                                                            device_map=device_map\n",
    "                                                      )\n",
    "        \n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"TOKEN_CLS\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pad_token': '[PAD]'}\n"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = dict()\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if llama_tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if llama_tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if llama_tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "print(special_tokens_dict)\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=special_tokens_dict,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            model=model,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(examples):\n",
    "    # Tokenize the 'crystal_llm' column using the LAMA tokenizer\n",
    "    tokenized_examples = llama_tokenizer(examples[\"crystal_llm_rep\"],truncation=True, padding=True,)\n",
    "    return tokenized_examples\n",
    "\n",
    "def _prepare_datasets(path: str) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        Prepare training and validation datasets.\n",
    "\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): DataFrame containing training data.\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: Dictionary containing training and validation datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        ds = load_dataset(\"json\", data_files=path,split=\"train\")\n",
    "        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n",
    "        return dataset.map(_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = _prepare_datasets(\"/work/so87pot/material_db/matbench_sml/train_matbench_log_kvrh_0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['cif_p1', 'cif_symmetrized', 'cif_bonding', 'slice', 'composition', 'crystal_llm_rep', 'robocrys_rep', 'wycoff_rep', 'mbid', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4858\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['cif_p1', 'cif_symmetrized', 'cif_bonding', 'slice', 'composition', 'crystal_llm_rep', 'robocrys_rep', 'wycoff_rep', 'mbid', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1215\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = True\n",
    "custom_logger = True\n",
    "def _callbacks(self) -> List[TrainerCallback]:\n",
    "        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n",
    "        callbacks = []\n",
    "\n",
    "        if early_stopping:\n",
    "            callbacks.append(EarlyStoppingCallback(\n",
    "                early_stopping_patience=self.callbacks.early_stopping_patience,\n",
    "                early_stopping_threshold=self.callbacks.early_stopping_threshold\n",
    "            ))\n",
    "\n",
    "        if custom_logger:\n",
    "            callbacks.append(CustomWandbCallback_FineTune())\n",
    "\n",
    "        callbacks.append(EvaluateFirstStepCallback)\n",
    "\n",
    "        return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_metrics(p: Any, eval=True) -> Dict[str, float]:\n",
    "        preds = torch.tensor(p.predictions.squeeze())  # Convert predictions to PyTorch tensor\n",
    "        label_ids = torch.tensor(p.label_ids)  # Convert label_ids to PyTorch tensor\n",
    "\n",
    "        if eval:\n",
    "            # Calculate RMSE as evaluation metric\n",
    "            eval_rmse = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
    "            return {\"eval_rmse\": round(eval_rmse, 3)}\n",
    "        else:\n",
    "            # Calculate RMSE as training metric\n",
    "            loss = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
    "            return {\"train_rmse\": round(loss, 3), \"loss\": round(loss, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "training_args = TrainingArguments(\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n",
    "    greater_is_better=False,  # Lower eval_rmse is better\n",
    "    dataloader_num_workers=2,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    #group_by_length=group_by_length,\n",
    "    output_dir = \"/home/so87pot/n0w0f/structllm/src/structllm/llama_out\",\n",
    "    overwrite_output_dir= True,\n",
    "    num_train_epochs= 2,\n",
    "    per_device_train_batch_size= 8,\n",
    "    save_strategy= \"steps\",\n",
    "    do_eval = True,\n",
    "    evaluation_strategy= 'steps' ,  \n",
    "    logging_strategy= 'steps',\n",
    "    logging_first_step= True,\n",
    "    save_steps= 5 ,# Number of epochs before saving\n",
    "    report_to= \"wandb\",\n",
    "    save_total_limit= 5,\n",
    "    learning_rate= 5e-4,\n",
    "    logging_steps= 5 ,\n",
    "    eval_steps= 5,\n",
    "    seed= 42,\n",
    "    load_best_model_at_end= True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=None,\n",
    "            compute_metrics=_compute_metrics,\n",
    "            tokenizer=llama_tokenizer,\n",
    "            train_dataset=dataset['train'],\n",
    "            eval_dataset=dataset['test'],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpvt-nawaf\u001b[0m (\u001b[33mlama-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/so87pot/n0w0f/structllm/src/structllm/wandb/run-20240422_104605-lbpdyymp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lama-lab/huggingface/runs/lbpdyymp' target=\"_blank\">frosty-tree-12</a></strong> to <a href='https://wandb.ai/lama-lab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lama-lab/huggingface' target=\"_blank\">https://wandb.ai/lama-lab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lama-lab/huggingface/runs/lbpdyymp' target=\"_blank\">https://wandb.ai/lama-lab/huggingface/runs/lbpdyymp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='1216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 136/1216 45:31 < 6:06:52, 0.05 it/s, Epoch 0.22/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.662400</td>\n",
       "      <td>1.010410</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.881400</td>\n",
       "      <td>0.636976</td>\n",
       "      <td>0.798000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.404698</td>\n",
       "      <td>0.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.537788</td>\n",
       "      <td>0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.589570</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.431164</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.347300</td>\n",
       "      <td>0.797490</td>\n",
       "      <td>0.893000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.476000</td>\n",
       "      <td>0.557375</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.517800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>1.756570</td>\n",
       "      <td>1.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.692300</td>\n",
       "      <td>1.528979</td>\n",
       "      <td>1.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.474862</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.246600</td>\n",
       "      <td>6.033188</td>\n",
       "      <td>2.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.891200</td>\n",
       "      <td>0.172354</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.228174</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>1.127073</td>\n",
       "      <td>1.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>0.337256</td>\n",
       "      <td>0.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.553137</td>\n",
       "      <td>0.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.392100</td>\n",
       "      <td>0.212226</td>\n",
       "      <td>0.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.316334</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.139381</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.192901</td>\n",
       "      <td>0.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.426015</td>\n",
       "      <td>0.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.436531</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.144523</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>0.219273</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='124' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [124/152 01:18 < 00:17, 1.56 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /home/so87pot/n0w0f/structllm/src/structllm/llama_out/checkpoint-5 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /home/so87pot/n0w0f/structllm/src/structllm/llama_out/checkpoint-15 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /home/so87pot/n0w0f/structllm/src/structllm/llama_out/checkpoint-20 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /home/so87pot/n0w0f/structllm/src/structllm/llama_out/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /home/so87pot/n0w0f/structllm/src/structllm/llama_out/checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x153f6afbea90>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 153f6a39c490, raw_cell=\"!nvidia-smi\" store_history=True silent=False shell_futures=True cell_id=88a1607b-a208-4127-aaf9-848927bbda5a>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 22 10:26:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB           On | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   28C    P0               74W / 500W|  11885MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1846213      C   ...iniconda3/envs/slice_llm/bin/python    11882MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x153f6afbea90>> (for post_run_cell), with arguments args (<ExecutionResult object at 153f6a39c4c0, execution_count=33 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 153f6a39c490, raw_cell=\"!nvidia-smi\" store_history=True silent=False shell_futures=True cell_id=88a1607b-a208-4127-aaf9-848927bbda5a> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo kill -9 3407242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rasheeda*0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
