{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-16 02:18:43.969920: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "2024-05-16 02:18:43.970118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "2024-05-16 02:18:44.028210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2024-05-16 02:18:44.156164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2024-05-16 02:18:45.456884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from functools import partial\n",
                "from typing import Any, Dict, List\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import wandb\n",
                "from datasets import DatasetDict, load_dataset\n",
                "from omegaconf import DictConfig\n",
                "from peft import (\n",
                "    LoraConfig,\n",
                "    get_peft_model,\n",
                "    prepare_model_for_kbit_training,\n",
                ")\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    EarlyStoppingCallback,\n",
                "    LlamaForSequenceClassification,\n",
                "    #LlamaTokenizer,\n",
                "    Trainer,\n",
                "    TrainerCallback,\n",
                "    TrainingArguments,\n",
                ")\n",
                "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
                "\n",
                "from mattext.models.utils import (\n",
                "    CustomWandbCallback_FineTune,\n",
                "    EvaluateFirstStepCallback,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd8a956d30, raw_cell=\"\n",
                        "IGNORE_INDEX = -100\n",
                        "MAX_LENGTH = 2048\n",
                        "DEFAULT_PAD..\" store_history=True silent=False shell_futures=True cell_id=18242c15-0617-4a8c-9a14-8f1cdc70ebcd>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd56c1fdc0, execution_count=25 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd8a956d30, raw_cell=\"\n",
                        "IGNORE_INDEX = -100\n",
                        "MAX_LENGTH = 2048\n",
                        "DEFAULT_PAD..\" store_history=True silent=False shell_futures=True cell_id=18242c15-0617-4a8c-9a14-8f1cdc70ebcd> result=None>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "\n",
                "IGNORE_INDEX = -100\n",
                "MAX_LENGTH = 2048\n",
                "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
                "DEFAULT_EOS_TOKEN = \"</s>\"\n",
                "DEFAULT_BOS_TOKEN = \"<s>\"\n",
                "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
                "use_flash_attention = True\n",
                "\n",
                "def smart_tokenizer_and_embedding_resize(\n",
                "    special_tokens_dict,\n",
                "    llama_tokenizer,\n",
                "    model,\n",
                "):\n",
                "    \"\"\"Resize tokenizer and embedding.\n",
                "\n",
                "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
                "    \"\"\"\n",
                "    num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
                "    llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
                "    model.resize_token_embeddings(len(llama_tokenizer), pad_to_multiple_of=8)\n",
                "\n",
                "    if num_new_tokens > 0:\n",
                "        input_embeddings = model.get_input_embeddings().weight.data\n",
                "        output_embeddings = model.get_output_embeddings().weight.data\n",
                "\n",
                "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
                "            dim=0, keepdim=True\n",
                "        )\n",
                "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
                "\n",
                "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
                "\n",
                "    model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
                "    output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "def _setup_model_tokenizer() -> None:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\n",
                "        \"meta-llama/Llama-2-7b-hf\",\n",
                "        model_max_length=MAX_LENGTH,\n",
                "        padding_side=\"right\",\n",
                "        use_fast=False,\n",
                "    )\n",
                "    #tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "\n",
                "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
                "                                    bnb_4bit_use_double_quant=True, \n",
                "                                    bnb_4bit_quant_type=\"nf4\", \n",
                "                                    bnb_4bit_compute_dtype=torch.bfloat16)\n",
                "\n",
                "    device_map = \"auto\" \n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        \"meta-llama/Llama-2-7b-hf\",\n",
                "        use_cache=False,\n",
                "        use_flash_attention_2=use_flash_attention,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=device_map,\n",
                "    )\n",
                "\n",
                "    peft_config = LoraConfig(\n",
                "                    lora_alpha=16,\n",
                "                    lora_dropout=0.1,\n",
                "                    r=64,\n",
                "                    bias=\"none\",\n",
                "                    task_type=\"CAUSAL_LM\",)\n",
                "    \n",
                "    #model = prepare_model_for_kbit_training(model)\n",
                "    #model = get_peft_model(model, peft_config)\n",
                "\n",
                "    special_tokens_dict = dict()\n",
                "    if tokenizer.pad_token is None:\n",
                "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
                "    if tokenizer.eos_token is None:\n",
                "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
                "    if tokenizer.bos_token is None:\n",
                "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
                "    if tokenizer.unk_token is None:\n",
                "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
                "\n",
                "    smart_tokenizer_and_embedding_resize(\n",
                "        special_tokens_dict=special_tokens_dict,\n",
                "        llama_tokenizer=tokenizer,\n",
                "        model=model,\n",
                "    )\n",
                "\n",
                "\n",
                "    print(len(tokenizer))\n",
                "    return model, tokenizer, peft_config\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd19952340, raw_cell=\"model\" store_history=True silent=False shell_futures=True cell_id=16a2197d-3599-4011-87b2-66b9beae9179>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "LlamaForCausalLM(\n",
                            "  (model): LlamaModel(\n",
                            "    (embed_tokens): Embedding(32008, 4096)\n",
                            "    (layers): ModuleList(\n",
                            "      (0-31): 32 x LlamaDecoderLayer(\n",
                            "        (self_attn): LlamaFlashAttention2(\n",
                            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                            "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                            "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                            "          (rotary_emb): LlamaRotaryEmbedding()\n",
                            "        )\n",
                            "        (mlp): LlamaMLP(\n",
                            "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
                            "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
                            "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
                            "          (act_fn): SiLU()\n",
                            "        )\n",
                            "        (input_layernorm): LlamaRMSNorm()\n",
                            "        (post_attention_layernorm): LlamaRMSNorm()\n",
                            "      )\n",
                            "    )\n",
                            "    (norm): LlamaRMSNorm()\n",
                            "  )\n",
                            "  (lm_head): Linear(in_features=4096, out_features=32008, bias=False)\n",
                            ")"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd199521f0, execution_count=29 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd19952340, raw_cell=\"model\" store_history=True silent=False shell_futures=True cell_id=16a2197d-3599-4011-87b2-66b9beae9179> result=LlamaForCausalLM(\n",
                        "  (model): LlamaModel(\n",
                        "    (embed_tokens): Embedding(32008, 4096)\n",
                        "    (layers): ModuleList(\n",
                        "      (0-31): 32 x LlamaDecoderLayer(\n",
                        "        (self_attn): LlamaFlashAttention2(\n",
                        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                        "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                        "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
                        "          (rotary_emb): LlamaRotaryEmbedding()\n",
                        "        )\n",
                        "        (mlp): LlamaMLP(\n",
                        "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
                        "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
                        "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
                        "          (act_fn): SiLU()\n",
                        "        )\n",
                        "        (input_layernorm): LlamaRMSNorm()\n",
                        "        (post_attention_layernorm): LlamaRMSNorm()\n",
                        "      )\n",
                        "    )\n",
                        "    (norm): LlamaRMSNorm()\n",
                        "  )\n",
                        "  (lm_head): Linear(in_features=4096, out_features=32008, bias=False)\n",
                        ")>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd56b2fbb0, raw_cell=\"model, tokenizer,peft_config = _setup_model_tokeni..\" store_history=True silent=False shell_futures=True cell_id=caf4527d-110d-4726-bab5-8bb5389588fd>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a83eef77032f4f998d98c07380abd893",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "32001\n",
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd56b2faf0, execution_count=28 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd56b2fbb0, raw_cell=\"model, tokenizer,peft_config = _setup_model_tokeni..\" store_history=True silent=False shell_futures=True cell_id=caf4527d-110d-4726-bab5-8bb5389588fd> result=None>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "model, tokenizer,peft_config = _setup_model_tokenizer()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "ds = load_dataset(\"json\", data_files=\"/work/so87pot/material_db/all_1/train_matbench_log_gvrh_0.json\", split=\"train\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dataset size: 6073\n"
                    ]
                }
            ],
            "source": [
                "from random import randrange\n",
                "print(f\"dataset size: {len(ds)}\")\n",
                "#print(ds[randrange(len(ds))])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Dataset({\n",
                            "    features: ['atoms', 'crystal_llm_rep', 'atoms_params', 'wycoff_rep', 'mbid', 'cif_symmetrized', 'zmatrix', 'slice', 'cif_p1', 'composition', 'labels'],\n",
                            "    num_rows: 6073\n",
                            "})"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd56baf970, raw_cell=\"def format_instruction(sample):\n",
                        "\treturn [f\"\"\"### I..\" store_history=True silent=False shell_futures=True cell_id=4dad3206-c3ee-4469-b3a8-2c8d22352ef9>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "59351fc84bae4b2fa221e3e83e5b322c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/6073 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd56baf250, execution_count=40 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd56baf970, raw_cell=\"def format_instruction(sample):\n",
                        "\treturn [f\"\"\"### I..\" store_history=True silent=False shell_futures=True cell_id=4dad3206-c3ee-4469-b3a8-2c8d22352ef9> result=None>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "def format_instruction(sample):\n",
                "\treturn [f\"\"\"### Instruction:\n",
                "Use the Input below to create an instruction, which could have been used to generate the input using an LLM.\n",
                " \n",
                "### Input:\n",
                "{sample['composition']}\n",
                " \n",
                "The response is ### Response:\n",
                "{sample['labels']}\"\"\"]\n",
                "\n",
                "\n",
                "\n",
                "def template_dataset(sample):\n",
                "    sample[\"text\"] = f\"{format_instruction(sample)}\"\n",
                "    return sample\n",
                "\n",
                "dataset = ds.map(template_dataset, remove_columns=list(ds.features))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd5676b880, raw_cell=\"response_template_with_context = \"response is ### ..\" store_history=True silent=False shell_futures=True cell_id=15aa85a1-0ba1-4925-9482-4388109f071e>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "response is ### Response [835, 13291]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ad523824a02047edbc50092144b0bd3b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/6073 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd19d5ec10, execution_count=41 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd5676b880, raw_cell=\"response_template_with_context = \"response is ### ..\" store_history=True silent=False shell_futures=True cell_id=15aa85a1-0ba1-4925-9482-4388109f071e> result=None>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "response_template_with_context = \"response is ### Response\"# \"\\n### Response:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
                "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
                "print(response_template_with_context,response_template_ids)\n",
                "\n",
                "data_collator = DataCollatorForCompletionOnlyLM(response_template_ids,tokenizer=tokenizer)\n",
                "\n",
                "args = TrainingArguments(\n",
                "    output_dir=\"try-llama\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
                "    gradient_accumulation_steps=2,\n",
                "    gradient_checkpointing=True,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    logging_steps=10,\n",
                "    save_strategy=\"epoch\",\n",
                "    learning_rate=2e-4,\n",
                "    bf16=True,\n",
                "    tf32=True,\n",
                "    max_grad_norm=0.3,\n",
                "    warmup_ratio=0.03,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
                ")\n",
                "\n",
                "from trl import SFTTrainer\n",
                " \n",
                "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
                " \n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    max_seq_length=max_seq_length,\n",
                "    tokenizer=tokenizer,\n",
                "    packing=False,\n",
                "    data_collator=data_collator,\n",
                "    dataset_text_field=\"text\",\n",
                "    #formatting_func=format_instruction,\n",
                "    args=args,\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd185c13d0, raw_cell=\"dataset\" store_history=True silent=False shell_futures=True cell_id=c4c54eaa-8633-4cb8-9b84-f5aeeb9a71cb>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Dataset({\n",
                            "    features: ['text'],\n",
                            "    num_rows: 6073\n",
                            "})"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd185c1850, execution_count=42 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd185c13d0, raw_cell=\"dataset\" store_history=True silent=False shell_futures=True cell_id=c4c54eaa-8633-4cb8-9b84-f5aeeb9a71cb> result=Dataset({\n",
                        "    features: ['text'],\n",
                        "    num_rows: 6073\n",
                        "})>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd18620340, raw_cell=\"tokenizer.encode(\"\\n\\n### Response:\\n1.72427586960..\" store_history=True silent=False shell_futures=True cell_id=19f22e16-b70f-4317-9592-386d246e63a4>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[1,\n",
                            " 29871,\n",
                            " 13,\n",
                            " 13,\n",
                            " 2277,\n",
                            " 29937,\n",
                            " 13291,\n",
                            " 29901,\n",
                            " 13,\n",
                            " 29896,\n",
                            " 29889,\n",
                            " 29955,\n",
                            " 29906,\n",
                            " 29946,\n",
                            " 29906,\n",
                            " 29955,\n",
                            " 29945,\n",
                            " 29947,\n",
                            " 29953,\n",
                            " 29929,\n",
                            " 29953,\n",
                            " 29900,\n",
                            " 29900,\n",
                            " 29955,\n",
                            " 29947,\n",
                            " 29929]"
                        ]
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd186200a0, execution_count=36 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd18620340, raw_cell=\"tokenizer.encode(\"\\n\\n### Response:\\n1.72427586960..\" store_history=True silent=False shell_futures=True cell_id=19f22e16-b70f-4317-9592-386d246e63a4> result=[1, 29871, 13, 13, 2277, 29937, 13291, 29901, 13, 29896, 29889, 29955, 29906, 29946, 29906, 29955, 29945, 29947, 29953, 29929, 29953, 29900, 29900, 29955, 29947, 29929]>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "tokenizer.encode(\"\\n\\n### Response:\\n1.724275869600789\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14fd185c12e0, raw_cell=\"trainer.train()\" store_history=True silent=False shell_futures=True cell_id=6efb2f0f-b870-434f-a1ed-efdc1a8be158>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'loss': 2.5563, 'grad_norm': 0.3671875, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
                        "{'loss': 1.9523, 'grad_norm': 1.9296875, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
                        "{'loss': 1.8117, 'grad_norm': 0.142578125, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
                        "{'loss': 1.75, 'grad_norm': 0.2392578125, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
                        "{'loss': 1.7561, 'grad_norm': 0.1708984375, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
                        "{'loss': 1.6973, 'grad_norm': 0.3984375, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
                        "{'loss': 1.6412, 'grad_norm': 0.44140625, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
                        "{'loss': 1.5684, 'grad_norm': 0.875, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
                        "{'loss': 1.4363, 'grad_norm': 0.8125, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
                        "{'loss': 1.3175, 'grad_norm': 0.7890625, 'learning_rate': 0.0002, 'epoch': 0.2}\n",
                        "{'loss': 1.2526, 'grad_norm': 1.3515625, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
                        "{'loss': 1.172, 'grad_norm': 1.5, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
                        "{'loss': 1.1475, 'grad_norm': 1.7734375, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
                        "{'loss': 1.0005, 'grad_norm': 1.5859375, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
                        "{'loss': 0.8779, 'grad_norm': 1.4375, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
                        "{'loss': 0.8517, 'grad_norm': 2.21875, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
                        "{'loss': 0.6309, 'grad_norm': 1.328125, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
                        "{'loss': 0.6534, 'grad_norm': 1.1328125, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
                        "{'loss': 0.5671, 'grad_norm': 2.109375, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
                        "{'loss': 0.6023, 'grad_norm': 1.2890625, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
                        "{'loss': 0.537, 'grad_norm': 1.15625, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
                        "{'loss': 0.4972, 'grad_norm': 1.9765625, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
                        "{'loss': 0.467, 'grad_norm': 1.1875, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
                        "{'loss': 0.4402, 'grad_norm': 0.91796875, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
                        "{'loss': 0.4452, 'grad_norm': 1.578125, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
                        "{'loss': 0.488, 'grad_norm': 0.81640625, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
                        "{'loss': 0.3609, 'grad_norm': 1.4921875, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
                        "{'loss': 0.3521, 'grad_norm': 0.89453125, 'learning_rate': 0.0002, 'epoch': 0.55}\n",
                        "{'loss': 0.3807, 'grad_norm': 0.4453125, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
                        "{'loss': 0.4017, 'grad_norm': 2.515625, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
                        "{'loss': 0.3347, 'grad_norm': 1.1015625, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
                        "{'loss': 0.3671, 'grad_norm': 0.62109375, 'learning_rate': 0.0002, 'epoch': 0.63}\n",
                        "{'loss': 0.3336, 'grad_norm': 1.5703125, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
                        "{'loss': 0.3483, 'grad_norm': 1.1328125, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
                        "{'loss': 0.3951, 'grad_norm': 1.25, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
                        "{'loss': 0.3171, 'grad_norm': 1.3046875, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
                        "{'loss': 0.3501, 'grad_norm': 0.515625, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
                        "{'loss': 0.3826, 'grad_norm': 1.671875, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
                        "{'loss': 0.3619, 'grad_norm': 1.3671875, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
                        "{'loss': 0.3234, 'grad_norm': 1.1015625, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
                        "{'loss': 0.3343, 'grad_norm': 0.609375, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
                        "{'loss': 0.3416, 'grad_norm': 0.70703125, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
                        "{'loss': 0.3273, 'grad_norm': 1.765625, 'learning_rate': 0.0002, 'epoch': 0.85}\n",
                        "{'loss': 0.3186, 'grad_norm': 0.75, 'learning_rate': 0.0002, 'epoch': 0.87}\n",
                        "{'loss': 0.2827, 'grad_norm': 0.85546875, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
                        "{'loss': 0.2878, 'grad_norm': 0.70703125, 'learning_rate': 0.0002, 'epoch': 0.91}\n",
                        "{'loss': 0.354, 'grad_norm': 0.8125, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
                        "{'loss': 0.3302, 'grad_norm': 0.94921875, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
                        "{'loss': 0.3404, 'grad_norm': 1.2890625, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
                        "{'loss': 0.3109, 'grad_norm': 1.2734375, 'learning_rate': 0.0002, 'epoch': 0.99}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
                        "  warnings.warn(\n",
                        "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'loss': 0.2769, 'grad_norm': 1.4921875, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
                        "{'loss': 0.3086, 'grad_norm': 0.796875, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
                        "{'loss': 0.3115, 'grad_norm': 0.5625, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
                        "{'loss': 0.2499, 'grad_norm': 0.279296875, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
                        "{'loss': 0.2893, 'grad_norm': 1.1484375, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
                        "{'loss': 0.3045, 'grad_norm': 1.125, 'learning_rate': 0.0002, 'epoch': 1.11}\n",
                        "{'loss': 0.3333, 'grad_norm': 1.4453125, 'learning_rate': 0.0002, 'epoch': 1.13}\n",
                        "{'loss': 0.3477, 'grad_norm': 0.77734375, 'learning_rate': 0.0002, 'epoch': 1.15}\n",
                        "{'loss': 0.2807, 'grad_norm': 0.59765625, 'learning_rate': 0.0002, 'epoch': 1.16}\n",
                        "{'loss': 0.2679, 'grad_norm': 0.63671875, 'learning_rate': 0.0002, 'epoch': 1.18}\n",
                        "{'loss': 0.3278, 'grad_norm': 0.8125, 'learning_rate': 0.0002, 'epoch': 1.2}\n",
                        "{'loss': 0.2873, 'grad_norm': 0.53515625, 'learning_rate': 0.0002, 'epoch': 1.22}\n",
                        "{'loss': 0.2761, 'grad_norm': 0.76953125, 'learning_rate': 0.0002, 'epoch': 1.24}\n",
                        "{'loss': 0.2503, 'grad_norm': 0.490234375, 'learning_rate': 0.0002, 'epoch': 1.26}\n",
                        "{'loss': 0.2759, 'grad_norm': 0.6953125, 'learning_rate': 0.0002, 'epoch': 1.28}\n",
                        "{'loss': 0.2598, 'grad_norm': 1.2421875, 'learning_rate': 0.0002, 'epoch': 1.3}\n",
                        "{'loss': 0.2779, 'grad_norm': 0.37109375, 'learning_rate': 0.0002, 'epoch': 1.32}\n",
                        "{'loss': 0.2538, 'grad_norm': 1.234375, 'learning_rate': 0.0002, 'epoch': 1.34}\n",
                        "{'loss': 0.2523, 'grad_norm': 0.91015625, 'learning_rate': 0.0002, 'epoch': 1.36}\n",
                        "{'loss': 0.2941, 'grad_norm': 0.337890625, 'learning_rate': 0.0002, 'epoch': 1.38}\n",
                        "{'loss': 0.2634, 'grad_norm': 0.26953125, 'learning_rate': 0.0002, 'epoch': 1.4}\n",
                        "{'loss': 0.2909, 'grad_norm': 0.251953125, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
                        "{'loss': 0.301, 'grad_norm': 1.5390625, 'learning_rate': 0.0002, 'epoch': 1.44}\n",
                        "{'loss': 0.2957, 'grad_norm': 0.5546875, 'learning_rate': 0.0002, 'epoch': 1.46}\n",
                        "{'loss': 0.2747, 'grad_norm': 0.59375, 'learning_rate': 0.0002, 'epoch': 1.48}\n",
                        "{'loss': 0.2383, 'grad_norm': 0.158203125, 'learning_rate': 0.0002, 'epoch': 1.5}\n",
                        "{'loss': 0.2861, 'grad_norm': 1.4921875, 'learning_rate': 0.0002, 'epoch': 1.52}\n",
                        "{'loss': 0.2765, 'grad_norm': 1.421875, 'learning_rate': 0.0002, 'epoch': 1.54}\n",
                        "{'loss': 0.2527, 'grad_norm': 0.890625, 'learning_rate': 0.0002, 'epoch': 1.56}\n",
                        "{'loss': 0.2772, 'grad_norm': 0.90234375, 'learning_rate': 0.0002, 'epoch': 1.58}\n",
                        "{'loss': 0.2807, 'grad_norm': 0.62890625, 'learning_rate': 0.0002, 'epoch': 1.6}\n",
                        "{'loss': 0.2706, 'grad_norm': 1.609375, 'learning_rate': 0.0002, 'epoch': 1.62}\n",
                        "{'loss': 0.2493, 'grad_norm': 0.3359375, 'learning_rate': 0.0002, 'epoch': 1.64}\n",
                        "{'loss': 0.2955, 'grad_norm': 1.4453125, 'learning_rate': 0.0002, 'epoch': 1.66}\n",
                        "{'loss': 0.2993, 'grad_norm': 1.1484375, 'learning_rate': 0.0002, 'epoch': 1.68}\n",
                        "{'loss': 0.2715, 'grad_norm': 0.91015625, 'learning_rate': 0.0002, 'epoch': 1.7}\n",
                        "{'loss': 0.289, 'grad_norm': 0.515625, 'learning_rate': 0.0002, 'epoch': 1.72}\n",
                        "{'loss': 0.2444, 'grad_norm': 0.54296875, 'learning_rate': 0.0002, 'epoch': 1.74}\n",
                        "{'loss': 0.3141, 'grad_norm': 0.298828125, 'learning_rate': 0.0002, 'epoch': 1.76}\n",
                        "{'loss': 0.2556, 'grad_norm': 0.41015625, 'learning_rate': 0.0002, 'epoch': 1.78}\n",
                        "{'loss': 0.2254, 'grad_norm': 0.60546875, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
                        "{'loss': 0.2742, 'grad_norm': 1.140625, 'learning_rate': 0.0002, 'epoch': 1.82}\n",
                        "{'loss': 0.2519, 'grad_norm': 1.1875, 'learning_rate': 0.0002, 'epoch': 1.84}\n",
                        "{'loss': 0.2495, 'grad_norm': 0.310546875, 'learning_rate': 0.0002, 'epoch': 1.86}\n",
                        "{'loss': 0.2289, 'grad_norm': 0.58203125, 'learning_rate': 0.0002, 'epoch': 1.88}\n",
                        "{'loss': 0.2361, 'grad_norm': 0.703125, 'learning_rate': 0.0002, 'epoch': 1.9}\n",
                        "{'loss': 0.251, 'grad_norm': 1.21875, 'learning_rate': 0.0002, 'epoch': 1.92}\n",
                        "{'loss': 0.2213, 'grad_norm': 0.1904296875, 'learning_rate': 0.0002, 'epoch': 1.93}\n",
                        "{'loss': 0.2573, 'grad_norm': 0.494140625, 'learning_rate': 0.0002, 'epoch': 1.95}\n",
                        "{'loss': 0.3252, 'grad_norm': 0.216796875, 'learning_rate': 0.0002, 'epoch': 1.97}\n",
                        "{'loss': 0.2345, 'grad_norm': 0.2412109375, 'learning_rate': 0.0002, 'epoch': 1.99}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
                        "  warnings.warn(\n",
                        "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'loss': 0.2386, 'grad_norm': 1.265625, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
                        "{'loss': 0.2511, 'grad_norm': 0.9609375, 'learning_rate': 0.0002, 'epoch': 2.03}\n",
                        "{'loss': 0.2579, 'grad_norm': 0.369140625, 'learning_rate': 0.0002, 'epoch': 2.05}\n",
                        "{'loss': 0.2284, 'grad_norm': 0.3671875, 'learning_rate': 0.0002, 'epoch': 2.07}\n",
                        "{'loss': 0.2642, 'grad_norm': 2.28125, 'learning_rate': 0.0002, 'epoch': 2.09}\n",
                        "{'loss': 0.2278, 'grad_norm': 0.59375, 'learning_rate': 0.0002, 'epoch': 2.11}\n",
                        "{'loss': 0.2171, 'grad_norm': 0.4140625, 'learning_rate': 0.0002, 'epoch': 2.13}\n",
                        "{'loss': 0.2396, 'grad_norm': 0.57421875, 'learning_rate': 0.0002, 'epoch': 2.15}\n",
                        "{'loss': 0.2173, 'grad_norm': 0.703125, 'learning_rate': 0.0002, 'epoch': 2.17}\n",
                        "{'loss': 0.2444, 'grad_norm': 0.2314453125, 'learning_rate': 0.0002, 'epoch': 2.19}\n",
                        "{'loss': 0.2482, 'grad_norm': 0.2236328125, 'learning_rate': 0.0002, 'epoch': 2.21}\n",
                        "{'loss': 0.2722, 'grad_norm': 0.2255859375, 'learning_rate': 0.0002, 'epoch': 2.23}\n",
                        "{'loss': 0.2288, 'grad_norm': 0.1435546875, 'learning_rate': 0.0002, 'epoch': 2.25}\n",
                        "{'loss': 0.243, 'grad_norm': 0.625, 'learning_rate': 0.0002, 'epoch': 2.27}\n",
                        "{'loss': 0.2455, 'grad_norm': 0.318359375, 'learning_rate': 0.0002, 'epoch': 2.29}\n",
                        "{'loss': 0.2344, 'grad_norm': 0.625, 'learning_rate': 0.0002, 'epoch': 2.31}\n",
                        "{'loss': 0.2532, 'grad_norm': 2.203125, 'learning_rate': 0.0002, 'epoch': 2.33}\n",
                        "{'loss': 0.2309, 'grad_norm': 1.640625, 'learning_rate': 0.0002, 'epoch': 2.35}\n",
                        "{'loss': 0.2326, 'grad_norm': 0.1640625, 'learning_rate': 0.0002, 'epoch': 2.37}\n",
                        "{'loss': 0.2495, 'grad_norm': 0.52734375, 'learning_rate': 0.0002, 'epoch': 2.39}\n",
                        "{'loss': 0.2439, 'grad_norm': 0.154296875, 'learning_rate': 0.0002, 'epoch': 2.41}\n",
                        "{'loss': 0.2168, 'grad_norm': 0.27734375, 'learning_rate': 0.0002, 'epoch': 2.43}\n",
                        "{'loss': 0.2295, 'grad_norm': 0.265625, 'learning_rate': 0.0002, 'epoch': 2.45}\n",
                        "{'loss': 0.2442, 'grad_norm': 0.166015625, 'learning_rate': 0.0002, 'epoch': 2.47}\n",
                        "{'loss': 0.2484, 'grad_norm': 1.234375, 'learning_rate': 0.0002, 'epoch': 2.49}\n",
                        "{'loss': 0.2465, 'grad_norm': 1.2421875, 'learning_rate': 0.0002, 'epoch': 2.51}\n",
                        "{'loss': 0.2544, 'grad_norm': 0.490234375, 'learning_rate': 0.0002, 'epoch': 2.53}\n",
                        "{'loss': 0.22, 'grad_norm': 0.1337890625, 'learning_rate': 0.0002, 'epoch': 2.55}\n",
                        "{'loss': 0.2191, 'grad_norm': 0.6484375, 'learning_rate': 0.0002, 'epoch': 2.57}\n",
                        "{'loss': 0.2235, 'grad_norm': 0.4140625, 'learning_rate': 0.0002, 'epoch': 2.59}\n",
                        "{'loss': 0.2336, 'grad_norm': 0.2734375, 'learning_rate': 0.0002, 'epoch': 2.61}\n",
                        "{'loss': 0.2348, 'grad_norm': 1.0078125, 'learning_rate': 0.0002, 'epoch': 2.63}\n",
                        "{'loss': 0.2313, 'grad_norm': 0.66796875, 'learning_rate': 0.0002, 'epoch': 2.65}\n",
                        "{'loss': 0.2518, 'grad_norm': 0.447265625, 'learning_rate': 0.0002, 'epoch': 2.67}\n",
                        "{'loss': 0.2237, 'grad_norm': 0.431640625, 'learning_rate': 0.0002, 'epoch': 2.69}\n",
                        "{'loss': 0.2615, 'grad_norm': 1.3828125, 'learning_rate': 0.0002, 'epoch': 2.7}\n",
                        "{'loss': 0.2193, 'grad_norm': 0.28125, 'learning_rate': 0.0002, 'epoch': 2.72}\n",
                        "{'loss': 0.2174, 'grad_norm': 0.25390625, 'learning_rate': 0.0002, 'epoch': 2.74}\n",
                        "{'loss': 0.2395, 'grad_norm': 0.57421875, 'learning_rate': 0.0002, 'epoch': 2.76}\n",
                        "{'loss': 0.2208, 'grad_norm': 0.158203125, 'learning_rate': 0.0002, 'epoch': 2.78}\n",
                        "{'loss': 0.275, 'grad_norm': 0.40625, 'learning_rate': 0.0002, 'epoch': 2.8}\n",
                        "{'loss': 0.2281, 'grad_norm': 0.412109375, 'learning_rate': 0.0002, 'epoch': 2.82}\n",
                        "{'loss': 0.2281, 'grad_norm': 0.1201171875, 'learning_rate': 0.0002, 'epoch': 2.84}\n",
                        "{'loss': 0.2251, 'grad_norm': 0.4765625, 'learning_rate': 0.0002, 'epoch': 2.86}\n",
                        "{'loss': 0.2142, 'grad_norm': 0.1611328125, 'learning_rate': 0.0002, 'epoch': 2.88}\n",
                        "{'loss': 0.2469, 'grad_norm': 0.78125, 'learning_rate': 0.0002, 'epoch': 2.9}\n",
                        "{'loss': 0.2086, 'grad_norm': 0.1962890625, 'learning_rate': 0.0002, 'epoch': 2.92}\n",
                        "{'loss': 0.2435, 'grad_norm': 0.1904296875, 'learning_rate': 0.0002, 'epoch': 2.94}\n",
                        "{'loss': 0.2499, 'grad_norm': 0.244140625, 'learning_rate': 0.0002, 'epoch': 2.96}\n",
                        "{'loss': 0.2044, 'grad_norm': 0.208984375, 'learning_rate': 0.0002, 'epoch': 2.98}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/so87pot/miniconda3/envs/slice_llm/lib/python3.9/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'train_runtime': 1468.153, 'train_samples_per_second': 12.409, 'train_steps_per_second': 1.034, 'train_loss': 0.4176751161595421, 'epoch': 3.0}\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=1518, training_loss=0.4176751161595421, metrics={'train_runtime': 1468.153, 'train_samples_per_second': 12.409, 'train_steps_per_second': 1.034, 'train_loss': 0.4176751161595421, 'epoch': 3.0})"
                        ]
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14fd8be22790>> (for post_run_cell), with arguments args (<ExecutionResult object at 14fd185c18e0, execution_count=43 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14fd185c12e0, raw_cell=\"trainer.train()\" store_history=True silent=False shell_futures=True cell_id=6efb2f0f-b870-434f-a1ed-efdc1a8be158> result=TrainOutput(global_step=1518, training_loss=0.4176751161595421, metrics={'train_runtime': 1468.153, 'train_samples_per_second': 12.409, 'train_steps_per_second': 1.034, 'train_loss': 0.4176751161595421, 'epoch': 3.0})>,),kwargs {}:\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
                    ]
                }
            ],
            "source": [
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "    def formatting_prompts_func(self,example):\n",
                "        output_texts = []\n",
                "        for i in range(len(example['labels'])):\n",
                "            text = f\"### Instruction: Below is a {self.material_} represented as string. Followed by a question. Write a response to the question.{example[self.representation]}Question: What is the {self.property_} of this material ? ### Answer: {round(float(example['labels'][i]),3)}\"\n",
                "            output_texts.append(text)\n",
                "        return output_texts\n",
                "\n",
                "### Response: {round(float(sample['labels']),3)}\n",
                "\n",
                "    def format_qstns(self, sample):\n",
                "        print(sample['labels'])\n",
                "        return f\"\"\"### Instruction: Below is a {self.material_} represented as string. Followed by a question. Write a response to the question.{sample[self.representation]}Question: What is the {self.property_} of this material ?\\n### Response: {round(float(sample['labels']),3)}\"\"\"\n",
                "\\\n",
                "\n",
                "    def _prepare_datasets(self, path: str) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        Prepare training and validation datasets.\n",
                "\n",
                "        Args:\n",
                "            train_df (pd.DataFrame): DataFrame containing training data.\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict: Dictionary containing training and validation datasets.\n",
                "        \"\"\"\n",
                "\n",
                "        ds = load_dataset(\"json\", data_files=path, split=\"train\")\n",
                "        ds = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n",
                "        #ds = dataset.map(self._tokenize, batched=True)\n",
                "\n",
                "        # trainset = ds[\"train\"].map(self.format_qstns)\n",
                "        # testset = ds[\"test\"].map(self.format_qstns)\n",
                "        # print(trainset[0][\"text\"])\n",
                "        # print(testset[0][\"text\"])\n",
                "        # return trainset, testset\n",
                "        return ds['train'], ds['test']\n",
                "\n",
                "    def _callbacks(self) -> List[TrainerCallback]:\n",
                "        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n",
                "        callbacks = []\n",
                "\n",
                "        if self.callbacks.early_stopping:\n",
                "            callbacks.append(\n",
                "                EarlyStoppingCallback(\n",
                "                    early_stopping_patience=self.callbacks.early_stopping_patience,\n",
                "                    early_stopping_threshold=self.callbacks.early_stopping_threshold,\n",
                "                )\n",
                "            )\n",
                "\n",
                "        if self.callbacks.custom_logger:\n",
                "            callbacks.append(CustomWandbCallback_FineTune())\n",
                "\n",
                "        #callbacks.append(GenerationCallback)\n",
                "        #callbacks.append(EvaluateFirstStepCallback)\n",
                "        return callbacks\n",
                "\n",
                "\n",
                "    def finetune(self) -> None:\n",
                "        \"\"\"\n",
                "        Perform fine-tuning of the language model.\n",
                "        \"\"\"\n",
                "\n",
                "        config_train_args = self.cfg.training_arguments\n",
                "        callbacks = self._callbacks()\n",
                "\n",
                "        # os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
                "        training_args = TrainingArguments(\n",
                "            **config_train_args,\n",
                "        )\n",
                "        #response_template = \" ### Response:\"\n",
                "\n",
                "        response_template_with_context = \"?\\n### Response:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
                "        response_template_ids = self.tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
                "\n",
                "        data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=self.tokenizer)\n",
                "\n",
                "        #collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=self.tokenizer)\n",
                "\n",
                "        max_seq_length = MAX_LENGTH\n",
                "        packing = False\n",
                "        trainer = SFTTrainer(\n",
                "            model=self.model,\n",
                "            peft_config=self.peft_config,\n",
                "            train_dataset=self.trainset,\n",
                "            #dataset_text_field=\"text\",\n",
                "            formatting_func=self.formatting_prompts_func,\n",
                "            max_seq_length=max_seq_length,\n",
                "            tokenizer=self.tokenizer,\n",
                "            args=training_args,\n",
                "            packing=packing,\n",
                "            callbacks=callbacks,\n",
                "            data_collator=data_collator,\n",
                "        )\n",
                "\n",
                "        wandb.log({\"Training Arguments\": str(config_train_args)})\n",
                "        wandb.log({\"model_summary\": str(self.model)})\n",
                "\n",
                "        trainer.save_model(\n",
                "            f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-no-fine-tune\"\n",
                "        )\n",
                "        trainer.train()\n",
                "        trainer.save_state(\n",
                "            f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\"\n",
                "        )\n",
                "        trainer.save_model(\n",
                "            f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\"\n",
                "        )\n",
                "        # eval_result = trainer.evaluate(eval_dataset=self.tokenized_dataset['test'])\n",
                "        # wandb.log(eval_result)\n",
                "\n",
                "        self.model.save_pretrained(f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-save-pretrain\", save_config=True)\n",
                "        wandb.finish()\n",
                "        return self.cfg.path.finetuned_modelname\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "IGNORE_INDEX = -100\n",
                "MAX_LENGTH = 2048\n",
                "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
                "DEFAULT_EOS_TOKEN = \"</s>\"\n",
                "DEFAULT_BOS_TOKEN = \"<s>\"\n",
                "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class FinetuneLLama():\n",
                "    \"\"\"Class to perform finetuning of a language model.\n",
                "        Initialize the FinetuneModel.\n",
                "\n",
                "    Args:\n",
                "        cfg (DictConfig): Configuration for the fine-tuning.\n",
                "        local_rank (int, optional): Local rank for distributed training. Defaults to None.\n",
                "    \"\"\"\n",
                "    def __init__(self, cfg: DictConfig,local_rank=None) -> None:\n",
                "        self.local_rank = local_rank\n",
                "        self.representation = cfg.model.representation\n",
                "        self.cfg = cfg.model.finetune\n",
                "        self.context_length: int = self.cfg.context_length\n",
                "        self.callbacks = self.cfg.callbacks\n",
                "        self.ckpt = self.cfg.path.pretrained_checkpoint\n",
                "        self.bnb_config = self.cfg.bnb_config\n",
                "        self.model, self.tokenizer = self._setup_model_tokenizer()\n",
                "        self.tokenized_dataset = self._prepare_datasets(self.cfg.path.finetune_traindata)\n",
                "\n",
                "    def _setup_model_tokenizer(self) -> None:\n",
                "\n",
                "\n",
                "        llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
                "        self.ckpt,\n",
                "        model_max_length=MAX_LENGTH,\n",
                "        padding_side=\"right\",\n",
                "        use_fast=False,\n",
                "        )\n",
                "\n",
                "        if (self.bnb_config.use_4bit and self.bnb_config.use_8bit):\n",
                "            raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
                "\n",
                "        elif (self.bnb_config.use_4bit or self.bnb_config.use_8bit):\n",
                "            compute_dtype = getattr(torch,  self.bnb_config.bnb_4bit_compute_dtype)\n",
                "            bnb_config = BitsAndBytesConfig(\n",
                "            load_in_4bit= self.bnb_config.use_4bit,\n",
                "            load_in_8bit= self.bnb_config.use_8bit,\n",
                "            bnb_4bit_quant_type= self.bnb_config.bnb_4bit_quant_type,\n",
                "            bnb_4bit_compute_dtype=compute_dtype,\n",
                "            bnb_4bit_use_double_quant= self.bnb_config.use_nested_quant,\n",
                "            )\n",
                "        else:\n",
                "            bnb_config = None\n",
                "\n",
                "        # Check GPU compatibility with bfloat16\n",
                "        if compute_dtype == torch.float16:\n",
                "            major, _ = torch.cuda.get_device_capability()\n",
                "            if major >= 8:\n",
                "                print(\"=\" * 80)\n",
                "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
                "                print(\"=\" * 80)\n",
                "\n",
                "        device_map = {\"\": 0}\n",
                "        model = LlamaForSequenceClassification.from_pretrained(self.ckpt,\n",
                "                                                            num_labels=1,\n",
                "                                                            quantization_config=bnb_config,\n",
                "                                                            device_map=device_map\n",
                "                                                            )\n",
                "        \n",
                "        lora_config = LoraConfig(\n",
                "            **self.cfg.lora_config\n",
                "        )\n",
                "        model = get_peft_model(model, lora_config)\n",
                "        model.print_trainable_parameters()\n",
                "        \n",
                "\n",
                "        special_tokens_dict = dict()\n",
                "        if llama_tokenizer.pad_token is None:\n",
                "            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
                "        if llama_tokenizer.eos_token is None:\n",
                "            special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
                "        if llama_tokenizer.bos_token is None:\n",
                "            special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
                "        if llama_tokenizer.unk_token is None:\n",
                "            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
                "\n",
                "        smart_tokenizer_and_embedding_resize(\n",
                "            special_tokens_dict=special_tokens_dict,\n",
                "            llama_tokenizer=llama_tokenizer,\n",
                "            model=model,\n",
                "        )\n",
                "\n",
                "        print(len(llama_tokenizer))\n",
                "        return model, llama_tokenizer\n",
                "    \n",
                "    def _tokenize(self, examples):\n",
                "        tokenized_examples = self.tokenizer(\n",
                "            examples[self.representation], truncation=True, padding=True, return_tensors=\"pt\"\n",
                "        )\n",
                "        return tokenized_examples\n",
                "    \n",
                "\n",
                "    def _prepare_datasets(self, path: str) -> DatasetDict:\n",
                "        \"\"\"\n",
                "        Prepare training and validation datasets.\n",
                "\n",
                "        Args:\n",
                "            train_df (pd.DataFrame): DataFrame containing training data.\n",
                "\n",
                "        Returns:\n",
                "            DatasetDict: Dictionary containing training and validation datasets.\n",
                "        \"\"\"\n",
                "\n",
                "        ds = load_dataset(\"json\", data_files=path,split=\"train\")\n",
                "        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n",
                "        return dataset.map(self._tokenize, batched=True)\n",
                "\n",
                "    def _callbacks(self) -> List[TrainerCallback]:\n",
                "        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n",
                "        callbacks = []\n",
                "\n",
                "        if self.callbacks.early_stopping:\n",
                "            callbacks.append(EarlyStoppingCallback(\n",
                "                early_stopping_patience=self.callbacks.early_stopping_patience,\n",
                "                early_stopping_threshold=self.callbacks.early_stopping_threshold\n",
                "            ))\n",
                "\n",
                "        if self.callbacks.custom_logger:\n",
                "            callbacks.append(CustomWandbCallback_FineTune())\n",
                "\n",
                "        callbacks.append(EvaluateFirstStepCallback)\n",
                "\n",
                "        return callbacks\n",
                "\n",
                "    def _compute_metrics(self, p: Any, eval=True) -> Dict[str, float]:\n",
                "        preds = torch.tensor(p.predictions.squeeze())  # Convert predictions to PyTorch tensor\n",
                "        label_ids = torch.tensor(p.label_ids)  # Convert label_ids to PyTorch tensor\n",
                "\n",
                "        if eval:\n",
                "            # Calculate RMSE as evaluation metric\n",
                "            eval_rmse = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
                "            return {\"eval_rmse\": round(eval_rmse, 3)}\n",
                "        else:\n",
                "            # Calculate RMSE as training metric\n",
                "            loss = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
                "            return {\"train_rmse\": round(loss, 3), \"loss\": round(loss, 3)}\n",
                "        \n",
                "    \n",
                "\n",
                "\n",
                "    def finetune(self) -> None:\n",
                "        \"\"\"\n",
                "        Perform fine-tuning of the language model.\n",
                "        \"\"\"\n",
                "\n",
                "        config_train_args = self.cfg.training_arguments\n",
                "        callbacks = self._callbacks()\n",
                "\n",
                "\n",
                "        #os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
                "        training_args = TrainingArguments(\n",
                "            **config_train_args,\n",
                "            metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n",
                "            greater_is_better=False,  # Lower eval_rmse is better\n",
                "\n",
                "        )\n",
                "    \n",
                "        trainer = Trainer(\n",
                "            model=self.model,\n",
                "            args=training_args,\n",
                "            data_collator=None,\n",
                "            compute_metrics=self._compute_metrics,\n",
                "            tokenizer=self.tokenizer,\n",
                "            train_dataset=self.tokenized_dataset['train'],\n",
                "            eval_dataset=self.tokenized_dataset['test'],\n",
                "            callbacks=callbacks,\n",
                "        )\n",
                "\n",
                "        wandb.log({\"Training Arguments\": str(config_train_args)})\n",
                "        wandb.log({\"model_summary\": str(self.model)})\n",
                "\n",
                "        trainer.train()\n",
                "        trainer.save_model(f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\")\n",
                "\n",
                "        eval_result = trainer.evaluate(eval_dataset=self.tokenized_dataset['test'])\n",
                "        wandb.log(eval_result)\n",
                "\n",
                "        self.model.save_pretrained(self.cfg.path.finetuned_modelname)\n",
                "        wandb.finish()\n",
                "        return self.cfg.path.finetuned_modelname\n",
                "\n",
                "  "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}