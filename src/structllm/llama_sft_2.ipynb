{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    ")\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer, \n",
    "    LlamaForSequenceClassification,\n",
    "    #AutoTokenizer,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    #AutoModelForSequenceClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "from structllm.models.utils import CustomWandbCallback_FineTune, EvaluateFirstStepCallback\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict, \n",
    "    llama_tokenizer, \n",
    "    model,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(llama_tokenizer),pad_to_multiple_of=8)\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "     #   output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "     #   output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "\n",
    "    model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "     #   output_embeddings[-num_new_tokens:] = output_embeddings_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "MAX_LENGTH = 2048\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FinetuneLLama():\n",
    "    \"\"\"Class to perform finetuning of a language model.\n",
    "        Initialize the FinetuneModel.\n",
    "\n",
    "    Args:\n",
    "        cfg (DictConfig): Configuration for the fine-tuning.\n",
    "        local_rank (int, optional): Local rank for distributed training. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: DictConfig,local_rank=None) -> None:\n",
    "        self.local_rank = local_rank\n",
    "        self.representation = cfg.model.representation\n",
    "        self.cfg = cfg.model.finetune\n",
    "        self.context_length: int = self.cfg.context_length\n",
    "        self.callbacks = self.cfg.callbacks\n",
    "        self.ckpt = self.cfg.path.pretrained_checkpoint\n",
    "        self.bnb_config = self.cfg.bnb_config\n",
    "        self.model, self.tokenizer = self._setup_model_tokenizer()\n",
    "        self.tokenized_dataset = self._prepare_datasets(self.cfg.path.finetune_traindata)\n",
    "\n",
    "    def _setup_model_tokenizer(self) -> None:\n",
    "\n",
    "\n",
    "        llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        self.ckpt,\n",
    "        model_max_length=MAX_LENGTH,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        )\n",
    "\n",
    "        if (self.bnb_config.use_4bit and self.bnb_config.use_8bit):\n",
    "            raise ValueError(\"You can't load the model in 8 bits and 4 bits at the same time\")\n",
    "\n",
    "        elif (self.bnb_config.use_4bit or self.bnb_config.use_8bit):\n",
    "            compute_dtype = getattr(torch,  self.bnb_config.bnb_4bit_compute_dtype)\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= self.bnb_config.use_4bit,\n",
    "            load_in_8bit= self.bnb_config.use_8bit,\n",
    "            bnb_4bit_quant_type= self.bnb_config.bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant= self.bnb_config.use_nested_quant,\n",
    "            )\n",
    "        else:\n",
    "            bnb_config = None\n",
    "\n",
    "        # Check GPU compatibility with bfloat16\n",
    "        if compute_dtype == torch.float16:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 8:\n",
    "                print(\"=\" * 80)\n",
    "                print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "                print(\"=\" * 80)\n",
    "\n",
    "        device_map = {\"\": 0}\n",
    "        model = LlamaForSequenceClassification.from_pretrained(self.ckpt,\n",
    "                                                            num_labels=1,\n",
    "                                                            quantization_config=bnb_config,\n",
    "                                                            device_map=device_map\n",
    "                                                            )\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            **self.cfg.lora_config\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "\n",
    "        special_tokens_dict = dict()\n",
    "        if llama_tokenizer.pad_token is None:\n",
    "            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "        if llama_tokenizer.eos_token is None:\n",
    "            special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "        if llama_tokenizer.bos_token is None:\n",
    "            special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "        if llama_tokenizer.unk_token is None:\n",
    "            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=special_tokens_dict,\n",
    "            llama_tokenizer=llama_tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        print(len(llama_tokenizer))\n",
    "        return model, llama_tokenizer\n",
    "    \n",
    "    def _tokenize(self, examples):\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            examples[self.representation], truncation=True, padding=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return tokenized_examples\n",
    "    \n",
    "\n",
    "    def _prepare_datasets(self, path: str) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        Prepare training and validation datasets.\n",
    "\n",
    "        Args:\n",
    "            train_df (pd.DataFrame): DataFrame containing training data.\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: Dictionary containing training and validation datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        ds = load_dataset(\"json\", data_files=path,split=\"train\")\n",
    "        dataset = ds.train_test_split(shuffle=True, test_size=0.2, seed=42)\n",
    "        return dataset.map(self._tokenize, batched=True)\n",
    "\n",
    "    def _callbacks(self) -> List[TrainerCallback]:\n",
    "        \"\"\"Returns a list of callbacks for early stopping, and custom logging.\"\"\"\n",
    "        callbacks = []\n",
    "\n",
    "        if self.callbacks.early_stopping:\n",
    "            callbacks.append(EarlyStoppingCallback(\n",
    "                early_stopping_patience=self.callbacks.early_stopping_patience,\n",
    "                early_stopping_threshold=self.callbacks.early_stopping_threshold\n",
    "            ))\n",
    "\n",
    "        if self.callbacks.custom_logger:\n",
    "            callbacks.append(CustomWandbCallback_FineTune())\n",
    "\n",
    "        callbacks.append(EvaluateFirstStepCallback)\n",
    "\n",
    "        return callbacks\n",
    "\n",
    "    def _compute_metrics(self, p: Any, eval=True) -> Dict[str, float]:\n",
    "        preds = torch.tensor(p.predictions.squeeze())  # Convert predictions to PyTorch tensor\n",
    "        label_ids = torch.tensor(p.label_ids)  # Convert label_ids to PyTorch tensor\n",
    "\n",
    "        if eval:\n",
    "            # Calculate RMSE as evaluation metric\n",
    "            eval_rmse = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
    "            return {\"eval_rmse\": round(eval_rmse, 3)}\n",
    "        else:\n",
    "            # Calculate RMSE as training metric\n",
    "            loss = torch.sqrt(((preds - label_ids) ** 2).mean()).item()\n",
    "            return {\"train_rmse\": round(loss, 3), \"loss\": round(loss, 3)}\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def finetune(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform fine-tuning of the language model.\n",
    "        \"\"\"\n",
    "\n",
    "        config_train_args = self.cfg.training_arguments\n",
    "        callbacks = self._callbacks()\n",
    "\n",
    "\n",
    "        #os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "        training_args = TrainingArguments(\n",
    "            **config_train_args,\n",
    "            metric_for_best_model=\"eval_rmse\",  # Metric to use for determining the best model\n",
    "            greater_is_better=False,  # Lower eval_rmse is better\n",
    "\n",
    "        )\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=None,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_dataset=self.tokenized_dataset['train'],\n",
    "            eval_dataset=self.tokenized_dataset['test'],\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        wandb.log({\"Training Arguments\": str(config_train_args)})\n",
    "        wandb.log({\"model_summary\": str(self.model)})\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model(f\"{self.cfg.path.finetuned_modelname}/llamav2-7b-lora-fine-tune\")\n",
    "\n",
    "        eval_result = trainer.evaluate(eval_dataset=self.tokenized_dataset['test'])\n",
    "        wandb.log(eval_result)\n",
    "\n",
    "        self.model.save_pretrained(self.cfg.path.finetuned_modelname)\n",
    "        wandb.finish()\n",
    "        return self.cfg.path.finetuned_modelname\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
