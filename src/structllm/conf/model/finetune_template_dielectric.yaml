representation: ???
special_tokens: {
    "unk_token": "[UNK]",
    "pad_token": "[PAD]",
    "cls_token": "[CLS]",
    "sep_token": "[SEP]",
    "mask_token": "[MASK]",
    "eos_token": "[EOS]",
    "bos_token": "[BOS]",
}

logging:
  wandb_project : "finetune_30k_wes_3"
  wandb_log_model : "checkpoint"

finetune:
  model_name: finetune_30k_wes_3
  freeze_base_model: False
  dataset_name: "matbench_dielectric"
  exp_name: [
    "train_${model.representation}_${model.finetune.dataset_name}_0",
    "train_${model.representation}_${model.finetune.dataset_name}_1",
    "train_${model.representation}_${model.finetune.dataset_name}_2",
    "train_${model.representation}_${model.finetune.dataset_name}_3",
    "train_${model.representation}_${model.finetune.dataset_name}_4",
    ]


  path:
    checkpoint_rootpath: "/work/so87pot/structllm/model_outs2/checkpoints/02_02_100k_pt_bpe_512"  # @deprecated
    checkpoint_name: "checkpoint-86000"                           # @deprecated

    pretrained_checkpoint: ???


    finetune_data_rootpath: "/work/so87pot/material_db/matbench_text"                     # <--- Change this to the path of the finetune data
    finetune_traindata: [
      "${model.finetune.path.finetune_data_rootpath}/train_${model.finetune.dataset_name}_0.json",
      "${model.finetune.path.finetune_data_rootpath}/train_${model.finetune.dataset_name}_1.json",
      "${model.finetune.path.finetune_data_rootpath}/train_${model.finetune.dataset_name}_2.json",
      "${model.finetune.path.finetune_data_rootpath}/train_${model.finetune.dataset_name}_3.json",
      "${model.finetune.path.finetune_data_rootpath}/train_${model.finetune.dataset_name}_4.json",
          ]

    finetune_testdata:
    root_path: "/work/so87pot/structllm/megaloop/finetune/${model.finetune.model_name}"  # <--- Change this to the path where chkpoints and logs will be saved
    output_dir: "${model.finetune.path.root_path}/checkpoints/${model.finetune.exp_name}"
    logging_dir: "${model.finetune.path.root_path}/logs/${model.finetune.exp_name}"
    finetuned_modelname: "${model.finetune.path.root_path}/checkpoints/finetuned_${model.finetune.exp_name}"

  context_length: ???
  callbacks:
    early_stopping: True
    custom_logger: True
    early_stopping_patience: 10
    early_stopping_threshold: 0.001

  training_arguments:
    output_dir: "${model.finetune.path.output_dir}"
    overwrite_output_dir: True
    num_train_epochs: 100
    per_device_train_batch_size: ???
    save_strategy: "epoch"
    evaluation_strategy: 'epoch'   
    logging_strategy: 'epoch'
    logging_first_step: True
    save_steps: 3 # Number of epochs before saving
    report_to: "wandb"
    save_total_limit: 5
    learning_rate: 2e-4
    logging_steps: 1 
    eval_steps: 1
    seed: 42
    load_best_model_at_end: True



inference:
  benchmark_dataset: "${model.finetune.dataset_name}"
  context_length: "${model.finetune.context_length}"
  exp_name: [
    "test_${model.representation}_${model.finetune.dataset_name}_0",
    "test_${model.representation}_${model.finetune.dataset_name}_1",
    "test_${model.representation}_${model.finetune.dataset_name}_2",
    "test_${model.representation}_${model.finetune.dataset_name}_3",
    "test_${model.representation}_${model.finetune.dataset_name}_4",
    ]
  path:
    pretrained_checkpoint: [
   
  ] 
    test_data_rootpath: "/work/so87pot/material_db/matbench_text"                     # <--- Change this to the path of the finetune data
    test_data: [
    "${model.inference.path.test_data_rootpath}/test_${model.finetune.dataset_name}_0.json",
    "${model.inference.path.test_data_rootpath}/test_${model.finetune.dataset_name}_1.json",
    "${model.inference.path.test_data_rootpath}/test_${model.finetune.dataset_name}_2.json",
    "${model.inference.path.test_data_rootpath}/test_${model.finetune.dataset_name}_3.json",
    "${model.inference.path.test_data_rootpath}/test_${model.finetune.dataset_name}_4.json",
  ]
    root_path: "/home/so87pot/n0w0f/structllm/src/structllm/models/predictions"        # <--- Change this to the path where predictions will be saved
    output_dir: "${model.inference.path.root_path}/checkpoints/${model.inference.exp_name}"
    logging_dir: "${model.inference.path.root_path}/logs/${model.inference.exp_name}"
    predictions: "${model.inference.path.root_path}/checkpoints/inference${model.inference.exp_name}"

  benchmark_save_file: "${model.finetune.path.root_path}"

  