pretrain:
  exp_name: pretrain_composition_300k
  model_name_or_path: "bert-base-uncased"

  model_config:
    hidden_size: 512
    num_hidden_layers: 4
    num_attention_heads: 8
    is_decoder: False
    add_cross_attention: False

  path:
    pretrained_checkpoint: 
    data_root_path: "/work/so87pot/material_db/nomad/07_02_data/composition/390k"
    traindata: "${model.pretrain.path.data_root_path}/train.csv"
    evaldata: "${model.pretrain.path.data_root_path}/val.csv"
    root_path: "/work/so87pot/structllm/results/checkpoints"
    output_dir: "${model.pretrain.path.root_path}/checkpoints/${model.pretrain.exp_name}"
    logging_dir: "${model.pretrain.path.root_path}/logs/${model.pretrain.exp_name}"
    finetuned_modelname: "${model.pretrain.path.root_path}/output/finetuned_${model.pretrain.exp_name}"

  context_length: 32


  callbacks:
    early_stopping: True
    custom_logger: True
    early_stopping_patience: 10
    early_stopping_threshold: 0.001


  training_arguments:
    output_dir: "${model.pretrain.path.output_dir}"   # Directory where model checkpoints and logs will be saved
    logging_dir: "${model.pretrain.path.logging_dir}"
    overwrite_output_dir: True     
    label_names: ["labels"]
    save_total_limit: 5      # Maximum number of checkpoints to save
    per_device_train_batch_size: 1024 # Batch size per device during training
    num_train_epochs: 50       # Number of training epochs
    learning_rate: 2e-4
    save_steps: 1000
    report_to: "wandb"
    evaluation_strategy:  'steps'          # check evaluation metrics at each epoch
    logging_steps: 50                    # we will log every 100 steps
    eval_steps: 50                   # we will perform evaluation every 500 steps
    load_best_model_at_end: True
    seed: 42 

  mlm:
    is_mlm: True 
    mlm_probability: 0.15
