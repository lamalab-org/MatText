tokenizer:
  name: bpe
  path: 
    root_path: "/home/so87pot/n0w0f/structllm/src/structllm/tokenizer"
    tokenizer_path: "${tokenizer.path.root_path}/tokenizer-slice_130k.json"

logging:
  wandb_project : "08_02_100k_ft_512bpe"
  wandb_log_model : "checkpoint"

finetune:
  model_name: 08_02_100k_bpe_512
  exp_name: [
    "train_matbench_log_kvrh_0",
    "train_matbench_log_kvrh_1",
    "train_matbench_log_kvrh_2",
    "train_matbench_log_kvrh_3",
    "train_matbench_log_kvrh_4",
    ]


  path:
    checkpoint_rootpath: "/work/so87pot/structllm/model_outs2/checkpoints/02_02_100k_pt_bpe_512"  # <--- Change this to the path of the pretrained checkpoint
    checkpoint_name: "checkpoint-86000"                           # <--- Change this to the name of the pretrained checkpoint

    pretrained_checkpoint: "${model.finetune.path.checkpoint_rootpath}/${model.finetune.path.checkpoint_name}"


    finetune_data_rootpath: "/work/so87pot/structllm/finetune_data/csv"                     # <--- Change this to the path of the finetune data
    finetune_traindata: [
      "${model.finetune.path.finetune_data_rootpath}/train_matbench_log_kvrh_0.csv",
      "${model.finetune.path.finetune_data_rootpath}/train_matbench_log_kvrh_1.csv",
      "${model.finetune.path.finetune_data_rootpath}/train_matbench_log_kvrh_2.csv",
      "${model.finetune.path.finetune_data_rootpath}/train_matbench_log_kvrh_3.csv",
      "${model.finetune.path.finetune_data_rootpath}/train_matbench_log_kvrh_4.csv",
          ]

    finetune_testdata:
    root_path: "/work/so87pot/structllm/results/finetune/${model.finetune.model_name}"  # <--- Change this to the path where chkpoints and logs will be saved
    output_dir: "${model.finetune.path.root_path}/checkpoints/${model.finetune.exp_name}"
    logging_dir: "${model.finetune.path.root_path}/logs/${model.finetune.exp_name}"
    finetuned_modelname: "${model.finetune.path.root_path}/checkpoints/finetuned_${model.finetune.exp_name}"

  context_length: 512
  callbacks:
    early_stopping: True
    custom_logger: True
    early_stopping_patience: 10
    early_stopping_threshold: 0.01

  training_arguments:
    output_dir: "${model.finetune.path.output_dir}"
    overwrite_output_dir: True
    num_train_epochs: 400
    per_device_train_batch_size: 128
#    save_strategy: "epoch"
    evaluation_strategy: 'steps'   
    save_steps: 1000  # Number of epochs before saving
    report_to: "wandb"
    save_total_limit: 5
    learning_rate: 2e-4
    logging_steps: 100 
    eval_steps: 50
    seed: 42
    load_best_model_at_end: True



inference:
  benchmark_dataset: "matbench_log_kvrh"
  context_length: "${model.finetune.context_length}"
  exp_name: [
    "test_matbench_log_kvrh_0",
    "test_matbench_log_kvrh_1",
    "test_matbench_log_kvrh_2",
    "test_matbench_log_kvrh_3",
    "test_matbench_log_kvrh_4",
    ]
  path:
    pretrained_checkpoint: [
   
  ] 
    test_data_rootpath: "/home/so87pot/n0w0f/structllm/data/mb_1/csv"                     # <--- Change this to the path of the finetune data
    test_data: [
    "${model.inference.path.test_data_rootpath}/test_matbench_log_kvrh_0.csv",
    "${model.inference.path.test_data_rootpath}/test_matbench_log_kvrh_1.csv",
    "${model.inference.path.test_data_rootpath}/test_matbench_log_kvrh_2.csv",
    "${model.inference.path.test_data_rootpath}/test_matbench_log_kvrh_3.csv",
    "${model.inference.path.test_data_rootpath}/test_matbench_log_kvrh_4.csv",
  ]
    root_path: "/home/so87pot/n0w0f/structllm/src/structllm/models/predictions"        # <--- Change this to the path where predictions will be saved
    output_dir: "${model.inference.path.root_path}/checkpoints/${model.inference.exp_name}"
    logging_dir: "${model.inference.path.root_path}/logs/${model.inference.exp_name}"
    predictions: "${model.inference.path.root_path}/checkpoints/inference${model.inference.exp_name}"

  benchmark_save_file: "${model.finetune.path.root_path}"

  