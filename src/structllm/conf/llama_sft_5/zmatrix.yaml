# @package _global_
model:
  representation: zmatrix
  logging:
    wandb_project: llama_collator_5
  
  finetune:
    model_name: llama_collator_5
    # context_length: 512
    # training_arguments:
    #   per_device_train_batch_size: 128
    # path:
    #   pretrained_checkpoint: /work/so87pot/structllm/megaloop/checkpoints/checkpoints/zmatrix_pt_30k_zmatrix/checkpoint-23000