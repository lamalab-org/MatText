# @package _global_

defaults:
  - override /model: pretrain_template

model:
  representation: crystal_llm_rep
  pretrain.context_length: 512
  pretrain.training_arguments.per_device_train_batch_size: 32