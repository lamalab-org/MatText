# @package _global_
model:
  representation: atoms_params
  alpha: 0.6
  logging:
    wandb_project: potential_lj_atoms_params
  special_num_token: False

  finetune:
    model_name: atoms_params_potential_0_6
    context_length: 32
    training_arguments:
      per_device_train_batch_size: 1024
    path:
      pretrained_checkpoint: /work/so87pot/mattext/megaloop2/checkpoints/checkpoints/atoms_params_pt_2m_atoms_params/checkpoint-24000
