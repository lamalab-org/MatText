representation: ???
special_num_token: False
dataset: pretrain
dataset_type: ???
data_repository: "n0w0f/MatText"
context_length: ???
special_tokens:
  {
    "unk_token": "[UNK]",
    "pad_token": "[PAD]",
    "cls_token": "[CLS]",
    "sep_token": "[SEP]",
    "mask_token": "[MASK]",
    "eos_token": "[EOS]",
    "bos_token": "[BOS]",
  }

logging:
  wandb_project: test-pretrain
  wandb_log_model: "checkpoint"

pretrain:
  name: test-pretrain
  exp_name: "${model.representation}_${model.pretrain.name}"
  model_name_or_path:            # "bert-base-uncased"
  dataset_name: "${model.dataset_type}"
  context_length: "${model.context_length}"

  model_config:
    hidden_size: 768
    # num_hidden_layers: 4
    # num_attention_heads: 8
    # is_decoder: False
    # add_cross_attention: False
    max_position_embeddings: 768

  path:
    pretrained_checkpoint:
    data_root_path: ???
    root_path: "${hydra:runtime.cwd}/results/${now:%Y-%m-%d}/${now:%H-%M-%S}/pretrain" #--> Change this to your preferred path where checkpints will be saved
    traindata: "${model.pretrain.dataset_name}"
    evaldata: "${model.pretrain.dataset_name}"
    output_dir: "${model.pretrain.path.root_path}/checkpoints/${model.pretrain.exp_name}"
    logging_dir: "${model.pretrain.path.root_path}/logs/${model.pretrain.exp_name}"
    finetuned_modelname: "${model.pretrain.path.root_path}/output/finetuned_${model.pretrain.exp_name}"

  

  callbacks:
    early_stopping: False
    custom_logger: True
    early_stopping_patience: 10
    early_stopping_threshold: 0.001

  training_arguments:
    output_dir: "${model.pretrain.path.output_dir}" # Directory where model checkpoints and logs will be saved
    logging_dir: "${model.pretrain.path.logging_dir}"
    overwrite_output_dir: True
    label_names: ["labels"]
    save_total_limit: 5 # Maximum number of checkpoints to save
    per_device_train_batch_size: 1024 # Batch size per device during training
    num_train_epochs: 2 # Number of training epochs
    learning_rate: 2e-4
    save_steps: 1000
    report_to: "wandb"
    evaluation_strategy: "steps" # check evaluation metrics at each epoch
    logging_steps: 50 # we will log every 100 steps
    eval_steps: 50 # we will perform evaluation every 500 steps
    load_best_model_at_end: True
    seed: 42

  mlm:
    is_mlm: True
    mlm_probability: 0.15

