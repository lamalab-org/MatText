# Using MatText

## Creating text representations for crystal structures

Converting structures into text representations can be done using our [`TextRep`](api.md#mattext.representations.TextRep) class

```python
from mattext.representations import TextRep
from pymatgen.core import Structure


# Load structure from a CIF file
from_file = "InCuS2_p1.cif"
structure = Structure.from_file(from_file, "cif")

# Initialize TextRep Class
text_rep = TextRep.from_input(structure)
```

### `get_requested_text_reps` method

The [`get_requested_text_reps`](api.md#mattext.representations.TextRep.get_requested_text_reps) method retrieves the requested text representations of the crystal structure and returns them in a dictionary.

For example, to obtain the `cif_p1`, `slice`, `atoms_params`, `crystal_llm_rep`, and `zmatrix` representations, use the following code:

```python
text_rep = TextRep(structure)

# Define a list of requested representations
requested_reps = [
    "cif_p1",
    "slice",
    "atoms_params",
    "crystal_llm_rep",
    "zmatrix"
]

# Get the requested text representations
requested_text_reps = text_rep.get_requested_text_reps(requested_reps)
```

### Supported text representations

The [`TextRep`](api.md#mattext.representations.TextRep) class currently supports the following text representations:

- **SlICES** (`slice`): SLICE representation of the crystal structure.
- **Composition** (`composition`): Chemical composition in hill format.
- **CIF Symmetrized** (`cif_symmetrized`): Multi-line CIF representation with symmetrized structure and rounded float numbers.
- **CIF $P_1$** (`cif_p1`): Multi-line CIF representation with the conventional unit cell and rounded float numbers.
- **Crystal-text-LLM Representation** (`crystal_llm_rep`): Representation following the format specified in the cited work.
- **Robocrystallographer Representation** (`robocrys_rep`): Representation generated by Robocrystallographer.
- **Atom Sequences** (`atoms`): List of atoms inside the unit cell.
- **Atoms Squences++** (`atoms_params`): List of atoms with lattice parameters.
- **Z-Matrix** (`zmatrix`): Z-Matrix representation of the crystal structure.
- **Local Env** (`local_env`):  List of Wyckoff label and SMILES separated by line breaks for each local environment.

For more details on each representation and how to obtain them, refer to the respective method documentation in the `TextRep` class.


## Transformations 


The `TextRep` class supports various transformations that can be applied to the input structure.



### Permute Structure 

The `permute_structure` transformation randomly permutes the order of atoms in a structure. 


```python
from mattext.representations import TextRep
from pymatgen.core.structure import Structure

structure_1 = Structure.from_file("N2_p1.cif", "cif")

transformations = [("permute_structure", {"seed": 42})]

text_rep = TextRep.from_input(structure_1, transformations)
text_representations_requested = ["atoms", "crystal_llm_rep"]
print("Permuted Pymatgen Structure:")
print(text_rep.structure)
print("Permuted Text Representations:")
print(text_rep.get_requested_text_reps(text_representations_requested))
```

### Translate Structure 

The `translate_structure` transformation randomly translates all atoms in a structure by a specified vector. This can simulate small displacements in the structure.

```python
transformations = [("translate_structure", {"seed": 42, "vector": [0.1, 0.1, 0.1]})]

text_rep = TextRep.from_input(structure_1, transformations)
text_representations_requested = ["crystal_llm_rep"]
print("Translated Crystal-text-LLM Representations:")
print(text_rep.get_requested_text_reps(text_representations_requested))
```

### Augmenting data

In principle, we can generate valid text representations with random transformations with physically meaningful parameters. Dummy example is shown below

```python
from mattext.representations import TextRep

# Define transformations
translation_vectors = np.random.uniform(low=0.1, high=0.5, size=(3, 3))
for vector in translation_vectors:
    transformations = [
        ("permute_structure", {"seed": 42}),
        ("perturb_structure", {"seed": 42, "max_distance": 0.1}),
        ("translate_structure", {"seed": 42, "vector": vector.tolist()})
    ]
    text_rep = TextRep.from_input(structure_2, transformations)
    text_representations_requested = ["crystal_llm_rep"]
    print("Translated Text Representations:")
    print(text_rep.get_requested_text_reps(text_representations_requested))

```
this would output 

```bash
Translated Text Representations:
{'crystal_llm_rep': '3.9 3.9 3.9\n90 90 90\nO2-\n0.76 0.98 0.41\nTi4+\n0.77 0.98 0.89\nO2-\n0.76 0.49 0.89\nO2-\n0.26 0.97 0.88\nSr2+\n0.25 0.47 0.38'}
Translated Text Representations:
{'crystal_llm_rep': '3.9 3.9 3.9\n90 90 90\nO2-\n0.85 0.66 0.18\nTi4+\n0.86 0.66 0.66\nO2-\n0.85 0.17 0.66\nO2-\n0.35 0.65 0.65\nSr2+\n0.34 0.15 0.15'}
Translated Text Representations:
{'crystal_llm_rep': '3.9 3.9 3.9\n90 90 90\nO2-\n0.63 0.94 0.35\nTi4+\n0.64 0.94 0.84\nO2-\n0.64 0.45 0.84\nO2-\n0.13 0.94 0.83\nSr2+\n0.12 0.43 0.33'}
```

> more examples are available as notebooks in the repository

The following transformations are available for transforming structures:

#### Randomly permute structure

[`permute_structure`](api.md#mattext.representations.transformations.TransformationCallback.permute_structure) randomly permutes the order of atoms in a structure.

#### Randomly translate single atom
[`translate_single_atom`](api.md#mattext.representations.transformations.TransformationCallback.translate_single_atom) randomly translates one or more atoms in a structure.


#### Randomly perturb structure

[`perturb_structure`](api.md#mattext.representations.transformations.TransformationCallback.perturb_structure) randomly perturbs atoms in a structure.

#### Randomly translate structure

[`translate_structure`](api.md#mattext.representations.transformations.TransformationCallback.translate_structure) randomly translates the atoms in a structure.

 >This transformation supports additional keyword arguments for fine-tuning the translation.

MatText leverages methods from pymatgen and support all the keyword arguments in `Structure.translate_sites` method.


All transformations utilize a common seed value for reproducibility and accept additional parameters for customization.

For more details on each transformation and its parameters, refer to the respective function documentation.


## Modeling and Benchmarking 

MatText have  pipelines for seamless pretraining([`pretrain`](api.md#mattext.models.pretrain)) and benchmarking ([`benchmark`](api.md#mattext.models.benchmark)) with finetuning ([`finetune`](api.md#mattext.models.finetune)) on multiple MatText representations. We use Hydra framework for dynamically creating hierarchical configuration based on the pipeline and representations that we want to use.


### Pretraining on single MatText Representation

```bash
python main.py -cn=pretrain model=pretrain_example +model.representation=composition +model.dataset_type=pretrain30k +model.context_length=32

```

Here, `model=pretrain_example` would select `pretrain_example` as the base config for pretrain run `-cn=pretrain`.

All config file can be found here, inside respective directories.
```bash
cd /conf/
```
Base configs can be found at `/conf/model`

`+model.representation` can be any MatText representation. The pipeline would use data represented using that particular representation for training.
`+model.dataset_type` can be one of the MatText pretraining datasets.
`+model.context_length` would define the context length.

>Note: Use meaningful context length according to the representation to avoid truncation.

The `+` symbol before a configuration key indicates that you are adding a new key-value pair to the configuration. This is useful when you want to specify parameters that are not part of the default configuration.


Inorder to override the existing default configuration from CLI use `++`, for eg `++model.pretrain.training_arguments.per_device_train_batch_size=32`. 


For advanced usage (changing architecture, training arguments or modelling parameters) it would be easier to make the changes in the base config file which is `/conf/model/pretrain_example` than having to override parameters with lengthy CLI commands!



### Running benchmark on single MatText Representation

```bash
python main.py -cn=benchmark model=benchmark_example +model.dataset_type=filtered +model.representation=composition +model.dataset=perovskites +model.checkpoint=path/to/checkpoint  
```


Here for the benchmarking pipeline(`-cn=benchmark`) base config is `benchmark_example.yaml`. 
You can define the parameters for the experiment hence at `\conf\model\benchmark_example.yaml`.

> Here +model.dataset_type=filtered would select the type of benchmark. It can be `filtered` (avoid having truncated structure in train and test set, Only relatively small structures are present here, but this would also mean having less number of sampels to train on ) or `matbench` (complete dataset, there are few big structures , which would be trunated if the context length for modelling is less than `2048`).


> `+model.dataset_type=filtered` would produce the report compatible with matbench leaderboard.





### Configuring experiments and model


The main configuration for the run is in config.yaml and other configs are grouped in respective folders. An example directory structure of configs is below.
```bash
├── conf
│   ├── config.yaml
│   ├── pretrain30k
│   │   ├── cifp1.yaml
│   │   ├── cifsymmetrized.yaml
│   │   ├── composition.yaml
│   │   ├── crystal_llm.yaml
│   │   └── slice.yaml
│   ├── finetune30k
│   │   ├── cifp1.yaml
│   │   ├── cifsymmetrized.yaml
│   │   ├── composition.yaml
│   │   ├── crystal_llm.yaml
│   │   └── slice.yaml
│   └── model
│       ├── finetune_template_dielectric.yaml
│       ├── finetune_template_gvrh.yaml
│       ├── finetune_template.yaml
│       └── pretrain_template.yaml
├── main.py
├── models
```

>Configs in the group __models__ are the _base config templates_ and other groups (__pretrain30K / finetune30K__ ) are experiment groups. Experiment group extends from the base template and add/override additional configs. read more about extending configs [here](https://hydra.cc/docs/patterns/extending_configs/).


configs inside __pretrain30k__ extends the `base pretrain template` and override the   `representation name`, `batch size`, `etc` in the base pretrain_template.
```yaml
# @package _global_
model:
  logging:
    wandb_project: pt_30k_test
  
  representation: cif_p1
  pretrain:
    name: pt_30k_test
    context_length:  1024
    training_arguments:
      per_device_train_batch_size: 32
    path:
      data_root_path: </path/to/dataset>
      
```

### Pretraining and Finetuning experiments

You can start a multirun job parallely with the below cli script. 

```bash
python /path/to/main.py --multirun model=pretrain_template ++hydra.launcher.gres=gpu:1 +pretrain30k=cifp1,cifsym,composition,crystal_llm,slice

```
We use HF Trainer and hence by default it support DP but for DDP support 
```bash
python -m torch.distributed.run --nproc_per_node=4  /path/to/main.py --multirun model=pretrain_template ++hydra.launcher.gres=gpu:1 +pretrain30k=cifp1,cifsym,composition,crystal_llm,slice

```

Here `model=pretrain_template` select pretrain_template as the base config and override/extend it with `+pretrain30k=cifp1`. This would essentially start pretraining with cifp1 representation for the dataset-30K

For finetuning select `model=finetune_template_<property>` as the base template

Note `+pretrain30k=cifp1,cifsym,composition,crystal_llm,slice` will launch 5 jobs parallely each of them with pretrain_template as the base config and corresponding experiment template extending them.

>By default we use [hydra submitit slurm launcher](https://hydra.cc/docs/plugins/submitit_launcher/). you can override it from cli / or change it in the main config file. For kubernetes based infrastructures [hydra submitit local launcher](https://hydra.cc/docs/plugins/submitit_launcher/) is ideal for parallel jobs. Or you can use the default hydra multirun launcher, which will run jobs sequentially.
You can configure the launcher configurations in main config file.

### Adding new experiments
New experiments can be easily added with the following step. 

1. Create an experiment config group inside `conf/` . Make a new directory and add experiment template inside it. 
2. Add / Edit the configs that you want for the new experiments. eg: override the pretrain checkpoints to new pretrained checkpoint
3. Launch runs similarly but now with new experiment group 

```bash
python main.py --multirun model=pretrain_template ++hydra.launcher.gres=gpu:1 +<new_exp_group>=<new_exp_template_1>,<new_exp_template_2>, ..

```
